{
	"ar": {
		"content": "تعديل - تعديل مصدري - تعديل ويكي بيانات\n\nفي علم الحاسوب، الترميز الموحد [2] (يونيكودأو يُونِكُود[3]) (بالإنجليزية: Unicode)‏ هو معيار يمكن الحواسيب من تمثيل النصوص المكتوبة بأغلب نظم الكتابة ومعالجتها، بصورة متناسقة. يتكون يونيكود من 100,000 محرف، وطقم من مخططات الرموز كمرجع مرئي، ونهج في الترميز، وطقم من ترميزات المحارف المعيارية، وسرد لخصائص المحارف، وطقم من البيانات المرجعية، وعدد من الأمور المتعلقة مثل خصائص المحارف، وقواعد تطبيع النص، وفك الحروف لوحداتها الأولية، والترتيب، والتصيير، وثنائية الاتجاه (لعرض النصوص الذي يحتوي على كتابات من اليمين لليسار، مثل العربية، مع كتابات من اليسار لليمين، مثل اللاتينية).[4] يطور يونيكود بالتوازي مع معيار طقم المحارف العالمي، وينشر على شكل كتاب يحمل الاسم معيار يونيكود (Unicode Standard).\n\nيطمح مجمع يونيكود -المنظمة غير الربحية التي تنسق تطوير يونيكود- في النهاية إلى استبدال ترميزات المحارف الموجودة حاليا، ليحل محلها يونيكود وتنسيق يونيكود المعياري للتحويل (Unicode Transformation Format ،UTF)، حيث أن الكثير من الترميزات الحالية محدودة السعة والمدى، ولا تتوافق مع البيئات متعددة اللغات.\n\nأدى نجاح يونيكود في توحيد أطقم المحارف إلى انتشار وغلبة استخدامه في توطين وعولمة برمجيات الحاسوب. وجرى تطبيق البرنامج في العديد من التقنيات الحديثة، مثل لغة الترميز القابلة للامتداد، ولغة البرمجة جافا وأنظمة التشغيل الحديثة.\n\nيهدف الترميز الموحد بشكل صريح إلى تجاوز القصور في ترميزات المحارف التقليدية، كهؤلاء الذين حددهم معيار آيزو/آي إي سي 8859 وينتشر استخدامهم في مختلف أقطار العالم لكنهم يفتقدون للتوافقية بينهم بشكل كبير.\n\nتشترك الكثير من ترميزات المحارف التقليدية في مشكلة تمكينهم للمعالجة الحاسوبية ثنائية اللغة (عادة باستخدام المحارف اللاتينية بالإضافة للغة المحلية) دون تعددية اللغات (معالجة العديد من اللغات مختلطة مع بعضها، كالعربية والإنجليزية والصينية والهندية في صفحة واحدة).\n\nيُرمِّز يونيكود -عن قصد- المحارف الأصلية (تلك التي تمثل الحروف ذاتها وما شابهها) وليس تنويعات الشكل النهائي (فترمز حرف الخاء في العربية، مثلا، وليس أشكال الخاء المختلفة في أول، أو وسط، أو آخر الكلمة، أو الشكل المنفصل). في حالة المحارف الصينية، يؤدى هذا أحيانا لبعض الخلافات حول تمييز التنويعات الشكلية من الحرف المشكل لها.\n\nيضطلع يونيكود بدور تحديد رمز فريد -رقما، وليس شكلا- لكل محرف. بعبارة أخرى، يمثل يونيكود المحارف بصورة مجرّدة ويترك العرض البصري (الحجم، والشكل، والخط، والأسلوب) لبرمجيات أخرى، مثل متصفح الوب أو معالج الكلمات. على الرغم من بساطة هذا الهدف، فقد أصبح معقدا نتيجة للتنازلات التي قدمها مصمموا يونيكود بغية تشجيع الإسراع في استخدامه.\n\nجُعل أول 256 رمز متطابقين مع محتويات ISO 8859-1 لتبسيط عملية تحويل النصوص الغربية الموجودة مسبقا. العديد من المحارف المتطابقة تم ترميزها العديد من المرات في نقاط ترميز مختلفة للحفاظ على التفريق المُستخدم في الترميزات العتيقة مما يسمح بالتحويل بين هذه الترميزات من وإلى يونيكود دون فقد أي معلومات. على سبيل المثال، قسم «كامل العرض» يحتوي على ألفبائية لاتينية كاملة منفصلة عن قسم الألفبائية اللاتينية. في الخطوط الصينية، واليابانية، والكورية (ص‌ي‌ك)، تعرض هذه المحارف بنفس عرض رموز ص‌ي‌ك بلا من نصف العرض.\n\nعند الكتابة عن يونيكود، تٌمثّل المحارف بكتابة \"U+\" يليها رقم ست عشري يمثل رمز المحرف.\n\nيُطوّر مجمع يونيكود -ومقره كاليفورنيا- معيار يونيكود. الشركات أو الأفراد الراغبون في دفع استحقاقات العضوية يمكنهم الانضمام للمنظمة. يشمل الأعضاء نظريا كل شركات العتاد والبرمجيات الرئيسية التي تهتم بمعايير معالجة النصوص، مثل أدوبي سيستمز، وأبل، وآي.بي.إم، ومايكروسوفت، وهيولت باكرد، وزيروكس وغيرها الكثير.\n\nنشر المجمع معيار يونيكود (ISBN 0-321-18578-1) لأول مرّة في 1991، ويواصل تطوير المعيار بناء على العمل الأصلي. يُطوّر يونيكود بالاشتراك مع المنظمة الدولية للمعايير (أيزو) ويشترك في مخطط المحارف مع ISO/IEC 10646: طقم المحارف العالمي.\n\nيعمل يونيكود و ISO/IEC 10646 كترميزات محارف بشكل متساو، لكن معيار يونيكود يشتمل على الكثير من المعلومات للمطبيقن، ويغطي -بالتفصيل- مواضيع مثل الترتيب (كالترتيب الأبجدي والألفبائي حسب كل لغة)، والتصيير. يسرد ينيكود زخما من خصائص المحارف، كتلك المطلوبة لدعم النصوص ثنائية الاتجاه.\n\nمُراجعات يونيكود حتى الآن:\n\nيُغطي يونيكود تقريبا كل أنظمة الكتابة المستخدمة حاليا.[13]\n\nعلى الرغم من أن أكثر من 30 نظام كتابة مدرجة في يونيكود، إلا أنه تبقى بعض أنظمة الكتابة التي تنتظر الترميز. كما يتم أيضا إضافة المزيد من المحارف لأنظمة الكتابة المرمّزة بالفعل، وأيضا الرموز مثل الرموز الموسيقية والرياضية.\n\nمن ضمن نظم الكتابة التي تنتظر الترميز، الهيروغليفية، البابلية والكتابة المسمارية والأبجدية الفينيقية. مع بعض أنظمة كتابة بعض الأقليات في آسيا وأوروبا وأفريقيا، العديد منها غير مفهوم.",
		"url": "https://ar.wikipedia.org/wiki/Unicode"
	},
	"cs": {
		"content": "Unicode je technická norma pro oblast výpočetní techniky definující jednotnou znakovou sadu a konzistentní kódování znaků pro reprezentaci a zpracovávání textů použitelné pro většinu písem používaných v současnosti na Zemi. Unicode je vyvíjen v součinnosti s ISO/IEC 10646 a je publikován elektronicky jako The Unicode Standard. Nejnovější verze obsahuje repertoár více než 140 000 znaků pokrývajících 159 moderních a historických písem a mnoho sad symbolů. Standard sestává ze sady tabulek pro vizuální referenci, popisu metod kódování, sady referenčních datových souborů a dalších položek, jako například vlastností znaků, pravidel pro normalizaci textů, dekompozici, řazení, vykreslování a zobrazování obousměrného textu (pro správné zobrazení textu obsahující písma psaná zprava doleva i zleva doprava, jako například arabské a hebrejské písmo).[1] Poslední verze je Unicode 14.0 ze září roku 2021. Normu udržuje Unicode Consortium.\n\nÚspěch Unicode v unifikaci znakových sad vedl k jeho rozšíření a převládajícímu používání pro internacionalizaci a lokalizaci počítačového softwaru. Unicode je implementován mnoha technologiemi, včetně moderních operačních systémů, XML, programovacím jazykem Java a .NET Frameworkem firmy Microsoft.\n\nUnicode definuje několik způsobů reprezentace textů různými znakovými kódy. K nejpoužívanějším kódováním patří UTF-8, UTF-16 a zastaralé UCS-2. UTF-8 používá jeden bajt pro libovolný ASCII znak, přičemž všechny ASCII znaky mají v UTF-8 stejné kódové hodnoty jako ASCII a dva až čtyři bajty pro jiné znaky. UCS-2 používá 16bitové kódové jednotky (dva 8bitové bajty) pro každý znak, ale neumožňuje kódovat všechny znaky v aktuálním standardu Unicode. UTF-16 je rozšíření UCS-2, které pomocí dvou 16bitových jednotek (4 × 8 bit) umožňuje kódovat všechny znaky z Unicode. V Číně se používá kódování GB18030, které přebírá celý znakový repertoár Unicode, proto je také jedním ze způsobů kódování Unicode. Mapování GB18030 na UTF-32 je však netriviální (potřebuje převodní tabulku).\n\nUmožňuje současně používat různá písma při vícejazyčném zpracování textu v počítači a kóduje široké portfolio znaků pro profesionální zpracování textů v prakticky jakémkoli moderním i historickém jazyce. Nevýhodou unicode může být složitější zpracování, stejný text zabírá více prostoru na disku nebo v operační paměti počítače. Ovšem výhody univerzální znakové sady drtivě převažují, což je vidět mj. na tom, že starší osmibitové znakové sady jsou dnes definované jako podmnožiny Unicode.\n\nKe konci osmdesátých let 20. století vznikla naléhavá potřeba sjednotit různé kódové tabulky znaků pro národní abecedy. Pro český jazyk se používalo nejméně 5 různých kódování (kódování bratří Kamenických, PC Latin 2, Windows-1250, ISO Latin 2, KOI8-CS[2]) a podobná situace byla ve všech jazycích, které nevystačily se základní 7bitovou tabulkou ASCII znaků, což přinášelo problémy při přenosech dat mezi programy a platformami a spolupráci aplikací.\n\nV té době vznikly současně dva projekty pro vytvoření jednotné univerzální kódovací tabulky znaků. Byl to projekt ISO 10646 organizace ISO a projekt Unicode. Norma ISO definuje tzv. UCS – Universal Character Set. Kolem roku 1991 došlo k dohodě a projekty spojily své úsilí na vytvoření jednotné tabulky. Oba projekty stále existují a publikují své standardy samostatně, ale tabulky znaků jsou kompatibilní a jejich rozšiřování je koordinováno.\n\nVšechny verze Unicode od 2.0 výše jsou zpětně kompatibilní, jsou přidávány pouze nové znaky, existující znaky nejsou vyřazovány nebo přejmenovávány. Poslední verze publikovaná v knižní podobě byla verze Unicode 5.2 (ISBN 0-321-48091-0); od verze 6.0 je plný text normy publikován pouze elektronicky; v roce 2012 bylo oznámeno, že od verze 6.1 bude v knižní podobě dostupné pouze jádro normy (v té době čítající 692 stránek) tištěných na žádost.[3] Na rozdíl od předchozích hlavních verzí výtisků normy, tištěná verze nezahrnuje žádné tabulky kódu nebo doplňky standardu. Celý standard, včetně jádra, je volně dostupný na WWW serveru Unicode konsorcia[4].\n\nStandard Unicode se oproti ISO 10646 navíc zabývá implementací algoritmů pro písma psaná zprava doleva (např. arabština), podporou oboustranných textů (jako např. směs hebrejštiny a latinky), algoritmy pro řazení a porovnávání textů.\n\nUnicode Consortium může neustále aktualizovat rozsah znaků v tabulce Unicode vydávaním nových verzí. Proto následující řádky nemusí být v budoucnu stále platné. Unicode verze XX (rok XX?????) nezahrnuje znaky, které neslouží k zápisu textu, jako jsou například taneční či hudební značky. Rovněž se nezabývá variantami zobrazení znaků (ve fontu může být více glyfů pro jeden význam, resp. pro jeden kód dle tabulky) a neobsahuje speciální znaky pro přechodné vkládání textu. Ve znakovém repertoáru Unicode kódování je vyhrazeno 6400 znaků pro potřebu programů, jejichž využití není nijak omezeno.\n\nV souvislosti s jazyky jako je wachánština, jejichž ortografie založené na latince používají několik znaků cyrilice či řeckého písma, byla řešena otázka, zda kvůli nim do Unicode přidat znaky jako latinská delta, latinská théta či latinské jery jako latinské protějšky těchto řeckých a cyrilských písmen.[11] Jeden z názorů na tuto problematiku je, že požadavek, aby jazyk byl zapisován pouze znaky jednoho písma, je umělý, a že v minulosti si různé jazyky půjčovaly písmena i z jiných písem, takže na soubor písmen latinky, cyrilice a řeckého písma může být nahlíženo jako na latinsko-cyrilsko-řecké metapísmo, a tedy pro zápis těchto jazyků používat písmena v Unicode již obsažená místo vytváření nových.[12] I v případě, kdy by tato písmena byla zavedena, lze očekávat, že by i nadále pro zápis byly používány řecké a cyrilské verze těchto písmen, protože latinské verze by byly obsaženy pouze v malém počtu fontů.[11]\n\nTento princip může způsobovat problémy při zpracování čínského, japonského a korejského písma (CJK), které mají společný historický základ, ale mají např. v Číně a Japonsku posunutý význam a odlišný tvar.\n\nUnicode byl původně navrhován jako 16bitová znaková sada, což se později (hlavně s ohledem na čínské znaky) ukázalo jako nedostatečné. Původní rozsah Unicode, tj. prvních 65 536 znaků, které jsou reprezentovatelné pomocí 16 bitů, se označuje jako BMP (Basic Multilingual Plane) – základní vícejazyčná rovina Unicode.\n\nStandard ISO/IEC 10646 oproti Unicode zpočátku používal 31bitové kódování znaků, které umožňuje reprezentaci více než 2 miliard znaků. Toto množství se ukázalo být zbytečně velké, proto bylo v listopadu 2003 v souvislosti se zavedením kódování UTF-16 omezeno na rozsah 0 až 10FFFF16 rozdělený na 17 tak zvaných rovin (anglicky plane) po 65 536 znacích (1000016). Celý rozsah kódů tak lze rozdělit na BMP (Plane 0), Plane 1, Plane 2, ... až Plane 16. Celková kapacita Unicode je tedy 1114112 kódových bodů.\n\nZnaky mimo BMP se v UTF-16 kódují dvojicí speciálních kódů, které se nazývají zástupné nebo náhradní páry (anglicky surrogate pairs, surrogates). Kódy používané pro náhradní páry spadají do BMP a nejsou jim přiřazeny žádné znaky.\n\nKaždý znak má jednoznačný číselný kód a svůj název. Navíc Unicode definuje u každého znaku některé základní vlastnosti, jako např. zda se jedná o písmeno, symbol atd., zda je písmeno velké či malé atp.\n\nTabulka Unicode poskytuje prostor pro 1 114 112 znaků s kódy 016 až 10FFFF16. Tento prostor se dělí na 17 částí, každý o velikosti 216. První část se nazývá Basic Multilingual Plane (BMP) a obsahuje znaky běžně používaných abeced. Původní 16bitový návrh Unicode počítal jen s BMP, následně se ale ukázalo, že pro pokrytí všech používaných abeced to nestačí.\n\nPrvních 128 znaků (tj. sedmibitové kódy) obsahuje znakovou sadu ASCII. Osmibitové kódy (tj. prvních 256 znaků) obsahují znakovou sadu ISO 8859-1 (ISO 8859-1 obsahuje ASCII).\n\nV Unicode jsou také rezervovány místa pro vlastní znaky. Pro tento účel byla v Unicode tabulce rezervována tzv. PUA sekce (Private Use Area), ve které je možné využít volné kódy pro osobní potřebu. Takto je možné, aby tvůrci fontů vkládali k písmu vlastní grafiky jako jsou loga apod.\n\nJe nutné si uvědomit, že v Unicode jsou to trvale volná místa a přináší jen jediný benefit – že nebudou zaměněny za jiný znak. V případě změny za cizí font je obvykle znak zvýrazněný jako chybějící.\n\nPoužití vlastních znaků v textu lze doporučit, jen pokud mají dekorativní funkci. Důvodem je jejich omezení v reprodukování. Vlastní znaky nelze volně přes internetovou síť dále reprodukovat (zobrazit, tisknout, číst hlasem), ale ani vyhledat. Existence těchto znaků může být spojená pouze s konkrétním fontem. Pokud se dokument bude pohybovat ve známém prostředí, např. intranet, tak s vlastním, resp. firemním fontem lze zajistit alespoň vizuální reprodukovatelnost (zobrazení, tisk) těchto znaků.\n\nZajistit správné zobrazení na internetové síti lze pomocí uzavřeného formátu, například v PDF souboru. Tento elektronický dokument by však měl splňovat kritéria nejen pro správné zobrazení, ale také například čitelnost pro vyhledávače, reprodukovatelnost přes screen-readery pro zrakově postižené apod. Zajistěte při použití vlastních znaků, aby nenesly „čtenou“ informaci a měly jen dekorativní funkci.\n\nExistuje několik různých způsobů, jak kódovat řetězce složené ze znaků (kódových bodů) Unicode. Unicode řetězec je sekvencí kódových bodů Unicode. Způsob jak tuto sekvenci uložit do paměti počítače nebo serializovat na disk se označuje jako kódovací znakové schéma (Character Encoding Scheme), které zahrnuje způsob kódování znaků a způsob jejich serializace do sekvence bajtů. Základní kódování Unicode jsou:\n\nKódování UTF-32, UTF-16 a UCS-2 mají každá své varianty podle používaného pořadí bajtů. Buď je napevno stanoveno pořadí little-endian, resp., big-endian, nebo se toto pořadí určuje podle tzv. byte order mark (BOM), speciální značky umístěné na začátku textu.\n\nV kódování UTF-32 (též označováno jako UCS-4) je každý znak reprezentován přímo 32bitovým číslem. Jedná se tedy o principiálně velmi jednoduché kódování (jeho hlavní výhodou je stejná délka všech znaků), které však má poměrně vysoké nároky na paměť. Při serializaci do posloupnosti bajtů se podle endianity rozlišují varianty UTF-32BE (big-endian), UTF-32LE (little-endian) a UTF-32 (nestanoveno, může být určeno pomocí BOM).\n\nV kódování UTF-16 se znaky BMP reprezentují jedním 16bitovým číslem, znaky mimo BMP jsou reprezentovány párem 16bitových čísel (tzv. surrogate pair). Pro surrogate pairs se používají čísla v rozsahu D80016–DFFF16, přičemž odpovídající znaky v BMP (U+D800 – U+DFFF) jsou rezervovány pro tento účel, nemohou se proto v původním textu vyskytnout.\n\nPodle endianity se rozlišují varianty UTF-16BE (big-endian), UTF-16LE (little-endian) a UTF-16 (nestanoveno, může být určeno pomocí BOM). UTF-16 je výrazně úspornější než UTF-32, ale ztrácí výhodu pevné šířky znaku – některé znaky jsou široké 2 bajty, některé 4. Přesto se jedná o kódování používané jako základní ve velkém množství operačních systémů a dalšího softwaru (např. Microsoft Windows, .NET Framework atd.).\n\nUTF-8 kóduje znaky různě dlouhou (1–4 bajty, pro původní 31bitové ISO/IEC 10646 až 6 bajtů) posloupností bajtů podle jejich kódu v Unicode. Znaky ASCII (U+0000 – U+007F) jsou kódovány jedním bajtem, identicky jako v ASCII, znaky v rozsahu U+0080 – U+07FF (kde jsou také všechny znaky s diakritikou používané v české abecedě) jsou kódovány dvěma bajty, znaky U+0800 – U+FFFF (kam patří znak Euro – € – U+20AC) jsou kódovány třemi bajty, znaky mimo BMP jsou kódovány čtyřmi bajty. Znaky s vyššími kódy podle původního návrhu ISO/IEC 10646 by používaly pětibajtové a šestibajtové kódování.\n\nUTF-8 se často používá pro přenos dat, neboť je prostorově úsporné (hlavně pro texty psané latinkou s nevelkým počtem znaků s diakritikou, které obsahují většinu jednobajtových a zbytek dvoubajtových kódů; v nelatinkových písmech je většina textu tvořena dvoubajtovými kódy, písma Dálného východu používají tříbajtové kódy), je odolné proti chybám a zpětně kompatibilní s ASCII. Při jeho zpracování je však nepříjemná nestejná délka znaků.[13] Je preferovaným kódováním v unixových systémech, které používají Unicode.\n\nPoužití BOM jako příznaku endianity je u UTF-8 zbytečné (pořadí bajtů je jednoznačně určeno), BOM však může posloužit pro snadnou detekci, že se jedná o UTF-8.\n\nUTF-8 je popsané v ISO 10646-1:2000 Annex D a také v RFC 3629.\n\nUCS-2 kóduje každý znak z BMP pomocí šestnáctibitového čísla; znaky mimo BMP nelze v UCS-2 reprezentovat, proto je UCS-2 omezená znaková sada, podobně jako ASCII. Při reprezentaci proudem oktetů (bajtů) má varianty big endian a little endian. Proud dat může být zahájen BOM. UCS-2 bylo vyřazeno ze standardu v roce 1996. Výhodou UCS-2 oproti UTF-8 nebo UTF-16 je konstantní délka znaku a snadné zjišťování počtu znaků v řetězci. Obsahuje-li text pouze znaky z BMP, je UCS-2 nerozlišitelné od UTF-16.\n\nZ různých důvodů existují také další, méně často používaná, kódování, jako UTF-7 či CESU-8.\n\nZnakovou sadu Unicode používá většina moderních operačních systémů. Operační systémy Microsoft Windows používají pro vnitřní zápis znaků (např. jména souborů a adresářů v NTFS) od Windows 2000 kódování UTF-16, avšak zároveň se v české mutaci používá kódování CP1250 (historicky) a CP852 (v příkazovém řádku).\n\nVětšina distribucí Linuxu používá buď rovnou UTF-8 nebo umožňuje jeho ruční nastavení.\n\nNěkteré starší aplikace Unicode (dosud) nepodporují. Na druhé straně pro některé systémy je Unicode již jedinou používanou znakovou sadou. Programovací jazyky Java a jazyky podporující Common Language Infrastructure (např. C#) vnitřně používají šestnáctibitovou verzi Unicode a navenek podporují mnoho různých kódování. Též systémy řízení báze dat dnes již často používají Unicode pro uložení znakových údajů. Na Unicode je založen kancelářský balík Microsoft Office od verze 97.\n\nUnicode je znakovou sadou pro HTML dokumenty od verze 4.0 a pro všechny XML dokumenty. Výchozím kódováním je UTF-8, které všechny prohlížeče podporují už delší dobu.\n\nNa rozdíl od dřívějších osmibitových tabulek znaků jako je bratří Kamenických, Latin 2, Windows-1250 či ISO-8859-2 lze všechny znaky zobrazit zároveň; v jednom textu lze tedy kombinovat např. češtinu (latinka), ruštinu (cyrilice) a řečtinu (alfabeta). Pro reprezentaci českých znaků existují dva způsoby. Buď lze použít \"předkomponovaný\" (precomposed) znak, tedy např. pro dlouhé A kód U+00C1, nebo je možné tento znak složit jako sekvneci <U+0041, U+0301>. Tedy ze znaku A (kód U+0041) a znaku COMBINING ACUTE ACCENT (kombinační dlouhý přízvuk, kód U+0301). Kombinační diakritický znak se vkládá za znak, který modifikuje. Z toho vyplývají možné problémy při porovnávání řetězců, tzn. řetězce je třeba před porovnáváním normalizovat.\n\nZákladní formy normalizace jsou: NFD (kanonická dekompozice), NFC (kanonická kompozice), NFKD (kompatibilní dekompozice), NFKC (kompatibilní kompozice). Více viz přílohu Unicode Normalization Forms standardu Unicode.\n\nTabulka českých diakritických znamének[pozn. 1]\n\nV tomto článku byl použit překlad textu z článku Unicode na anglické Wikipedii.\n\n",
		"url": "https://cs.wikipedia.org/wiki/Unicode"
	},
	"de": {
		"content": "Der Unicode-Standard (Aussprachen: amerikanisches Englisch [], britisches Englisch []; dt. []) legt fest, wie Schrift elektronisch gespeichert wird, z. B. auf einem Computer oder Telefon. Der durch den Standard festgelegte Zeichensatz enthält 149.813 Zeichen in der Version Unicode 15.1.[1] Das Unicode-Konsortium hat dazu 161 moderne und alte Schriften berücksichtigt, wie auch Symbole, Emojis und nicht druckbare Steuerzeichen. Die ISO bezeichnet den Standard als ISO 10646 und den Zeichensatz als Universal Coded Character Set (UCS).\n\nUnicode muss für die Verarbeitung im Computer in Nullen und Einsen (Binärcode) übersetzt werden. Eine solche Umwandlung wird als Unicode Transformation Format (UTF) bezeichnet. Durchgesetzt hat sich dabei UTF-8. In einigen Fällen ist auch noch UTF-16 anzutreffen, speziell bei Betriebssystemen und Programmiersprachen, für die eine Verwendung von UTF-8 nicht so einfach ist.\n\nHerkömmliche Computer-Zeichensätze umfassen nur einen begrenzten Vorrat an Zeichen, bei westlichen Zeichenkodierungen liegt diese Grenze meistens bei 128 (7 Bit) Codepositionen – wie bei dem sehr bekannten ASCII-Standard – oder 256 (8 Bit) Positionen, wie z. B. bei ISO 8859-1 (auch als Latin-1 bekannt) oder EBCDIC. Davon sind nach Abzug der Steuerzeichen 95 Elemente bei ASCII und 191 Elemente bei den 8-Bit ISO-Zeichensätzen als Schrift- und Sonderzeichen darstellbar. Diese Zeichenkodierungen erlauben die gleichzeitige Darstellung nur weniger Sprachen im selben Text, wenn man sich nicht damit behilft, in einem Text verschiedene Schriften mit unterschiedlichen Zeichensätzen zu verwenden. Das behinderte den internationalen Datenaustausch in den 1980er und 1990er Jahren erheblich.\n\nISO 2022[2] war ein erster Versuch, mehrere Sprachen mit nur einer Zeichenkodierung darstellen zu können. Die Kodierung benutzt Escape-Sequenzen, um zwischen verschiedenen Zeichensätzen (z. B. zwischen Latin-1 und Latin-2) wechseln zu können. Das System setzte sich jedoch nur in Ostasien durch.[3]\n\nJoseph D. Becker von Xerox schrieb 1988 den ersten Entwurf für einen universalen Zeichensatz. Dieser 16-Bit-Zeichensatz sollte nach den ursprünglichen Plänen lediglich die Zeichen moderner Sprachen kodieren:\n\n“Unicode gives higher priority to ensuring utility for the future than to preserving past antiquities. Unicode aims in the first instance at the characters published in modern text (e.g. in the union of all newspapers and magazines printed in the world in 1988), whose number is undoubtedly far below 214 = 16,384. Beyond those modern-use characters, all others may be defined to be obsolete or rare, these are better candidates for private-use registration than for congesting the public list of generally-useful Unicodes.”\n\n„Unicode legt größeren Wert darauf, die Verwendbarkeit für die Zukunft sicherzustellen, als vergangene Altertümlichkeiten zu erhalten. Unicode zielt in erster Linie auf alle Zeichen, die in modernen Texten veröffentlicht werden (etwa in allen Zeitungen und Zeitschriften der Welt des Jahres 1988), deren Anzahl zweifelsfrei weit unter 214 = 16.384 liegt. Weitere Zeichen, die über diese heutigen Zeichen hinausgehen, können als veraltet oder selten erachtet werden, diese sollten besser über einen privaten Modus registriert werden, statt die öffentliche Liste der allgemein nützlichen Unicodes zu überfüllen.“\n\nIm Oktober 1991[5] wurde nach mehrjähriger Entwicklungszeit die Version 1.0.0 des Unicode-Standards veröffentlicht, die damals nur die europäischen, nahöstlichen und indischen Schriften kodierte.[6] Erst acht Monate später, nachdem die Han-Vereinheitlichung abgeschlossen war, erschien Version 1.0.1, die erstmals ostasiatische Zeichen kodierte. Mit der Veröffentlichung von Unicode 2.0 im Juli 1996 wurde der Standard von ursprünglich 65.536 auf die heutigen 1.114.112 Codepunkte, von U+0000 bis U+10FFFF erweitert.[7]\n\nDie Veröffentlichung neuer Versionen zieht sich teilweise über einen längeren Zeitraum hin, sodass zum Veröffentlichungszeitpunkt zunächst nur die Zeichentabellen und einzelne Teile der Spezifikation fertig sind, während die endgültige Veröffentlichung der Hauptspezifikation erst einige Zeit später erfolgt.\n\nDas Unicode-Konsortium stellt mehrere Dokumente zur Unterstützung von Unicode bereit. Neben dem eigentlichen Zeichensatz sind dies weitere Dokumente, die zwar nicht zwingend notwendig, aber dennoch hilfreich zur Interpretation des Unicode-Standards sind.\n\nIm Gegensatz zu früheren Zeichenkodierungen, die meist nur ein bestimmtes Schriftsystem kodierten, ist es das Ziel von Unicode, alle in Gebrauch befindlichen Schriftsysteme und Zeichen zu kodieren.[37] Der Zeichenumfang ist dazu in 17 Ebenen (englisch planes) gegliedert, welche jeweils 216 = 65.536 Codepoints umfassen.[38] Sechs dieser Ebenen werden bereits verwendet, die restlichen sind für spätere Nutzung reserviert:\n\nInnerhalb dieser Ebenen werden zusammengehörende Zeichen in Blöcken (engl. blocks) zusammengefasst. Meist behandelt ein Unicodeblock ein Schriftsystem, aus historischen Gründen hat sich allerdings ein gewisses Maß an Fragmentierung eingestellt. Oft wurden später noch Zeichen hinzugefügt und in anderen Blöcken als Ergänzung untergebracht.[40]\n\nJedes im Unicode-Standard codierte elementare Zeichen ist einem Codepunkt (engl. code points) zugeordnet. Diese werden üblicherweise hexadezimal (mindestens vierstellig, d. h. ggf. mit führenden Nullen) und mit einem vorangestellten U+ dargestellt, z. B. U+00DF für das ß.[42]\n\nDer gesamte vom Unicode-Standard beschriebene Bereich umfasst 1.114.112 Codepunkte (U+0000 … U+10FFFF, 17 Ebenen zu je 216, d. h. 65536 Zeichen). Davon lässt der Standard jedoch einige Bereiche zur Zeichenkodierung nicht zu:\n\nSomit stehen für die Zeichencodierung insgesamt 1.111.998 Codepunkte zur Verfügung. Die Anzahl der tatsächlich zugewiesenen Codepunkte ist jedoch deutlich geringer; eine Übersicht, wie viele Codepunkte in den verschiedenen Versionen jeweils zugewiesen sind und wofür sie genutzt werden, bieten die Tabellen D-2 und D-3 im Anhang D des Unicode-Standards.[43]\n\nSpezielle Bereiche sind für private Nutzung reserviert, d. h. in diesen werden niemals Codepunkte für in Unicode standardisierte Zeichen zugewiesen. Diese können für privat definierte Zeichen verwendet werden, die zwischen den Erzeugern und Verwendern der Texte, die sie enthalten, individuell abgesprochen sein müssen. Diese Bereiche sind:\n\nEs haben sich für verschiedene Anwendungen spezielle Konventionen entwickelt, die speziell für den PUA-Bereich der BMP Zeichenbelegungen vorgeben. Zum einen finden sich hier häufig precomposed characters aus Grundzeichen und diakritischen Zeichen, da in vielen (speziell älteren) Software-Anwendungen nicht davon ausgegangen werden kann, dass solche Zeichen gemäß den Unicode-Regeln bei Eingabe als Folge aus Grundzeichen und diakritischem Zeichen korrekt dargestellt werden. Zum anderen finden sich Zeichen, die nicht den Regeln für eine Aufnahme in Unicode entsprechen, oder deren Beantragung zur Aufnahme in Unicode aus anderen Gründen erfolglos war oder unterblieb. So findet sich in vielen Fonts auf der Position U+F000 ein Hersteller-Logo (Logos werden in Unicode prinzipiell nicht codiert).\n\nQuellen für PUA-Zeichen sind z. B.:\n\nNeben dem eigentlichen Zeichensatz, der jedem Zeichen eine Nummer zuordnet, definiert Unicode auch mehrere Verfahren, um Zeichen in Computern zu speichern. Sie werden Unicode Transformation Format (kurz UTF) genannt. Die verbreitetsten Varianten sind:\n\nNeben den von Unicode standardisierten Verfahren gibt es noch:\n\nViele Zeichen, die im Unicode-Standard enthalten sind, sind sogenannte Kompatibilitätszeichen, die aus Unicode-Sicht bereits mit anderen in Unicode kodierten Zeichen bzw. Zeichensequenzen dargestellt werden können, so z. B. die deutschen Umlaute, die theoretisch mit einer Sequenz aus dem Basisbuchstaben und einem kombinierenden Trema (horizontaler Doppelpunkt) dargestellt werden können. Bei der Unicode-Normalisierung werden die Kompatibilitätszeichen automatisch durch die in Unicode vorgesehenen Sequenzen ersetzt. Dies erleichtert die Verarbeitung von Unicode-Texten erheblich, da so nur eine mögliche Kombination für ein bestimmtes Zeichen steht, und nicht mehrere verschiedene.\n\nFür viele Schriftsysteme sind die Zeichen in Unicode nicht in einer Reihenfolge codiert, die einer bei den Anwendern dieses Schriftsystems üblichen Sortierung entspricht. Deshalb kann bei einer Sortierung z. B. in einer Datenbankanwendung üblicherweise nicht die Reihenfolge der Codepunkte verwendet werden. Außerdem sind die Sortierungen in vielen Schriftsystemen von komplexen, kontextabhängigen Regelungen geprägt. Hier definiert der Unicode Collation Algorithm, wie Zeichenfolgen innerhalb eines bestimmten Schriftsystems oder auch schriftsystemübergreifend sortiert werden können.\n\nIn vielen Fällen ist jedoch die tatsächlich anzuwendende Reihenfolge von anderen Faktoren (z. B. der verwendeten Sprache) abhängig (z. B. sortiert „ä“ im Deutschen anwendungsabhängig wie „ae“ oder „a“, im Schwedischen jedoch hinter „z“ und „å“), sodass der Unicode-Sortierungsalgorithmus dann anzuwenden ist, wenn die Sortierung nicht von spezielleren Rahmenbedingungen bestimmt wird.\n\nDas gemeinnützige Unicode-Konsortium wurde 1991 gegründet und ist für den Industriestandard Unicode verantwortlich. Von der ISO (Internationale Organisation für Normung) wird in Zusammenarbeit mit IEC die internationale Norm ISO 10646 herausgegeben. Beide Institutionen arbeiten eng zusammen. Seit 1993 sind Unicode und ISO 10646 bezüglich der Zeichenkodierung praktisch identisch. Während ISO 10646 lediglich die eigentliche Zeichenkodierung festlegt, gehört zum Unicode ein umfassendes Regelwerk, das unter anderem für alle Zeichen weitere zur konkreten Anwendung wichtige Eigenschaften (sogenannte Properties) eindeutig festlegt wie Sortierreihenfolge, Leserichtung und Regeln für das Kombinieren von Zeichen.[51]\n\nSeit einiger Zeit entspricht der Codeumfang von ISO 10646 exakt dem von Unicode, da auch dort der Codebereich auf 17 Ebenen, darstellbar mit 21 Bit, beschränkt wurde.[52]\n\nGegenüber anderen Normen gibt es bei Unicode die Besonderheit, dass einmal kodierte Zeichen niemals wieder entfernt werden, um die Langlebigkeit digitaler Daten zu gewährleisten.[53] Sollte sich die Normierung eines Zeichens nachträglich als Fehler erweisen, wird allenfalls von seiner Verwendung abgeraten. Daher bedarf die Aufnahme eines Zeichens in den Standard einer äußerst sorgfältigen Prüfung, die sich über Jahre hinziehen kann.\n\nIm Unicode werden lediglich „abstrakte Zeichen“ (englisch: characters) kodiert, nicht dagegen die grafische Darstellung (Glyphen) dieser Zeichen, die von Schriftart zu Schriftart extrem unterschiedlich ausfallen kann, beim lateinischen Alphabet etwa in Form der Antiqua, Fraktur, der irischen Schrift oder der verschiedenen Handschriften.[54] Für Glyphenvarianten, deren Normierung als sinnvoll und notwendig nachgewiesen wird, sind dabei allerdings vorsorglich 256 „Variation Selectors“ reserviert, die ggf. dem eigentlichen Code nachgestellt werden können. In vielen Schriftsystemen können Zeichen außerdem je nach Position unterschiedliche Formen annehmen oder Ligaturen bilden. Von Ausnahmen abgesehen (z. B. Arabisch) werden solche Varianten ebenfalls nicht in den Unicode-Standard übernommen, sondern es wird eine sogenannte Smartfont-Technik wie OpenType vorausgesetzt, die die Formen angemessen ersetzen kann.\n\nAndererseits werden identische Glyphen, wenn sie verschiedene Bedeutungen haben, auch mehrfach kodiert, etwa die Glyphen А, В, Е, K, М, Н, О, Р, Т und Х, die – mit zum Teil unterschiedlicher Bedeutung – sowohl im lateinischen als auch im griechischen und kyrillischen Alphabet vorkommen.\n\nIn Grenzfällen wird hart um die Entscheidung gerungen, ob es sich um Glyphenvarianten oder tatsächlich unterschiedliche, einer eigenen Kodierung würdige Zeichen (Grapheme) handelt. Beispielsweise sind nicht wenige Fachleute der Meinung, man könne das phönizische Alphabet als Glyphenvarianten des hebräischen Alphabets betrachten, da der gesamte Zeichenvorrat des Phönizischen dort eindeutige Entsprechungen hat und auch beide Sprachen sehr eng miteinander verwandt sind. Letztlich durchgesetzt hat sich allerdings schließlich die Auffassung, es handele sich um separate Zeichensysteme, in der Unicode-Terminologie „scripts“ genannt.[55]\n\nAnders verhält es sich bei CJK (Chinesisch, Japanisch und Koreanisch): Hier haben sich in den letzten Jahrhunderten die Formen vieler gleichbedeutender Schriftzeichen auseinanderentwickelt. Dennoch teilen sich die sprachspezifischen Glyphen dieselben Codes im Unicode (mit Ausnahme einiger Zeichen aus Kompatibilitätsgründen). In der Praxis werden hier überwiegend sprachspezifische Schriftarten verwendet, wodurch der Platzbedarf der Schriften zusammen hoch ist. Die einheitliche Kodierung der CJK-Schriftzeichen (Han Unification) war eine der wichtigsten und umfangreichsten Vorarbeiten für die Entwicklung von Unicode. Besonders in Japan ist sie durchaus umstritten.\n\nAls der Grundstein für Unicode gelegt wurde, musste berücksichtigt werden, dass bereits eine Vielzahl unterschiedlicher Kodierungen im Einsatz waren. Unicode-basierte Systeme sollten herkömmlich kodierte Daten mit geringem Aufwand handhaben können. Dazu wurde für die unteren 256 Zeichen die weit verbreitete ISO-8859-1-Kodierung (Latin1) ebenso wie die Kodierungsarten verschiedener nationaler Normen beibehalten, z. B. TIS-620 für Thailändisch (fast identisch mit ISO 8859-11) oder ISCII für indische Schriften, die in der ursprünglichen Reihenfolge lediglich in höhere Bereiche verschoben wurden.\n\nJedes Zeichen maßgeblicher überkommener Kodierungen wurde in den Standard übernommen, auch wenn es den normalerweise angelegten Maßstäben nicht gerecht wird. Hierbei handelt es sich zu einem großen Teil um Zeichen, die aus zwei oder mehr Zeichen zusammengesetzt sind, wie Buchstaben mit diakritischen Zeichen. Im übrigen verfügt auch heute noch ein großer Teil der Software nicht über die Möglichkeit, Zeichen mit Diakritika ordentlich zusammenzusetzen. Die exakte Festlegung von äquivalenten Kodierungen ist Teil des zum Unicode gehörenden umfangreichen Regelwerks.\n\nDarüber hinaus gibt es viele Unicode-Zeichen, denen keine Glyphe zugeordnet ist und die trotzdem als „characters“ behandelt werden. So sind neben Steuerzeichen wie dem Tabulatorzeichen (U+0009), dem Zeilenvorschub (U+000A) usw. allein 19 verschiedene Zeichen explizit als Leerzeichen definiert, sogar solche ohne Breite, die u. a. für Sprachen wie Thai, die ohne Wortzwischenraum geschrieben werden, als Worttrenner eingesetzt werden. Für bidirektionalen Text, z. B. Arabisch mit Lateinisch, sind sieben Formatierungszeichen kodiert. Darüber hinaus gibt es weitere unsichtbare Zeichen, die nur unter bestimmten Umständen ausgewertet werden sollen, etwa der Combining Grapheme Joiner.\n\nDie DIN 91379 definiert eine Teilmenge der Unicode-Buchstaben, Sonderzeichen und Sequenzen von Grundbuchstaben und diakritischen Zeichen um eine korrekte Darstellung von Namen zu gewährleisten und den Datenaustausch in Europa zu vereinfachen. Sie unterstützt alle Amtssprachen der Länder der Europäischen Union, Islands, Liechtensteins, Norwegens und der Schweiz sowie die deutschen Minderheitensprachen. Um die Transliteration von Namen in anderen Schriftsystemen in die lateinische Schrift gemäß den einschlägigen ISO-Normen zu ermöglichen, werden alle notwendigen Kombinationen von Grundbuchstaben und diakritischen Zeichen bereitgestellt.[56]\n\nUnter Windows (ab Windows 2000) kann in einigen Programmen (genauer in RichEdit-Feldern) der Code dezimal als Alt+<dezimales Unicode> (bei eingeschaltetem Num-Lock) auf dem numerischen Tastaturfeld eingegeben werden. Dabei ist jedoch zu beachten, dass Zeichennummern kleiner als 1000 um eine führende Null zu ergänzen sind (z. B. Alt+0234 für Codepoint 23410 [ê]). Diese Maßnahme ist notwendig, da die (immer noch in Windows verfügbare) Eingabemethode Alt+<ein- bis dreistellige dezimale Zeichennummer ohne führende Null> bereits in MS-DOS-Zeiten genutzt wurde, um die Zeichen der Codepage 850 (vor allem bei früheren MS-DOS-Versionen auch Codepage 437) einzugeben.\n\nEine weitere Eingabemethode setzt voraus, dass in der Registrierungsdatenbank im Schlüssel HKEY_CURRENT_USER\\Control Panel\\Input Method ein Eintrag (Wert) vom Typ REG_SZ („Zeichenfolge“) namens EnableHexNumpad existiert und ihm der Wert (das Datum) 1 zugewiesen ist. Nach dem Editieren der Registry müssen Benutzer sich unter Windows 8.1, Windows 8, Windows 7 und Vista vom Windows-Benutzerkonto ab- und wieder anmelden, bei früheren Windows-Versionen ist ein Neustart des Rechners notwendig, damit die Änderungen in der Registry wirksam werden. Danach können Unicode-Zeichen wie folgt eingegeben werden: Zuerst die (linke) Alt-Taste drücken und halten, dann auf dem Ziffernblock die Plus-Taste drücken und wieder loslassen und anschließend den hexadezimalen Code des Zeichens eingeben, wobei für Ziffern der Ziffernblock verwendet werden muss. Abschließend die Alt-Taste wieder loslassen.\n\nZwar funktioniert diese Eingabemethode prinzipiell in jedem Eingabefeld jedes Windows-Programms, allerdings kann es vorkommen, dass Schnellzugriffstasten für Menüfunktionen die Eingabe hexadezimaler Codepunkte verhindern: Will man beispielsweise den Buchstaben Ø (U+00D8) eingeben, so führt die Kombination Alt+D in vielen Programmen dazu, dass stattdessen das Menü Datei geöffnet wird.\n\nEin weiterer Nachteil besteht darin, dass Windows hier die explizite Angabe der (intern in Windows verwendeten) UTF-16-Codierung statt der Unicode-Kodierung selbst verlangt[57] und daher nur die Eingabe vierstelliger Codewerte zulässt; für Zeichen, die oberhalb der BMP liegen und über Codepunkte mit fünf- oder sechsstelliger Hexadezimaldarstellung verfügen, sind stattdessen sogenannte Surrogate Pairs zu verwenden, bei denen ein fünf- oder sechsstelliger Codepunkt auf zwei je vierstellige Ersatzcodepunkte abgebildet wird. So ist etwa der Violinschlüssel 𝄞 (U+1D11E) als hexadezimales UTF-16-Wertpaar D834 und DD1E einzugeben; eine direkte Eingabe fünf- oder sechsstelliger Codepunkte ist hier also nicht möglich.\n\nBei Apple macOS muss die Eingabe von Unicode-Zeichen als Sonderfall zuerst über die Systemeinstellungen „Tastatur“ aktiviert werden.[58] Hierzu ist im Dialog Registerkarte „Eingabequellen“ über das Plus-Symbol die „Unicode-Hex-Eingabe“ hinzuzufügen. Diese befindet sich unter dem Oberpunkt „Andere“. Danach kann der Unicode-Wert bei gedrückter ⌥Option-Taste mit dem vierstelligen Hex-Code des Unicode-Zeichens eingegeben werden; sollte der Hexcode kleiner als vierstellig sein, so müssen führende Nullen eingegeben werden.[58] Sollte der Hexcode fünfstellig sein, so ist keine unmittelbare Eingabe per Tastatur möglich und es muss über den Dialog „Zeichenübersicht“ ausgewählt werden.[59] Wenn die Unicode-Hex-Eingabe aktiviert ist, dann liegt keine deutschsprachige Tastaturbelegung vor (u. a. für Umlaute), so dass zwischen beiden Tastatur-Modi gewechselt werden muss. Der jeweilige Status der Tastaturbelegung lässt sich per Zusatzoption in der Menüzeile einblenden.[59]\n\nUnter Microsoft Office (ab Office XP) kann Unicode auch hexadezimal eingegeben werden, indem im Dokument <Unicode> oder U+<Unicode> eingetippt wird und anschließend die Tastenkombination Alt+c, bzw. in Dialogfeldern Alt+x, gedrückt wird. Diese Tastenkombination kann auch benutzt werden, um den Code des vor dem Cursor stehenden Zeichens anzuzeigen.[60] (LibreOffice hat eine ähnliche Funktion mit der Tastenkombination Alt+c oder Alt+x.[61]) Eine alternative Möglichkeit, welche auch in älteren Versionen funktioniert, ist, mit „Einfügen“ – „Sonderzeichen“ eine Tabelle mit Unicode-Zeichen aufzurufen, darin mit dem Cursor ein gewünschtes auszusuchen und in den Text einzufügen. Das Programm ermöglicht auch, für häufiger benötigte Zeichen Makros festzulegen, die dann mit einer Tastenkombination abgerufen werden können.\n\nGTK, Qt und alle darauf basierenden Programme und Umgebungen (wie beispielsweise die Desktop-Umgebung Gnome) unterstützen die Eingabe über die Kombination Strg+Umschalttaste bzw. in neueren Versionen Strg+U bzw. Strg+Umschalttaste+u. Nach dem Drücken der Tasten erscheint ein unterstrichenes kleines u. Danach kann der Unicode in hexadezimaler Form eingegeben werden und wird auch unterstrichen, damit man erkennen kann, was zum Unicode gehört. Nach einem Druck der Leer- oder Eingabetaste erscheint dann das entsprechende Zeichen. In Desktop-Umgebungen, welche nicht auf GTK basieren, beispielsweise KDE, wird diese Funktionalität durch Installation des IBus-Frameworks ermöglicht.\n\nIm Texteditor Vim können Unicode-Zeichen mit Strg+v, gefolgt von der Taste u und dem Unicode in hexadezimaler Form, eingegeben werden.\n\nSeit Windows NT 4.0 ist das Programm charmap.exe, genannt Zeichentabelle, in Windows integriert. Mit diesem Programm ist über eine grafische Benutzeroberfläche möglich, Unicode-Zeichen einzufügen. Außerdem bietet es ein Eingabefeld für den Hexadezimalcode.\n\nUnter macOS steht unter Einfügen → Sonderzeichen ebenfalls eine systemweite Zeichenpalette bereit.\n\nDie freien Programme gucharmap (für Windows und Linux/Unix) und kcharselect (für Linux/UNIX) stellen den Unicode-Zeichensatz auf dem Bildschirm dar und bieten zusätzliche Informationen zu den einzelnen Zeichen.\n\nHTML und XML unterstützen Unicode mit Zeichencodes, die unabhängig vom eingestellten Zeichensatz das Unicode-Zeichen darstellen. Die Notation lautet &#0000; für dezimale Notation bzw. &#x0000; für hexadezimale Notation, wobei das 0000 die Unicode-Nummer des Zeichens darstellt. Für bestimmte Zeichen sind auch benannte Zeichen (engl. named entities) definiert, so z. B. stellt &auml; das ä dar,[62] das gilt allerdings nur für HTML; XML und das davon abgeleitete XHTML definieren benannte Notationen nur für die Zeichen, die bei normalem Gebrauch als Teile der Auszeichnungssprache interpretiert würden, also < als &lt;, > als &gt;, & als &amp; und \" als &quot;.\n\nUnicode wird vor allem aus den Reihen der Wissenschaftler und in ostasiatischen Ländern kritisiert. Einer der Kritikpunkte ist hierbei die Han-Vereinheitlichung; aus ostasiatischer Sicht werden bei diesem Vorgehen Schriftzeichen verschiedener nicht verwandter Sprachen vereinigt.[63] Unter anderem wird kritisiert, dass antike Texte in Unicode aufgrund dieser Vereinheitlichung ähnlicher CJK-Schriftzeichen nicht originalgetreu wiedergegeben werden können.[64] Aufgrund dessen wurden in Japan zahlreiche Alternativen zu Unicode entwickelt, wie etwa der Mojikyō-Standard.\n\nDie Kodierung der thailändischen Schrift wird teilweise kritisiert, weil sie anders als alle anderen Schriftsysteme in Unicode nicht auf logischer, sondern visueller Reihenfolge basiert, was unter anderem die Sortierung thailändischer Wörter erheblich erschwert.[63] Die Unicode-Kodierung basiert auf dem thailändischen Standard TIS-620, der ebenfalls die visuelle Reihenfolge verwendet.[65] Umgekehrt wird die Kodierung der indischen Schriften manchmal als „zu kompliziert“ bezeichnet, vor allem von Vertretern der Tamil-Schrift. Das Modell separater Konsonanten- und Vokalzeichen, welches Unicode vom indischen Standard ISCII übernommen hat,[66] wird von jenen abgelehnt, die separate Codepunkte für alle möglichen Konsonant-Vokal-Verbindungen bevorzugen.[67] Die Regierung der Volksrepublik China machte einen ähnlichen Vorschlag, die tibetische Schrift als Silbenfolgen anstatt als einzelne Konsonanten und Vokale zu kodieren.[68]\n\nOb das entsprechende Unicode-Zeichen auch tatsächlich auf dem Bildschirm erscheint, hängt davon ab, ob die verwendete Schriftart eine Glyphe für das gewünschte Zeichen (also eine Grafik für die gewünschte Zeichennummer) enthält. Oftmals, z. B. unter Windows, wird, falls die verwendete Schrift ein Zeichen nicht enthält, nach Möglichkeit ein Zeichen aus einer anderen Schrift eingefügt.\n\nMittlerweile hat der Coderaum von Unicode/ISO einen Umfang angenommen (mehr als 100.000 Schriftzeichen), der sich nicht mehr vollständig in einer Schriftdatei unterbringen lässt. Die heute gängigsten Schriftdateiformate, TrueType und OpenType, können maximal 65.536 Glyphen enthalten. Unicode/ISO-Konformität einer Schrift bedeutet also nicht, dass der komplette Zeichensatz enthalten ist, sondern lediglich, dass die darin enthaltenen Zeichen normgerecht kodiert sind. In der Publikation »decodeunicode«, die alle Zeichen vorstellt, werden insgesamt 66 Fonts genannt, aus denen die Zeichentabellen zusammengesetzt sind.\n\nEine Ersatzschriftart dient der Ersatzdarstellung für Zeichen, für die kein Font mit korrekter Darstellung zur Verfügung steht.\n\nHier gibt z. B. folgende Fonts:\n\nBasis-Lateinisch • Lateinisch-1, Ergänzung • Lateinisch, erw.-A • Lateinisch, erw.-B • IPA-Erweiterungen • Spacing Modifier Letters • Kombinierende diakritische Zeichen • Griechisch und Koptisch • Kyrillisch • Kyrillisch, Ergänzung • Armenisch • Hebräisch • Arabisch • Syrisch • Arabisch, Ergänzung • Thaana • N’Ko • Samaritanisch • Mandäisch • Syrisch, Ergänzung • Arabisch, erw.-B • Arabisch, erw.-A • Devanagari • Bengalisch • Gurmukhi • Gujarati • Oriya • Tamilisch • Telugu • Kannada • Malayalam • Singhalesisch • Thailändisch • Laotisch • Tibetisch • Birmanisch • Georgisch • Hangeul-Jamo • Äthiopisch • Äthiopisch, Zusatz • Cherokee • Vereinh. Silbenz. kanad. Ureinw. • Ogam • Runen • Tagalog • Hanunóo • Buid • Tagbanuwa • Khmer • Mongolisch • Vereinh. Silbenz. kanad. Ureinw., erw. • Limbu • Tai Le • Neu-Tai-Lue • Khmer-Symbole • Buginesisch • Lanna • Kombinierende diakritische Zeichen, erw. • Balinesisch • Sundanesisch • Batak • Lepcha • Ol Chiki • Kyrillisch, erw.-C • Georgisch, erweitert • Sundanesisch, Ergänzung • Vedische Erweiterungen • Phonetische Erweiterungen • Phonetische Erweiterungen, Ergänzung • Kombinierende diakritische Zeichen, Ergänzung • Lateinisch, weiterer Zusatz • Griechisch, Zusatz • Allgemeine Interpunktion • Hoch- und tiefgestellte Zeichen • Währungszeichen • Kombinierende diakritische Zeichen für Symbole • Buchstabenähnliche Symbole • Zahlzeichen • Pfeile • Mathematische Operatoren • Verschiedene technische Zeichen • Symbole für Steuerzeichen • Optische Zeichenerkennung • Umschlossene alphanum. Zeichen • Rahmenzeichnung • Blockelemente • Geometrische Formen • Verschiedene Symbole • Dingbats • Verschiedene mathem. Symbole-A • Zusätzliche Pfeile-A • Braille-Zeichen • Zusätzliche Pfeile-B • Verschiedene mathem. Symbole-B • Zusätzliche mathem. Operatoren • Verschiedene Symbole und Pfeile • Glagolitisch • Lateinisch, erw.-C • Koptisch • Georgisch, Ergänzung • Tifinagh • Äthiopisch, erweitert • Kyrillisch, erw.-A • Zusätzliche Interpunktion • CJK-Radikale, Ergänzung • Kangxi-Radikale • Ideographische Beschreibungszeichen • CJK-Symbole und -Interpunktion • Hiragana • Katakana • Bopomofo • Hangeul-Jamo, Kompatibilität • Kanbun • Bopomofo, erweitert • CJK-Striche • Katakana, Phonetische Erweiterungen • Umschlossene CJK-Zeichen und -Monate • CJK-Kompatibilität • Vereinh. CJK-Ideogramme, Erw. A • I-Ging-Hexagramme • Vereinh. CJK-Ideogramme • Yi-Silbenzeichen • Yi-Radikale • Lisu • Vai • Kyrillisch, erw.-B • Bamum • Modifizierende Tonzeichen • Lateinisch, erw.-D • Syloti Nagri • Allgemeine indische Ziffern • Phagspa • Saurashtra • Devanagari, erw. • Kayah Li • Rejang • Hangeul-Jamo, erw.-A • Javanisch • Birmanisch, erw.-B • Cham • Birmanisch, erw.-A • Tai Viet • Meitei-Mayek, Erw. • Äthiopisch, erw.-A • Lateinisch, erw.-E • Cherokee, Zusatz • Meitei-Mayek • Hangeul-Silbenzeichen • Hangeul-Jamo, erw.-B • Private Use Zone • CJK-Ideogramme, Kompatibilität • Alphabetische Präsentationsformen • Arabische Präsentationsformen-A • Variantenselektoren • Vertikale Formen • Kombinierende halbe diakritische Zeichen • CJK-Kompatibilitätsformen • Kleine Formvarianten • Arabische Präsentationsformen-B • Halbbreite und vollbreite Formen • Spezielles\n\nLinear-B-Silbenzeichen • Linear-B-Ideogramme • Ägäische Zahlzeichen • Altgriechische Zahlzeichen • Alte Symbole • Diskos von Phaistos • Lykisch • Karisch • Koptische Zahlzeichen • Altitalisch • Gotisch • Altpermisch • Ugaritisch • Altpersisch • Mormonen-Alphabet • Shaw-Alphabet • Osmaniya • Osage • Albanisch • Alwanisch • Vithkuq-Alphabet • Linear A • Lateinisch, erw.-F • Kyprisch • Aramäisch • Palmyrenisch • Nabatäisch • Hatra-Schrift • Phönizisch • Lydisch • Meroitische Hieroglyphen • Meroitisch-demotisch • Kharoshthi • Altsüdarabisch • Altnordarabisch • Manichäisch • Avestisch • Parthisch • Inschriften-Pahlavi • Psalter-Pahlavi • Alttürkisch • Altungarisch • Hanifi Rohingya • Rumi-Ziffern • Jesidisch • Arabisch, erw.-C • Altsogdisch • Sogdisch • Altuigurisch • Choresmisch • Elymäisch • Brahmi • Kaithi • Sorang-Sompeng • Chakma • Mahajani • Sharada • Singhalesische Zahlzeichen • Khojki • Multanisch • Khudabadi • Grantha • Newa • Tirhuta • Siddham • Modi • Mongolisch, Ergänzung • Takri • Ahom • Dogra • Varang Kshiti • Dives Akuru • Nandinagari • Dsanabadsar-Quadratschrift • Sojombo • Vereinh. Silbenz. kanad. Ureinw., erw.-A • Pau Cin Hau • Devanagari, erw.-A • Bhaiksuki • Marchen • Masaram Gondi • Gunjala Gondi • Makassar • Kawi • Lisu, Ergänzung • Tamilisch, Ergänzung • Keilschrift • Keilschrift-Zahlzeichen und -Interpunktion • Frühe Keilschrift • Kypro-minoisch • Ägyptische Hieroglyphen • Ägypt. Hieroglyphen-Steuerzeichen • Anatolische Hieroglyphen • Bamum, Ergänzung • Mro • Tangsa • Bassa Vah • Pahawh Hmong • Medefaidrin • Pollard-Schrift • Ideographische Symbole und Interpunktion • Xixia • Xixia-Komponenten • Kleine Kitan-Schrift • Xixia, Ergänzung • Kana, erw.-B • Kana, Ergänzung • Kana, erw.-A • Kleine Kana, erweitert • Frauenschrift • Duployé-Kurzschrift • Kurzschrift-Steuerzeichen • Snamennyj-Notenschrift • Byzantinische Noten • Notenschrift • Altgriechische Noten • Kaktovik-Zahlzeichen • Maya-Zahlzeichen • Tai-Xuan-Jing-Symbole • Zählstabziffern • Mathem. alphanum. Symbole • SignWriting • Lateinisch, erw.-G • Glagolitisch, Ergänzung • Kyrillisch, erw.-D • Nyiakeng Puachue Hmong • Toto • Wancho • Nag Mundari • Äthiopisch, erw.-B • Mende-Schrift • Adlam • Indische Siyaq-Zahlzeichen • Osmanische Siyaq-Zahlzeichen • Arab. mathem. alphanum. Symbole • Mahjonggsteine • Dominosteine • Spielkarten • Zusätzliche umschlossene alphanum. Zeichen • Zusätzliche umschlossene CJK-Zeichen • Verschiedene piktografische Symbole • Smileys • Ziersymbole • Verkehrs- und Kartensymbole • Alchemistische Symbole • Geometrische Formen, erw. • Zusätzliche Pfeile-C • Zusätzliche piktografische Symbole • Schachsymbole • Piktografische Symbole, erw.-A • Symbole für Retrocomputer\n\nVereinh. CJK-Ideogramme, Erw. B • Vereinh. CJK-Ideogramme, Erw. C • Vereinh. CJK-Ideogramme, Erw. D • Vereinh. CJK-Ideogramme, Erw. E • Vereinh. CJK-Ideogramme, Erw. F • Vereinh. CJK-Ideogramme, Erw. I • CJK-Ideogramme, Kompatibilität, Ergänzung • Vereinh. CJK-Ideogramme, Erw. G • Vereinh. CJK-Ideogramme, Erw. H\n\nTags • Variantenselektoren, Ergänzung • Zusätzlicher Privatnutzungsbereich–A • Zusätzlicher Privatnutzungsbereich–B\n\nAdlam • Ägäische Zahlzeichen • Ägyptische Hieroglyphen • Ägypt. Hieroglyphen-Steuerzeichen • Ahom • Albanisch • Alchemistische Symbole • Allgemeine indische Ziffern • Allgemeine Interpunktion • Alphabetische Präsentationsformen • Alte Symbole • Altgriechische Noten • Altgriechische Zahlzeichen • Altitalisch • Altnordarabisch • Altpermisch • Altpersisch • Altsogdisch • Altsüdarabisch • Alttürkisch • Altuigurisch • Altungarisch • Alwanisch • Anatolische Hieroglyphen • Arabisch • Arabisch, Ergänzung • Arabisch, erw.-A • Arabisch, erw.-B • Arabisch, erw.-C • Arab. mathem. alphanum. Symbole • Arabische Präsentationsformen-A • Arabische Präsentationsformen-B • Aramäisch • Armenisch • Äthiopisch • Äthiopisch, erweitert • Äthiopisch, erw.-A • Äthiopisch, erw.-B • Äthiopisch, Zusatz • Avestisch • Balinesisch • Bamum • Bamum, Ergänzung • Basis-Lateinisch • Bassa Vah • Batak • Bengalisch • Bhaiksuki • Birmanisch • Birmanisch, erw.-A • Birmanisch, erw.-B • Blockelemente • Bopomofo • Bopomofo, erweitert • Brahmi • Braille-Zeichen • Buchstabenähnliche Symbole • Buginesisch • Buid • Byzantinische Noten • Chakma • Cham • Cherokee • Cherokee, Zusatz • Choresmisch • CJK-Ideogramme, Kompatibilität • CJK-Ideogramme, Kompatibilität, Ergänzung • CJK-Kompatibilität • CJK-Kompatibilitätsformen • CJK-Radikale, Ergänzung • CJK-Striche • CJK-Symbole und -Interpunktion • Devanagari • Devanagari, erw. • Devanagari, erw.-A • Dingbats • Diskos von Phaistos • Dives Akuru • Dogra • Dominosteine • Dsanabadsar-Quadratschrift • Duployé-Kurzschrift • Elymäisch • Frauenschrift • Frühe Keilschrift • Geometrische Formen • Geometrische Formen, erw. • Georgisch • Georgisch, Ergänzung • Georgisch, erweitert • Glagolitisch • Glagolitisch, Ergänzung • Gotisch • Grantha • Griechisch und Koptisch • Griechisch, Zusatz • Gujarati • Gunjala Gondi • Gurmukhi • Halbbreite und vollbreite Formen • Hangeul-Jamo • Hangeul-Jamo, erw.-A • Hangeul-Jamo, erw.-B • Hangeul-Jamo, Kompatibilität • Hangeul-Silbenzeichen • Hanifi Rohingya • Hanunóo • Hatra-Schrift • Hebräisch • Hiragana • Hoch- und tiefgestellte Zeichen • Ideographische Beschreibungszeichen • Ideographische Symbole und Interpunktion • I-Ging-Hexagramme • Indische Siyaq-Zahlzeichen • Inschriften-Pahlavi • IPA-Erweiterungen • Javanisch • Jesidisch • Kaithi • Kaktovik-Zahlzeichen • Kana, Ergänzung • Kana, erw.-A • Kana, erw.-B • Kanbun • Kangxi-Radikale • Kannada • Karisch • Katakana • Katakana, Phonetische Erweiterungen • Kawi • Kayah Li • Keilschrift • Keilschrift-Zahlzeichen und -Interpunktion • Kharoshthi • Khmer • Khmer-Symbole • Khojki • Khudabadi • Kleine Formvarianten • Kleine Kana, erweitert • Kleine Kitan-Schrift • Kombinierende diakritische Zeichen für Symbole • Kombinierende diakritische Zeichen • Kombinierende diakritische Zeichen, Ergänzung • Kombinierende diakritische Zeichen, erw. • Kombinierende halbe diakritische Zeichen • Koptisch • Koptische Zahlzeichen • Kurzschrift-Steuerzeichen • Kyprisch • Kypro-minoisch • Kyrillisch • Kyrillisch, Ergänzung • Kyrillisch, erw.-A • Kyrillisch, erw.-B • Kyrillisch, erw.-C • Kyrillisch, erw.-D • Lanna • Laotisch • Lateinisch, erw.-A • Lateinisch, erw.-B • Lateinisch, erw.-C • Lateinisch, erw.-D • Lateinisch, erw.-E • Lateinisch, erw.-F • Lateinisch, erw.-G • Lateinisch, weiterer Zusatz • Lateinisch-1, Ergänzung • Lepcha • Limbu • Linear A • Linear-B-Ideogramme • Linear-B-Silbenzeichen • Lisu • Lisu, Ergänzung • Lydisch • Lykisch • Mahajani • Mahjonggsteine • Makassar • Malayalam • Mandäisch • Manichäisch • Marchen • Masaram Gondi • Mathem. alphanum. Symbole • Mathematische Operatoren • Maya-Zahlzeichen • Medefaidrin • Meitei-Mayek • Meitei-Mayek, Erw. • Mende-Schrift • Meroitisch-demotisch • Meroitische Hieroglyphen • Modi • Modifizierende Tonzeichen • Mongolisch • Mongolisch, Ergänzung • Mormonen-Alphabet • Mro • Multanisch • Nabatäisch • Nag Mundari • Nandinagari • Neu-Tai-Lue • Newa • N’Ko • Notenschrift • Nyiakeng Puachue Hmong • Ogam • Ol Chiki • Optische Zeichenerkennung • Oriya • Osage • Osmanische Siyaq-Zahlzeichen • Osmaniya • Pahawh Hmong • Palmyrenisch • Parthisch • Pau Cin Hau • Pfeile • Phagspa • Phonetische Erweiterungen • Phonetische Erweiterungen, Ergänzung • Phönizisch • Piktografische Symbole, erw.-A • Pollard-Schrift • Privatnutzungsbereich • Zusätzlicher Privatnutzungsbereich-A • Zusätzlicher Privatnutzungsbereich-B • Psalter-Pahlavi • Rahmenzeichnung • Rejang • Rumi-Ziffern • Runen • Samaritanisch • Saurashtra • Schachsymbole • Sharada • Shaw-Alphabet • Siddham • Singhalesisch • Singhalesische Zahlzeichen • Smileys • Snamennyj-Notenschrift • Sogdisch • Sojombo • Sorang-Sompeng • Spacing Modifier Letters • Spezielles • Spielkarten • Sundanesisch • Sundanesisch, Ergänzung • SignWriting • Syloti Nagri • Symbole für Retrocomputer • Symbole für Steuerzeichen • Syrisch • Syrisch, Ergänzung • Tagalog • Tagbanuwa • Tags • Tai Le • Tai Viet • Tai-Xuan-Jing-Symbole • Takri • Tamilisch • Tamilisch, Ergänzung • Tangsa • Telugu • Thaana • Thailändisch • Tibetisch • Tifinagh • Tirhuta • Toto • Ugaritisch • Umschlossene alphanum. Zeichen • Umschlossene CJK-Zeichen und -Monate • Vai • Varang Kshiti • Variantenselektoren • Variantenselektoren, Ergänzung • Vedische Erweiterungen • Vereinh. CJK-Ideogramme • Vereinh. CJK-Ideogramme, Erw. A • Vereinh. CJK-Ideogramme, Erw. B • Vereinh. CJK-Ideogramme, Erw. C • Vereinh. CJK-Ideogramme, Erw. D • Vereinh. CJK-Ideogramme, Erw. E • Vereinh. CJK-Ideogramme, Erw. F • Vereinh. CJK-Ideogramme, Erw. G • Vereinh. CJK-Ideogramme, Erw. H • Vereinh. CJK-Ideogramme, Erw. I • Vereinh. Silbenz. kanad. Ureinw. • Vereinh. Silbenz. kanad. Ureinw., erw. • Vereinh. Silbenz. kanad. Ureinw., erw.-A • Verkehrs- und Kartensymbole • Verschiedene mathem. Symbole-A • Verschiedene mathem. Symbole-B • Verschiedene piktografische Symbole • Verschiedene Symbole und Pfeile • Verschiedene Symbole • Verschiedene technische Zeichen • Vertikale Formen • Vithkuq-Alphabet • Währungszeichen • Wancho • Xixia • Xixia, Ergänzung • Xixia-Komponenten • Yi-Radikale • Yi-Silbenzeichen • Zählstabziffern • Zahlzeichen • Ziersymbole • Zusätzliche Interpunktion • Zusätzliche mathem. Operatoren • Zusätzliche Pfeile-A • Zusätzliche Pfeile-B • Zusätzliche Pfeile-C • Zusätzliche piktografische Symbole • Zusätzliche umschlossene alphanum. Zeichen • Zusätzliche umschlossene CJK-Zeichen",
		"url": "https://de.wikipedia.org/wiki/Unicode"
	},
	"el": {
		"content": "Στους υπολογιστές, το διεθνές πρότυπο Unicode στοχεύει στην κωδικοποίηση όλων των συστημάτων γραφής που χρησιμοποιούνται στον πλανήτη, ώστε να γίνει δυνατή η αποθήκευση -στη μνήμη ενός υπολογιστή- γραπτού κειμένου όλων των γλωσσών συμπεριλαμβανομένων και συμβόλων επιστημών, όπως μαθηματικά, φυσική κτλ.\n\nΗ καθιέρωση του Unicode είναι ένα φιλόδοξο σχέδιο αφού σκοπεύει να αντικαταστήσει όλες τις υπάρχουσες κωδικοποιήσεις συνόλων χαρακτήρων, οι οποίες έχουν περιορισμούς που τις καθιστούν προβληματικές για χρήση σε πολυγλωσσικά υπολογιστικά συστήματα.\n\nΠαρά τα τεχνικά προβλήματα που έχουν παρουσιαστεί το Unicode έχει καθιερωθεί ως το πιο πλήρες σύνολο χαρακτήρων και ως η προτιμότερη κωδικοποίηση σε πολυγλωσσικό λογισμικό. Πολλά πρόσφατα πρότυπα όπως το XML, καθώς και λογισμικό συστήματος όπως λειτουργικά συστήματα, έχουν υιοθετήσει το Unicode για να αναπαριστούν εσωτερικά κείμενο.\n\nΤο πρότυπο Unicode είχε τον ρητό στόχο να ξεπεράσει τους περιορισμούς των παλαιότερων (\"παραδοσιακών\") προτύπων για κωδικοποίηση χαρακτήρων όπως για παράδειγμα το ISO 8859 πρότυπο, το οποίο χρησιμοποιήθηκε ευρέως σε πολλές χώρες στον κόσμο, αλλά παρουσίαζε προβλήματα ασυμβατότητας μεταξύ των διαφορετικών υλοποιήσεών του. Γενικά, πολλά παλαιότερα πρότυπα για κωδικοποίηση χαρακτήρων μοιράζονται ένα κοινό πρόβλημα: το ότι επιτρέπουν υποστήριξη μονο δύο αλφαβήτων σε ένα συγκεκριμένο υπολογιστή, συνήθως του Λατινικού και ενός τοπικού, δηλαδή δεν υποστηρίζουν πολλά αλφάβητα στον ίδιο υπολογιστή. Για παράδειγμα, οσον αφορά το πρότυπο ISO 8859, ένας υπολογιστής στην Ελλάδα είναι (συνήθως) ρυθμισμένος να υποστηρίζει μονο το Λατινικό και το Ελληνικό αλφάβητο για κείμενο κωδικοποιημένο κατά ISO 8859. Έτσι, η ανάγνωση κειμένου (κωδικοποιημένου κατά ISO 8859) που περιέχει κάποιο άλλο αλφάβητο (όπως για παράδειγμα Ρώσικο, Εβραϊκό κλπ) θα είναι προβληματική σε Ελληνικό υπολογιστή, αφού οι (Ρωσικοί, Εβραϊκοί κλπ) χαρακτήρες δεν θα εμφανίζονται σωστά. Αυτό συμβαίνει για το πρότυπο ISO 8859 χρησιμοποιεί μόνο 8 bit για να κωδικοποιήσει κάθε χαρακτήρα (δηλαδή, συνολικά μπορούμε να αναπαραστήσουμε μονο 256 διαφορετικούς χαρακτήρες). Οι πρώτες 128 θέσεις δεσμεύονται για το Λατινικό αλφάβητο και μερικά σύμβολα, ενώ οι υπόλοιπες 128 για το τοπικό μη-Λατινογενές αλφάβητο (όποιο και αν είναι αυτό Ελληνικό, Ρώσικο, Εβραϊκό κλπ). Έτσι πρακτικά, οι χαρακτήρες των μη-λατινογενών αλφάβητων \"μοιράζονται\" τις ίδιες θέσεις με τους χαρακτήρες κάποιου άλλου μη-λατινογενούς αλφαβήτου, και άρα δεν μπορούν να υποστηρίζονται ταυτόχρονα από το σύστημα (δεν είναι δυνατόν μια θέση να αντιστοιχεί σε πάνω από έναν χαρακτήρες).\n\nΤο Unicode μετατρέπει κάθε χαρακτήρα σε ένα κωδικό σημείο (\"θέση/τιμή κώδικα\"), χρησιμοποιώντας έως και 16 bit ανά χαρακτήρα το οποίο εξασφαλίζει ότι όλοι οι χαρακτήρες μπορούν να υποστηρίζονται από όλα τα υπολογιστικά συστήματα. Το unicode δεν κωδικοποιεί τη μορφοποίηση του κειμένου (στυλ/γραμματοσειρά/μέγεθος κλπ).Το πρότυπο Unicode αφήνει το ανάλογο λογισμικό (πλοηγός Διαδικτύου, επεξεργαστής κειμένου) να εφαρμόσει την οπτική αναπαράσταση (στυλ, μέγεθος, γραμματοσειρά) των χαρακτήρων.\n\nΕπίσης στο πρότυπο περιλαμβάνει και σχετικά θέματα όπως ιδιότητες χαρακτήρων, φόρμες κανονικοποίησης κειμένου,κατεύθυνση εμφάνισης(για γλώσσες που διαβάζονται και από τα δεξιά προς τα αριστερά όπως η Αραβική γλώσσα και τα Εβραϊκά.\n\nΤο Unicode περιλαμβάνει σχεδόν όλα τα συστήματα γραφής που είναι σε χρήση σήμερα. Αυτά είναι τα:\n\nκαι άλλα\n\nΤο Unicode έχει προσθέσει και άλλα αλφάβητα όπως ιστορικά αλφάβητα και εξαφανισμένα αλφάβητα για ακαδημαϊκούς λόγους:\n\n...\n\nΕπίσης περιλαμβάνει και άλλα σύμβολα που χρησιμοποιούνται στα μαθηματικά και την μουσική.\n\nΤο 1997 Μίχαελ Έβερσον πρότεινε να κωδικοποιηθούν και οι χαρακτήρες της φανταστικής Κλίνγκον γλώσσας στο Επίπεδο 1 του ISO/IEC 10646-2. Αλλά η πρόταση αυτή απορρίφθηκε όπως και η πρόταση για συμπερίληψη γλωσσών του Τόλκιν.\n\nΗ κοινοπραξία Unicode με έδρα την Καλιφόρνια, αναπτύσσει το πρότυπο Unicode. Οποιαδήποτε οργάνωση ή ιδιώτης μπορεί να γίνει μέλος της εφόσον πληρώσει συνδρομή. Στα μέλη συμπεριλαμβάνονται σχεδόν όλες τις μεγάλες εταιρείες λογισμικού και υλικού που ενδιαφέρονται σχετικά όπως οι Apple, Microsoft, IBM, Xerox, HP, Adobe Systems και πολλές άλλες.\n\nΗ κοινοπραξία δημοσίευσε πρώτη φορά Το πρότυπο Unicode (ISBN 0-321-18578-1) το 1991,και συνεχίζει να αναπτύσσει πρότυπα βασισμένα στην αρχική αυτή εργασία. Η κοινοπραξία Unicode αναπτύχθηκε σε συνδυασμό με τον Διεθνή Οργανισμό Τυποποίησης ISO, και το πρότυπό της μοιράζεται το σύνολο χαρακτήρων της με το πρότυποISO/IEC 10646. Το Unicode και το ISO/IEC 10646 είναι ισοδύναμα ως κωδικοποιήσεις χαρακτήρων αλλά το Unicode περιέχει πολύ περισσότερες πληροφορίες για προγραμματιστές που το υλοποιούν, καλύπτοντας σε βάθος θέματα όπως κωδικοποίηση βασισμένη σε μπιτ, Unicode collation αλγόριθμοι, και γραφική απόδοση. Το Unicode απαριθμεί αρκετές ιδιότητες χαρακτήρων, περιλαμβάνοντας και αυτές που χρειάζονται για BiDi υποστήριξη. Τα δυο πρότυπα χρησιμοποιούν μερικώς διαφορετική ορολογία.\n\nΌταν γράφουμε για κωδικά σημεία του Unicode είναι σύνηθες να χρησιμοποιούμε τη μορφή U+xxxx or U+xxxxxx όπου xxxx ή xxxxxx είναι το κωδικό σημείο στο δεκαεξαδικό σύστημα.\n\nΜέχρι τώρα το Unicode παρουσιάστηκε απλά ως μια απεικόνιση κάθε χαρακτήρα που χρησιμοποιείται σε κάποιο αλφάβητο στη Γη σε έναν μοναδικό αριθμό, το κωδικό σημείο. Όμως η αποθήκευση αυτών των αριθμών κατά την επεξεργασία κειμένου είναι ένα εντελώς διαφορετικό θέμα. Προβλήματα ανακύπτουν από το γεγονός ότι το λογισμικό που γράφεται στον δυτικό κόσμο χειρίζεται μόνο κωδικοποιήσεις 8-bit με την unicode υποστήριξη να προστίθεται πολύ αργότερα.\n\nΗ εσωτερική λογική παραδοσιακών 8-μπιτ εφαρμογών επιτρέπει μόνο 8 μπιτς για κάθε χαρακτήρα κάνοντας αδύνατη τη χρησιμοποίηση περισσότερων των 256 κωδικών σημείων χωρίς ειδική επεξεργασία. Έτσι οι μηχανικοί λογισμικού έχουν προτείνει διάφορους μηχανισμούς για την υλοποίηση του Unicode. Ποια υλοποίηση χρησιμοποιεί ο κάθε προγραμματιστής από θέματα χωρητικότητας, συμβατότητας πηγαίου κώδικα και διαλειτουργικότητας με άλλα συστήματα.\n\nΤο Unicode ορίζει δύο τρόπους απεικόνισης:\n\nΑυτές οι κωδικοποιήσεις περιλαμβάνουν τις εξής κύριες:\n\n(Ο αριθμός υποδηλώνει τον αριθμό των μπιτς σε κάθε μονάδα (για UTF κωδικοποιήσεις) ή byter ανά μονάδα (για UCS κωδικοποιήσεις).)\n\nΤο Unicode περιλαμβάνει ένα μηχανισμό τροποποίησης του σχήματος των χαρακτήρων κι έτσι επεκτείνει το ρεπερτόριο των υποστηριζόμενων γλυφών. Αυτό καλύπτει τη χρήση συνδυαζόμενων διακριτικών χαρακτήρων. Αυτοί εισάγονται μετά τον κύριο χαρακτήρα (μπορεί κανείς να σωρρεύσει περισσότερους συνδυαζόμενους διακριτικούς χαρακτήρες στον ίδιο χαρακτήρα). Ωστόσο, για λόγους συμβατότητας, το Unicode περιλαμβάνει επίσης μια μεγάλη ποσότητα από προσυντεθιμένους χαρακτήρες. Έτσι, σε πολλές περιπτώσεις, οι χρήστες έχουν στη διάθεσή τους πολλούς τρόπους κωδικοποίησης του ίδιου χαρακτήρα. Για να το αντιμετωπίσει αυτό, το Unicode παρέχει τον μηχανισμό των μετασχηματισμών κανονικής ισοδυναμίας. Παρόμοια κατάσταση ισχύει και με το Hangul. Το Unicode παρέχει τον μηχανισμό σύνθεσης συλλαβών Hangul με το Hangul Jamo. Ωστόσο, παρέχει επίσης προσυντεθιμένες συλλαβές Hangul (11,171 συλλαβές)\n\nΤα ιδεογράμματα CJK (Κίνα, Ιαπωνία, Κορέα) έχουν επί του παρόντος κώδικες μόνο για την προσυντεθιμένη μορφή τους. Ακόμα όμως, τα περισσότερα από αυτά τα ιδεογραφήματα τελικώς αποτελούνται από απλούστερα στοιχεία, έτσι, κατ' αρχήν το Unicode μπορεί να τα αποσυνθέσει όπως συμβαίνει με τα Hangul. Αυτό θα μείωνε σε μεγάλο βαθμό τον αριθμό των απαιτούμενων κωδικών σημείων, ενώ ταυτόχρονα επιτρέπει την εμφάνιση στην πραγματικότητα κάθε νοητού ιδεογραφήματος (καταργώντας έτσι τα προβλήματα της Han unification) Μια παρόμοια ιδέα καλύπτει ορισμένες μεθόδους εισαγωγής όπως η μέθοδος Cangjie και Wubi. Ωστόσο, οι προσπάθειες να γίνει αυτό για κωδικοποίηση χαρακτήρων σκόνταψαν πάνω στο γεγονός ότι τα ιδεογραφήματα δεν αποσυντίθενται τόσο απλά ή τόσο κανονικά όσο δείχνουν.\n\nΟι συνδυαζόμενοι χαρακτήρες, όπως η περίπλοκη μορφοποίηση του script που απαιτείται για να αποδώσει κανείς σωστά ένα Αραβικό κείμενο και πολλά άλλα scripts, συχνά εξαρτώνται από περίπλοκες τεχνολογίες γραμματοσειρών, όπως η Open Type (από την Adobe και τη Microsoft), την Graphite (από την SIL International) και την Apple Advanced Typography AAT (από την Apple Computer), μέσω της οποίας ο σχεδιαστής γραμματοσειράς περιλαμβάνει οδηγίες σε ένα λογισμικό δημιουργίας γραμματοσειρών για το πως θα παράγει διαφορετικές σειρές χαρακτήρων. Μια άλλη μέθοδος που χρησιμοποιείται σε γραμματοσειρές με σταθερό πλάτος είναι να τοποθετείται το σύμβολο του συνδυαζόμενου χαρακτήρα πριν τα δεξιά του (sidebearing). Αυτή η μέθοδος, ωστόσο, λειτουργεί μόνο για ορισμένα διακριτικά και η σώρρευση δεν θα γίνει σωστά.\n\nΜέχρι και το 2004, τα περισότερα λογισμικά ακόμα δεν μπορούσαν να χειριστούν αξιόπιστα πολλά χαρακτηριστικά που δεν υποστηρίζονταν από παλαιότερες τυποποιήσεις γραμματοσειρών, έτσι το να συνδυάζει κανείς χαρακτήρες, γενικώς δεν λειτουργούσε σωστά. Υποθετικά, το ḗ (προσυντεθιμένο e με περισπωμένη και οξεία) και το ḗ (e που ακολουθείται από συνδυαζόμενη περισπωμένη επάνω και συνδυαζόμενη οξεία επάνω) είναι πανομοιότυπα στην εμφάνιση, δίνοντας και τα δύο ένα e με περισπωμένη και οξεία, αλλά η εμφάνιση μπορεί να διαφοροποιείται σε μεγάλο βαθμό κατά την εφαρμογή εφαρμογών λογισμικού.\n\nΕπίσης, οι κάτω τελείες, οι οποίες χρειάζονται στα Ινδικά Romanization, συχνά θα τοποθετούνται λανθασμένα ή ακόμα χειρότερα. Παράδειγμα:\n\nΒέβαια, αυτό δεν είναι στην πραγματικότητα μια αδυναμία του ίδιου του Unicode, αλλά απλώς αποκαλύπτει κενά της τεχνολογίας απόδοσης και των γραμματοσειρών.\n\nΠαρά τα τεχνικά προβλήματα τους περιορισμούς και την κριτική στη πορεία, το Unicode έχει επικρατήσει ως το κυρίαρχο σχήμα κωδικοποίησης χαρακτήρων. Τα Windows NT και οι απόγονοί του Windows 2000 και Windows XP κάνουν εκτεταμένη χρήση του σχήματος κωδικοποίησης UTF-16 για εσωτερική αναπαράσταση κειμένου. UNIX λειτουργικά συστήματα όπως GNU/Linux, Plan 9 από Bell Labs, BSD και Mac OS X έχουν υιοθετήσει το σχήμα UTF-8, ως τη βάση για την αναπαράσταση πολυγλωσσικό κείμενο.\n\nΤο πρότυπο MIME ορίζει δύο διαφορετικούς μηχανισμούς για κωδικοποίηση όχι-ASCII χαρακτήρων στα μηνύματα ηλεκτρονικής αλληλογραφίας,e-mails, ανάλογα με το αν οι χαρακτήρες είναι στις επικεφαλίδες του ηλ.μηνύματος όπως π.χ. η επικεφαλίδα \"Θέμα:\" ή βρίσκονται στο κυρίος κείμενο του ηλεκτρονικού μηνύματος. Και στις δυο περιπτώσεις, προσδιορίζεται το αρχικό σύνολο χαρακτήρων καθώς και η κωδικοποίηση μεταφοράς. Για ηλεκτρονική αλληλογραφία με Unicode χαρακτήρες προτείνονται το σχήμα κωδικοποίησης UTF-8 και η κωδικοποίηση μεταφοράς Base64. Οι λεπτομέρειες των δύο μηχανισμών καθορίζονται στο πρότυπο MIME και γενικά είναι κρυμμένοι από τον απλό χρήστη λογισμικού ηλ. αλληλογραφίας.\n\nΗ υιοθέτηση του Unicode στην Ηλεκτρονική αλληλογραφία είναι πολύ αργή. Τα περισσότερα κείμενα στην ανατολική Ασία κωδικοποιούνται άκομα σε τοπικές κωδικοποιήσεις όπως η Shift-JIS, και πολλά δημοφιλή προγράμματα ηλ.αλληλογραφίας ακόμα και αν έχουν κάποια unicode υποστήριξη εντούτοις δεν μπορούν να χειριστούν Unicode δεδομένα σωστά. Η κατάσταση αυτή δεν προβλέπεται να αλλάξει το προσεχές μέλλον.\n\nΟι καινούργιοι πλοηγοί διαδικτύου μπορούν και απεικονίζουν σωστά ιστοσελίδες με Unicode χαρακτήρες εφόσον έχει εγκατασταθεί η ανάλογη γραμματοσειρά.\n\nΠαρόλο που συντακτικοί κανόνες μπορεί να επηρεάζουν τη σειρά με την οποία οι χαρακτήρες επιτρέπεται να εμφανίζονται και η γλώσσα HTML 4.0 αλλά και η XML 1.0 εξ ορισμού υποστηρίζουν έγγραφα που αποτελούνται από χαρακτήρες από όλο το εύρος των κωδικών σημείων του Unicode εξαιρουμένων μόνο κάποιων χαρακτήρων ελέγχου τα μόνιμα μη-διαθέσιμα κωδικά σημεία D800-DFFF, οποιοδήποτε κωδικό σημείο που τελειώνει σε FFFE or FFFF και οποιοδήποτε κωδικό σημείο πάνω από 10FFFF. Αυτοί οι χαρακτήρες παρουσιάζονται είτε απευθείας ως μπάιτς σύμφωνα με την κωδικοποίηση του εγγράφου,εφόσον υποστηρίζονται από την κωδικοποίηση,ή μπορούν να γραφτούν ως αριθμητικές αναφορές χαρακτήρων βασισμένες στο κωδικό σημείο του Unicode χαρακτήρα, εφόσον η κωδικοποίηση που χρησιμοποιεί το έγγραφο επιτρέπει τα ψηφία και τα σύμβολα που χρειάζονται για να γράψουμε τις αναφορές (κάτι που συμβαίνει με όλες τις κωδικοποιήσεις που έχουν υιοθετηθεί στο διαδίκτυο) Για παράδειγμα οι αναφορές : Δ Й ק م ๗ あ 叶 葉 냻 (ή η ίδια τιμή στο δεκαεξαδικό με πρόθεμα &#x ) εμφανίζεται στον πλοηγό σου ως Δ, Й, ק, م, ๗, あ, 叶, 葉 και 냻—εφόσον έχεις την κατάλληλη γραμματοσειρά, αυτά τα σύμβολα φαίνονται σαν Greek capital letter \"Delta\", Cyrillic capital letter \"Short I\", Arabic letter \"Meem\", Hebrew letter \"Qof\", Thai numeral 7, Japanese Hiragana \"A\", simplified Chinese \"Leaf\", traditional Chinese \"Leaf\", and Korean Hangul syllable \"Nyaelh\", αντίστοιχα.\n\nΕλεύθερες και εμπορεύσιμες γραμματοσειρές που βασίζονται στο Unicode πρότυπο είναι κοινές, με πρώτες τις TrueType και τώρα τις OpenType γραμματοσειρές που υποστηρίζουν και οι δύο Unicode απεικονίζοντας κωδικά σημεία σε συγκεκριμένες εμφανίσεις χαρακτήρων.\n\nΥπάρχουν χιλίαδες γραμματοσειρές στην αγορά,αλλά λιγότερες από δώδεκα προσπαθούν να υποστηρίξουν την πλειοψηφία του συνόλου χαρακτήρων του προτύπου Unicode. Αντίθετα οι βασισμένες στο Unicode γραμματοσειρές συνήθως υποστηρίζουν μόνο βασικό ASCII και κάποια συγκεκριμένα αλφάβητα. Αυτό γίνεται κυρίως για λόγους οικονομίας των δημιουργών γραμματοσειρών και απόδοσης των προγραμμάτων που μπορεί να γονατίσουν καθώς η απόδοση γραμματοσειρών είναι μια διαδικασία που καταναλώνει πολλούς πόρους ενός υπολογιστή.\n\nΧαρακτήρες Unicode που δεν μπορούν να αποδωθούν γραφικά απεικονίζονται με ένα λευκό τετράγωνο.\n\nΟι επεξεργαστές κειμένου Microsoft Word επιτρέπουν την εισαγωγή χαρακτήρων Unicode με δυο τρόπους:\n\nΤο Gnome2 ακολουθεί το πρότυπο ISO 14755. Κράτησε πατημένα τα πλήκτρα Ctrl and Shift και εισήγαγε στο δεκαεξαδικό το κωδικό σημείο του unicode χαρακτήρα που θέλεις να εμφανιστεί.\n\n",
		"url": "https://el.wikipedia.org/wiki/Unicode"
	},
	"en": {
		"content": "\n\nUnicode, formally The Unicode Standard,[note 1] is a text encoding standard maintained by the Unicode Consortium designed to support the use of text in all of the world's writing systems that can be digitized. Version 15.1 of the standard[A] defines 149813 characters[3] and 161 scripts used in various ordinary, literary, academic, and technical contexts.\n\nMany common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\n\nUnicode has largely supplanted the previous environment of a myriad of incompatible character sets, each used within different locales and on different computer architectures. Unicode is used to encode the vast majority of text on the Internet, including most web pages, and relevant Unicode support has become a common consideration in contemporary software development.\n\nThe Unicode character repertoire is synchronized with ISO/IEC 10646, each being code-for-code identical with one another. However, The Unicode Standard is more than just a repertoire within which characters are assigned. To aid developers and designers, the standard also provides charts and reference data, as well as annexes explaining concepts germane to various scripts, providing guidance for their implementation. Topics covered by these annexes include character normalization, character composition and decomposition, collation, and directionality.[5]\n\nUnicode text is processed and stored as binary data using one of several encodings, which define how to translate the standard's abstracted codes for characters into sequences of bytes. The Unicode Standard itself defines three encodings: UTF-8, UTF-16, and UTF-32, though several others exist. Of these, UTF-8 is the most widely used by a large margin, in part due to its backwards-compatibility with ASCII.\n\nUnicode was originally designed with the intent of transcending limitations present in all text encodings designed up to that point: each encoding was relied upon for use in its own context, but with no particular expectation of compatibility with any other. Indeed, any two encodings chosen were often totally unworkable when used together, with text encoded in one interpreted as garbage characters by the other. Most encodings had only been designed to facilitate interoperation between a handful of scripts—often primarily between a given script and Latin characters—not between a large number of scripts, and not with all of the scripts supported being treated in a consistent manner.\n\nThe philosophy that underpins Unicode seeks to encode the underlying characters—graphemes and grapheme-like units—rather than graphical distinctions considered mere variant glyphs thereof, that are instead best handled by the typeface, through the use of markup, or by some other means. In particularly complex cases, such as the treatment of orthographical variants in Han characters, there is considerable disagreement regarding which differences justify their own encodings, and which are only graphical variants of other characters.\n\nAt the most abstract level, Unicode assigns a unique number called a code point to each character. Many issues of visual representation—including size, shape, and style—are intended to be up to the discretion of the software actually rendering the text, such as a web browser or word processor. However, partially with the intent of encouraging rapid adoption, the simplicity of this original model has become somewhat more elaborate over time, and various pragmatic concessions have been made over the course of the standard's development.\n\nThe first 256 code points mirror the ISO/IEC 8859-1 standard, with the intent of trivializing the conversion of text already written in Western European scripts. To preserve the distinctions made by different legacy encodings, therefore allowing for conversion between them and Unicode without any loss of information, many characters nearly identical to others, in both appearance and intended function, were given distinct code points. For example, the Halfwidth and Fullwidth Forms block encompasses a full semantic duplicate of the Latin alphabet, because legacy CJK encodings contained both \"fullwidth\" (matching the width of CJK characters) and \"halfwidth\" (matching ordinary Latin script) characters.\n\nThe Unicode Bulldog Award is given to people deemed to be influential in Unicode's development, with recipients including Tatsuo Kobayashi, Thomas Milo, Roozbeh Pournader, Ken Lunde, and Michael Everson.[6]\n\nThe origins of Unicode can be traced back to the 1980s, to a group of individuals with connections to Xerox's Character Code Standard (XCCS).[7] In 1987, Xerox employee Joe Becker, along with Apple employees Lee Collins and Mark Davis, started investigating the practicalities of creating a universal character set.[8] With additional input from Peter Fenwick and Dave Opstad,[7] Becker published a draft proposal for an \"international/multilingual text character encoding system in August 1988, tentatively called Unicode\". He explained that \"the name 'Unicode' is intended to suggest a unique, unified, universal encoding\".[7]\n\nIn this document, entitled Unicode 88, Becker outlined a scheme using 16-bit characters:[7]\n\nUnicode is intended to address the need for a workable, reliable world text encoding. Unicode could be roughly described as \"wide-body ASCII\" that has been stretched to 16 bits to encompass the characters of all the world's living languages. In a properly engineered design, 16 bits per character are more than sufficient for this purpose.\n\nThis design decision was made based on the assumption that only scripts and characters in \"modern\" use would require encoding:[7]\n\nUnicode gives higher priority to ensuring utility for the future than to preserving past antiquities. Unicode aims in the first instance at the characters published in the modern text (e.g. in the union of all newspapers and magazines printed in the world in 1988), whose number is undoubtedly far below 214 = 16,384. Beyond those modern-use characters, all others may be defined to be obsolete or rare; these are better candidates for private-use registration than for congesting the public list of generally useful Unicode.\n\nIn early 1989, the Unicode working group expanded to include Ken Whistler and Mike Kernaghan of Metaphor, Karen Smith-Yoshimura and Joan Aliprand of Research Libraries Group, and Glenn Wright of Sun Microsystems. In 1990, Michel Suignard and Asmus Freytag of Microsoft and NeXT's Rick McGowan had also joined the group. By the end of 1990, most of the work of remapping existing standards had been completed, and a final review draft of Unicode was ready.\n\nThe Unicode Consortium was incorporated in California on 3 January 1991,[9] and the first volume of The Unicode Standard was published that October. The second volume, now adding Han ideographs, was published in June 1992.\n\nIn 1996, a surrogate character mechanism was implemented in Unicode 2.0, so that Unicode was no longer restricted to 16 bits. This increased the Unicode codespace to over a million code points, which allowed for the encoding of many historic scripts, such as Egyptian hieroglyphs, and thousands of rarely used or obsolete characters that had not been anticipated for inclusion in the standard. Among these characters are various rarely used CJK characters—many mainly being used in proper names, making them far more necessary for a universal encoding than the original Unicode architecture envisioned.[10]\n\nVersion 1.0 of Microsoft's TrueType specification, published in 1992, used the name \"Apple Unicode\" instead of \"Unicode\" for the Platform ID in the naming table.\n\nThe Unicode Consortium is a nonprofit organization that coordinates Unicode's development. Full members include most of the main computer software and hardware companies (and few others) with any interest in text-processing standards, including Adobe, Apple, Google, IBM, Meta (previously as Facebook), Microsoft, Netflix, and SAP.[11]\n\nOver the years several countries or government agencies have been members of the Unicode Consortium. Presently only the Ministry of Endowments and Religious Affairs (Oman) is a full member with voting rights.[11]\n\nThe Consortium has the ambitious goal of eventually replacing existing character encoding schemes with Unicode and its standard Unicode Transformation Format (UTF) schemes, as many of the existing schemes are limited in size and scope and are incompatible with multilingual environments.\n\nUnicode currently covers most major writing systems in use today.[12][better source needed]\n\nAs of 2024[update], a total of 161 scripts[13] are included in the latest version of Unicode (covering alphabets, abugidas and syllabaries), although there are still scripts that are not yet encoded, particularly those mainly used in historical, liturgical, and academic contexts. Further additions of characters to the already encoded scripts, as well as symbols, in particular for mathematics and music (in the form of notes and rhythmic symbols), also occur.\n\nThe Unicode Roadmap Committee (Michael Everson, Rick McGowan, Ken Whistler, V.S. Umamaheswaran)[14] maintain the list of scripts that are candidates or potential candidates for encoding and their tentative code block assignments on the Unicode Roadmap[15] page of the Unicode Consortium website. For some scripts on the Roadmap, such as Jurchen and Khitan large script, encoding proposals have been made and they are working their way through the approval process. For other scripts, such as Numidian and Rongorongo, no proposal has yet been made, and they await agreement on character repertoire and other details from the user communities involved.\n\nSome modern invented scripts which have not yet been included in Unicode (e.g., Tengwar) or which do not qualify for inclusion in Unicode due to lack of real-world use (e.g., Klingon) are listed in the ConScript Unicode Registry, along with unofficial but widely used Private Use Areas code assignments.\n\nThere is also a Medieval Unicode Font Initiative focused on special Latin medieval characters. Part of these proposals has been already included in Unicode.\n\nThe Script Encoding Initiative,[16] a project run by Deborah Anderson at the University of California, Berkeley was founded in 2002 with the goal of funding proposals for scripts not yet encoded in the standard. The project has become a major source of proposed additions to the standard in recent years.[17]\n\nThe Unicode Consortium together with the ISO have developed a shared repertoire following the initial publication of The Unicode Standard: Unicode and the ISO's Universal Coded Character Set (UCS) use identical character names and code points. However, the Unicode versions do differ from their ISO equivalents in two significant ways.\n\nWhile the UCS is a simple character map, Unicode specifies the rules, algorithms, and properties necessary to achieve interoperability between different platforms and languages. Thus, The Unicode Standard includes more information, covering in-depth topics such as bitwise encoding, collation, and rendering. It also provides a comprehensive catalog of character properties, including those needed for supporting bidirectional text, as well as visual charts and reference data sets to aid implementers. Previously, The Unicode Standard was sold as a print volume containing the complete core specification, standard annexes,[note 2] and code charts. However, version 5.0, published in 2006, was the last version printed this way. Starting with version 5.2, only the core specification, published as a print-on-demand paperback, may be purchased.[18] The full text, on the other hand, is published as a free PDF on the Unicode website.\n\nA practical reason for this publication method highlights the second significant difference between the UCS and Unicode—the frequency with which updated versions are released and new characters added. The Unicode Standard has regularly released annual expanded versions, occasionally with more than one version released in a calendar year and with rare cases where the scheduled release had to be postponed. For instance, in April 2020, a month after version 13.0 was published, the Unicode Consortium announced they had changed the intended release date for version 14.0, pushing it back six months to September 2021 due to the COVID-19 pandemic.\n\nUnicode 15.1, the latest version, was released on 12 September 2023. It is a minor version update to version 15.0—released on 13 September 2022—which added a total of 4,489 new characters, including two new scripts, an extension to the CJK Unified Ideographs block, and multiple additions to existing blocks. 33 new emoji were added, such as the \"wireless\" (network) symbol and additional colored hearts.[19][20]\n\nThus far, the following versions of The Unicode Standard have been published. Update versions, which do not include any changes to character repertoire, are signified by the third number (e.g., \"version 4.0.1\") and are omitted in the table below.[21]\n\n[b]\n\n[d]\n\n[e]\n\n[f]\n\n[g]\n\n[h]\n\n[53]\n\nThe Unicode Consortium normally releases a new version of The Unicode Standard once a year, or occasionally twice a year. Version 16.0, the next major version, is scheduled to be published in 2024, and is projected to include six new scripts (Todhri, Sunuwar, Gurung Khema, Kirat Rai, Garay, and Ol Onal), additional Burmese numerals for Shan and Mon alphabets, additional symbols for legacy computing, and at least six new emoji.[57][58]\n\nThe Unicode Standard defines a codespace:[59] a sequence of integers called code points[60] in the range from 0 to 1114111, notated according to the standard as U+0000–U+10FFFF.[61] The codespace is a systematic, architecture-independent representation of The Unicode Standard; actual text is processed as binary data via one of several Unicode encodings, such as UTF-8.\n\nIn this normative notation, the two-character prefix U+ always precedes a written code point,[62] and the code points themselves are written as hexadecimal numbers. At least four hexadecimal digits are always written, with leading zeros prepended as needed. For example, the code point U+00F7 ÷ DIVISION SIGN is padded with two leading zeros, but U+13254 𓉔 EGYPTIAN HIEROGLYPH O004 () is not padded.[63]\n\nThere are a total of 220 + (216 − 211) = 1112064 valid code points within the codespace. (This number arises from the limitations of the UTF-16 character encoding, which can encode the 216 code points in the range U+0000 through U+FFFF except for the 211 code points in the range U+D800 through U+DFFF, which are used as surrogate pairs to encode the 220 code points in the range U+10000 through U+10FFFF.)\n\nThe Unicode codespace is divided into 17 planes, numbered 0 to 16. Plane 0 is the Basic Multilingual Plane (BMP), and contains the most commonly used characters. All code points in the BMP are accessed as a single code unit in UTF-16 encoding and can be encoded in one, two or three bytes in UTF-8. Code points in planes 1 through 16 (the supplementary planes) are accessed as surrogate pairs in UTF-16 and encoded in four bytes in UTF-8.\n\nWithin each plane, characters are allocated within named blocks of related characters. The size of a block is always a multiple of 16, and is often a multiple of 128, but is otherwise arbitrary. Characters required for a given script may be spread out over several different, potentially disjunct blocks within the codespace.\n\nEach code point is assigned a classification, listed as the code point's General Category property. Here, at the uppermost level code points are categorized as one of Letter, Mark, Number, Punctuation, Symbol, Separator, or Other. Under each category, each code point is then further subcategorized. In most cases, other properties must be used to adequately describe all the characteristics of any given code point.\n\nThe 1024 points in the range U+D800–U+DBFF are known as high-surrogate code points, and code points in the range U+DC00–U+DFFF (1024 code points) are known as low-surrogate code points. A high-surrogate code point followed by a low-surrogate code point forms a surrogate pair in UTF-16 in order to represent code points greater than U+FFFF. In principle, these code points cannot otherwise be used, though in practice this rule is often ignored, especially when not using UTF-16.\n\nA small set of code points are guaranteed never to be assigned to characters, although third-parties may make independent use of them at their discretion. There are 66 of these noncharacters: U+FDD0–U+FDEF and the last two code points in each of the 17 planes (e.g. U+FFFE, U+FFFF, U+1FFFE, U+1FFFF, ..., U+10FFFE, U+10FFFF). The set of noncharacters is stable, and no new noncharacters will ever be defined.[64] Like surrogates, the rule that these cannot be used is often ignored, although the operation of the byte order mark assumes that U+FFFE will never be the first code point in a text. The exclusion of surrogates and noncharacters leaves 1111998 code points available for use.\n\nPrivate-use code points are considered to be assigned, but they intentionally have no interpretation specified by The Unicode Standard[65] such that any interchange of such code points requires an independent agreement between the sender and receiver as to their interpretation. There are three private-use areas in the Unicode codespace:\n\nGraphic characters are those defined by The Unicode Standard to have particular semantics, either having a visible glyph shape or representing a visible space. As of Unicode 15.1, there are 149641 graphic characters.\n\nFormat characters are characters that do not have a visible appearance but may have an effect on the appearance or behavior of neighboring characters. For example, U+200C ZERO WIDTH NON-JOINER and U+200D ZERO WIDTH JOINER may be used to change the default shaping behavior of adjacent characters (e.g. to inhibit ligatures or request ligature formation). There are 172 format characters in Unicode 15.1.\n\n65 code points, the ranges U+0000–U+001F and U+007F–U+009F, are reserved as control codes, corresponding to the C0 and C1 control codes as defined in ISO/IEC 6429. U+0089 LINE TABULATION, U+008A LINE FEED, and U+000D CARRIAGE RETURN are widely used in texts using Unicode. In a phenomenon known as mojibake, the C1 code points are improperly decoded according to the Windows-1252 codepage, previously widely used in Western European contexts.\n\nTogether, graphic, format, control code, and private use characters are collectively referred to as assigned characters. Reserved code points are those code points that are valid and available for use, but have not yet been assigned. As of Unicode 15.1, there are 824652 reserved code points.\n\nThe set of graphic and format characters defined by Unicode does not correspond directly to the repertoire of abstract characters representable under Unicode. Unicode encodes characters by associating an abstract character with a particular code point.[66] However, not all abstract characters are encoded as a single Unicode character, and some abstract characters may be represented in Unicode by a sequence of two or more characters. For example, a Latin small letter \"i\" with an ogonek, a dot above, and an acute accent, which is required in Lithuanian, is represented by the character sequence U+012F; U+0307; U+0301. Unicode maintains a list of uniquely named character sequences for abstract characters that are not directly encoded in Unicode.[67]\n\nAll assigned characters have a unique and immutable name by which they are identified. This immutability has been guaranteed since version 2.0 of The Unicode Standard by its Name Stability policy.[64] In cases where a name is seriously defective and misleading, or has a serious typographical error, a formal alias may be defined that applications are encouraged to use in place of the official character name. For example, U+A015 ꀕ YI SYLLABLE WU has the formal alias YI SYLLABLE ITERATION MARK, and U+FE18 ︘ PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRAKCET (sic) has the formal alias PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRACKET.[68]\n\nUnicode includes a mechanism for modifying characters that greatly extends the supported repertoire of glyphs. This covers the use of combining diacritical marks that may be added after the base character by the user. Multiple combining diacritics may be simultaneously applied to the same character. Unicode also contains precomposed versions of most letter/diacritic combinations in normal use. These make the conversion to and from legacy encodings simpler, and allow applications to use Unicode as an internal text format without having to implement combining characters. For example, é can be represented in Unicode as U+0065 e LATIN SMALL LETTER E followed by U+0301 ◌́ COMBINING ACUTE ACCENT), and equivalently as the precomposed character U+00E9 é LATIN SMALL LETTER E WITH ACUTE. Thus, users often have multiple equivalent ways of encoding the same character. The mechanism of canonical equivalence within The Unicode Standard ensures the practical interchangeability of these equivalent encodings.\n\nAn example of this arises with the Korean alphabet Hangul: Unicode provides a mechanism for composing Hangul syllables from their individual Hangul Jamo subcomponents. However, it also provides 11172 combinations of precomposed syllables made from the most common jamo.\n\nCJK characters presently only have codes for uncomposable radicals and precomposed forms. Most Han characters have either been intentionally composed from, or reconstructed as compositions of, simpler orthographic elements called radicals, so in principle Unicode could have enabled their composition as it did with Hangul. While this could have greatly reduced the number of required code points, as well as allowing the algorithmic synthesis of many arbitrary new characters, the complexities of character etymologies and the post-hoc nature of radical systems add immense complexity to the proposal. Indeed, attempts to design CJK encodings on the basis of composing radicals have been met with difficulties resulting from the reality that Chinese characters do not decompose as simply or as regularly as Hangul does.\n\nThe CJK Radicals Supplement block is assigned to the range U+2E80–U+2EFF, and the Kangxi radicals are assigned to U+2F00–U+2FDF. The Ideographic Description Sequences block covers the range U+2FF0–U+2FFB, but The Unicode Standard warns against using its characters as an alternate representation for characters encoded elsewhere:\n\nThis process is different from a formal encoding of an ideograph. There is no canonical description of unencoded ideographs; there is no semantic assigned to described ideographs; there is no equivalence defined for described ideographs. Conceptually, ideographic descriptions are more akin to the English phrase \"an 'e' with an acute accent on it\" than to the character sequence <U+0065, U+0301>.\n\nMany scripts, including Arabic and Devanāgarī, have special orthographic rules that require certain combinations of letterforms to be combined into special ligature forms. The rules governing ligature formation can be quite complex, requiring special script-shaping technologies such as ACE (Arabic Calligraphic Engine by DecoType in the 1980s and used to generate all the Arabic examples in the printed editions of The Unicode Standard), which became the proof of concept for OpenType (by Adobe and Microsoft), Graphite (by SIL International), or AAT (by Apple).\n\nInstructions are also embedded in fonts to tell the operating system how to properly output different character sequences. A simple solution to the placement of combining marks or diacritics is assigning the marks a width of zero and placing the glyph itself to the left or right of the left sidebearing (depending on the direction of the script they are intended to be used with). A mark handled this way will appear over whatever character precedes it, but will not adjust its position relative to the width or height of the base glyph; it may be visually awkward and it may overlap some glyphs. Real stacking is impossible but can be approximated in limited cases (for example, Thai top-combining vowels and tone marks can just be at different heights to start with). Generally, this approach is only effective in monospaced fonts but may be used as a fallback rendering method when more complex methods fail.\n\nSeveral subsets of Unicode are standardized: Microsoft Windows since Windows NT 4.0 supports WGL-4 with 657 characters, which is considered to support all contemporary European languages using the Latin, Greek, or Cyrillic script. Other standardized subsets of Unicode include the Multilingual European Subsets:[70] MES-1 (Latin scripts only; 335 characters), MES-2 (Latin, Greek, and Cyrillic; 1062 characters)[71] and MES-3A & MES-3B (two larger subsets, not shown here). MES-2 includes every character in MES-1 and WGL-4.\n\nThe standard DIN 91379[72] specifies a subset of Unicode letters, special characters, and sequences of letters and diacritic signs to allow the correct representation of names and to simplify data exchange in Europe. This standard supports all of the official languages of all European Union countries, as well as the German minority languages and the official languages of Iceland, Liechtenstein, Norway, and Switzerland. To allow the transliteration of names in other writing systems to the Latin script according to the relevant ISO standards, all necessary combinations of base letters and diacritic signs are provided.\n\nRendering software that cannot process a Unicode character appropriately often displays it as an open rectangle, or as U+FFFD to indicate the position of the unrecognized character. Some systems have made attempts to provide more information about such characters. Apple's Last Resort font will display a substitute glyph indicating the Unicode range of the character, and the SIL International's Unicode fallback font will display a box showing the hexadecimal scalar value of the character.\n\nSeveral mechanisms have been specified for storing a series of code points as a series of bytes.\n\nUnicode defines two mapping methods: the Unicode Transformation Format (UTF) encodings, and the Universal Coded Character Set (UCS) encodings. An encoding maps (possibly a subset of) the range of Unicode code points to sequences of values in some fixed-size range, termed code units. All UTF encodings map code points to a unique sequence of bytes.[73] The numbers in the names of the encodings indicate the number of bits per code unit (for UTF encodings) or the number of bytes per code unit (for UCS encodings and UTF-1). UTF-8 and UTF-16 are the most commonly used encodings. UCS-2 is an obsolete subset of UTF-16; UCS-4 and UTF-32 are functionally equivalent.\n\nUTF encodings include:\n\nUTF-8 uses one to four 8-bit units (bytes) per code point and, being compact for Latin scripts and ASCII-compatible, provides the de facto standard encoding for the interchange of Unicode text. It is used by FreeBSD and most recent Linux distributions as a direct replacement for legacy encodings in general text handling.\n\nThe UCS-2 and UTF-16 encodings specify the Unicode byte order mark (BOM) for use at the beginnings of text files, which may be used for byte-order detection (or byte endianness detection). The BOM, encoded as U+FEFF ZERO WIDTH NO-BREAK SPACE, has the important property of unambiguity on byte reorder, regardless of the Unicode encoding used; U+FFFE (the result of byte-swapping U+FEFF) does not equate to a legal character, and U+FEFF in places other than the beginning of text conveys the zero-width non-break space.\n\nThe same character converted to UTF-8 becomes the byte sequence EF BB BF. The Unicode Standard allows the BOM \"can serve as a signature for UTF-8 encoded text where the character set is unmarked\".[74] Some software developers have adopted it for other encodings, including UTF-8, in an attempt to distinguish UTF-8 from local 8-bit code pages. However RFC 3629, the UTF-8 standard, recommends that byte order marks be forbidden in protocols using UTF-8, but discusses the cases where this may not be possible. In addition, the large restriction on possible patterns in UTF-8 (for instance there cannot be any lone bytes with the high bit set) means that it should be possible to distinguish UTF-8 from other character encodings without relying on the BOM.\n\nIn UTF-32 and UCS-4, one 32-bit code unit serves as a fairly direct representation of any character's code point (although the endianness, which varies across different platforms, affects how the code unit manifests as a byte sequence). In the other encodings, each code point may be represented by a variable number of code units. UTF-32 is widely used as an internal representation of text in programs (as opposed to stored or transmitted text), since every Unix operating system that uses the gcc compilers to generate software uses it as the standard \"wide character\" encoding. Some programming languages, such as Seed7, use UTF-32 as an internal representation for strings and characters. Recent versions of the Python programming language (beginning with 2.2) may also be configured to use UTF-32 as the representation for Unicode strings, effectively disseminating such encoding in high-level coded software.\n\nPunycode, another encoding form, enables the encoding of Unicode strings into the limited character set supported by the ASCII-based Domain Name System (DNS). The encoding is used as part of IDNA, which is a system enabling the use of Internationalized Domain Names in all scripts that are supported by Unicode. Earlier and now historical proposals include UTF-5 and UTF-6.\n\nGB18030 is another encoding form for Unicode, from the Standardization Administration of China. It is the official character set of the People's Republic of China (PRC). BOCU-1 and SCSU are Unicode compression schemes. The April Fools' Day RFC of 2005 specified two parody UTF encodings, UTF-9 and UTF-18.\n\nUnicode, in the form of UTF-8, has been the most common encoding for the World Wide Web since 2008.[75] It has near-universal adoption, and much of the non-UTF-8 content is found in other Unicode encodings, e.g. UTF-16. As of 2024[update], UTF-8 accounts for on average 97.8% of all web pages (and 987 of the top 1,000 highest-ranked web pages).[76] Although many pages only use ASCII characters to display content, UTF-8 was designed with 8-bit ASCII as a subset and almost no websites now declare their encoding to only be ASCII instead of UTF-8.[77] Over a third of the languages tracked have 100% UTF-8 use.\n\nAll internet protocols maintained by Internet Engineering Task Force, e.g. FTP,[78] have required support for UTF-8 since the publication of RFC 2277 in 1998, which specified that all IETF protocols \"MUST be able to use the UTF-8 charset\".[79]\n\nUnicode has become the dominant scheme for the internal processing and storage of text. Although a great deal of text is still stored in legacy encodings, Unicode is used almost exclusively for building new information processing systems. Early adopters tended to use UCS-2 (the fixed-length two-byte obsolete precursor to UTF-16) and later moved to UTF-16 (the variable-length current standard), as this was the least disruptive way to add support for non-BMP characters. The best known such system is Windows NT (and its descendants, 2000, XP, Vista, 7, 8, 10, and 11), which uses UTF-16 as the sole internal character encoding. The Java and .NET bytecode environments, macOS, and KDE also use it for internal representation. Partial support for Unicode can be installed on Windows 9x through the Microsoft Layer for Unicode.\n\nUTF-8 (originally developed for Plan 9)[80] has become the main storage encoding on most Unix-like operating systems (though others are also used by some libraries) because it is a relatively easy replacement for traditional extended ASCII character sets. UTF-8 is also the most common Unicode encoding used in HTML documents on the World Wide Web.\n\nMultilingual text-rendering engines which use Unicode include Uniscribe and DirectWrite for Microsoft Windows, ATSUI and Core Text for macOS, and Pango for GTK+ and the GNOME desktop.\n\nBecause keyboard layouts cannot have simple key combinations for all characters, several operating systems provide alternative input methods that allow access to the entire repertoire.\n\nISO/IEC 14755,[81] which standardises methods for entering Unicode characters from their code points, specifies several methods. There is the Basic method, where a beginning sequence is followed by the hexadecimal representation of the code point and the ending sequence. There is also a screen-selection entry method specified, where the characters are listed in a table on a screen, such as with a character map program.\n\nOnline tools for finding the code point for a known character include Unicode Lookup[82] by Jonathan Hedley and Shapecatcher[83] by Benjamin Milde. In Unicode Lookup, one enters a search key (e.g. \"fractions\"), and a list of corresponding characters with their code points is returned. In Shapecatcher, based on Shape context, one draws the character in a box and a list of characters approximating the drawing, with their code points, is returned.\n\nMIME defines two different mechanisms for encoding non-ASCII characters in email, depending on whether the characters are in email headers (such as the \"Subject:\"), or in the text body of the message; in both cases, the original character set is identified as well as a transfer encoding. For email transmission of Unicode, the UTF-8 character set and the Base64 or the Quoted-printable transfer encoding are recommended, depending on whether much of the message consists of ASCII characters. The details of the two different mechanisms are specified in the MIME standards and generally are hidden from users of email software.\n\nThe IETF has defined[84][85] a framework for internationalized email using UTF-8, and has updated[86][87][88][89] several protocols in accordance with that framework.\n\nThe adoption of Unicode in email has been very slow.[citation needed] Some East Asian text is still encoded in encodings such as ISO-2022, and some devices, such as mobile phones,[citation needed] still cannot correctly handle Unicode data. Support has been improving, however. Many major free mail providers such as Yahoo! Mail, Gmail, and   support it.\n\nAll W3C recommendations have used Unicode as their document character set since HTML 4.0. Web browsers have supported Unicode, especially UTF-8, for many years. There used to be display problems resulting primarily from font related issues; e.g. v6 and older of Microsoft Internet Explorer did not render many code points unless explicitly told to use a font that contains them.[90]\n\nAlthough syntax rules may affect the order in which characters are allowed to appear, XML (including XHTML) documents, by definition,[91] comprise characters from most of the Unicode code points, with the exception of:\n\nHTML characters manifest either directly as bytes according to the document's encoding, if the encoding supports them, or users may write them as numeric character references based on the character's Unicode code point. For example, the references &#916;, &#1049;, &#1511;, &#1605;, &#3671;, &#12354;, &#21494;, &#33865;, and &#47568; (or the same numeric values expressed in hexadecimal, with &#x as the prefix) should display on all browsers as Δ, Й, ק ,م, ๗, あ, 叶, 葉, and 말.\n\nWhen specifying URIs, for example as URLs in HTTP requests, non-ASCII characters must be percent-encoded.\n\nUnicode is not in principle concerned with fonts per se, seeing them as implementation choices.[92] Any given character may have many allographs, from the more common bold, italic and base letterforms to complex decorative styles. A font is \"Unicode compliant\" if the glyphs in the font can be accessed using code points defined in The Unicode Standard.[93] The standard does not specify a minimum number of characters that must be included in the font; some fonts have quite a small repertoire.\n\nFree and retail fonts based on Unicode are widely available, since TrueType and OpenType support Unicode (and Web Open Font Format (WOFF and WOFF2) is based on those). These font formats map Unicode code points to glyphs, but OpenType and TrueType font files are restricted to 65,535 glyphs. Collection files provide a \"gap mode\" mechanism for overcoming this limit in a single font file. (Each font within the collection still has the 65,535 limit, however.) A TrueType Collection file would typically have a file extension of \".ttc\".\n\nThousands of fonts exist on the market, but fewer than a dozen fonts—sometimes described as \"pan-Unicode\" fonts—attempt to support the majority of Unicode's character repertoire. Instead, Unicode-based fonts typically focus on supporting only basic ASCII and particular scripts or sets of characters or symbols. Several reasons justify this approach: applications and documents rarely need to render characters from more than one or two writing systems; fonts tend to demand resources in computing environments; and operating systems and applications show increasing intelligence in regard to obtaining glyph information from separate font files as needed, i.e., font substitution. Furthermore, designing a consistent set of rendering instructions for tens of thousands of glyphs constitutes a monumental task; such a venture passes the point of diminishing returns for most typefaces.\n\nUnicode partially addresses the newline problem that occurs when trying to read a text file on different platforms. Unicode defines a large number of characters that conforming applications should recognize as line terminators.\n\nIn terms of the newline, Unicode introduced U+2028 LINE SEPARATOR and U+2029 PARAGRAPH SEPARATOR. This was an attempt to provide a Unicode solution to encoding paragraphs and lines semantically, potentially replacing all of the various platform solutions. In doing so, Unicode does provide a way around the historical platform-dependent solutions. Nonetheless, few if any Unicode solutions have adopted these Unicode line and paragraph separators as the sole canonical line ending characters. However, a common approach to solving this issue is through newline normalization. This is achieved with the Cocoa text system in Mac OS X and also with W3C XML and HTML recommendations. In this approach, every possible newline character is converted internally to a common newline (which one does not really matter since it is an internal operation just for rendering). In other words, the text system can correctly treat the character as a newline, regardless of the input's actual encoding.\n\nThe Ideographic Research Group (IRG) is tasked with advising the Consortium and ISO regarding Han unification, or Unihan, especially the further addition of CJK unified and compatibility ideographs to the repertoire. The IRG is composed of experts from each region that has historically used Chinese characters. However, despite the deliberation within the committee, Han unification has consistently been one of the most contested aspects of The Unicode Standard since the genesis of the project.[94]\n\nExisting character set standards such as the Japanese JIS X 0208 (encoded by Shift JIS) defined unification criteria, meaning rules for determining when a variant Chinese character is to be considered a handwriting/font difference (and thus unified), versus a spelling difference (to be encoded separately). Unicode's character model for CJK characters was based on the unification criteria used by JIS X 0208, as well as those developed by the Association for a Common Chinese Code in China.[95] Due to the standard's principle of encoding semantic instead of stylistic variants, Unicode has received criticism for not assigning code points to certain rare and archaic kanji variants, possibly complicating processing of ancient and uncommon Japanese names. Since it places particular emphasis on Chinese, Japanese and Korean sharing many characters in common, Han unification is also sometimes perceived as treating the three as the same thing.[96]\n\nLess-frequently-used alternative encodings exist, often predating Unicode, with character models differing from this paradigm, aimed at preserving the various stylistic differences between regional and/or nonstandard character forms. One example is the TRON Code favored by some users for handling historical Japanese text, though not widely adopted among the Japanese public. Another is the CCCII encoding adopted by library systems in Hong Kong, Taiwan and the United States. These have their own drawbacks in general use, leading to the Big5 encoding (introduced in 1984, four years after CCCII) having become more common than CCCII outside of library systems.[97] Although work at Apple based on Research Libraries Group's CJK Thesaurus, which was used to maintain the EACC variant of CCCII, was one of the direct predecessors of Unicode's Unihan set, Unicode adopted the JIS-style unification model.[95]\n\nThe earliest version of Unicode had a repertoire of fewer than 21,000 Han characters, largely limited to those in relatively common modern usage. As of version 15.1, the standard now encodes more than 97,000 Han characters, and work is continuing to add thousands more—largely historical and dialectal variant characters used throughout the Sinosphere.\n\nModern typefaces provide a means to address some of the practical issues in depicting unified Han characters with various regional graphical representations. The 'locl' OpenType table allows a renderer to select a different glyph for each code point based on the text locale.[98] The Unicode variation sequences can also provide in-text annotations for a desired glyph selection; this requires registration of the specific variant in the Ideographic Variation Database.\n\nIf the appropriate glyphs for characters in the same script differ only in the italic, Unicode has generally unified them, as can be seen in the comparison among a set of seven characters' italic glyphs as typically appearing in Russian, traditional Bulgarian, Macedonian, and Serbian texts at right, meaning that the differences are displayed through smart font technology or manually changing fonts. The same OpenType 'locl' technique is used.[99]\n\nFor use in the Turkish alphabet and Azeri alphabet, Unicode includes a separate dotless lowercase I (ı) and a dotted uppercase I (İ). However, the usual ASCII letters are used for the lowercase dotted I and the uppercase dotless I, matching how they are handled in the earlier ISO 8859-9. As such, case-insensitive comparisons for those languages have to use different rules than case-insensitive comparisons for other languages using the Latin script.[100]\n\nBy contrast, the Icelandic eth (ð), the barred D (đ) and the retroflex D (ɖ), which usually[note 4] look the same in uppercase (Đ), are given the opposite treatment, and encoded separately in both letter-cases (in contrast to the earlier ISO 6937, which unifies the uppercase forms). Although it allows for case-insensitive comparison without needing to know the language of the text, this approach also has issues, requiring security measures relating to homoglyph attacks.[101]\n\nWhether the lowercase letter I is expected to retain its tittle when a diacritic applies also depends on local conventions.\n\nUnicode has a large number of homoglyphs, many of which look very similar or identical to ASCII letters. Substitution of these can make an identifier or URL that looks correct, but directs to a different location than expected.[102] Additionally, homoglyphs can also be used for manipulating the output of natural language processing (NLP) systems.[103] Mitigation requires disallowing these characters, displaying them differently, or requiring that they resolve to the same identifier;[104] all of this is complicated due to the huge and constantly changing set of characters.[105][106]\n\nA security advisory was released in 2021 by two researchers, one from the University of Cambridge and the other from the University of Edinburgh, in which they assert that the BiDi marks can be used to make large sections of code do something different from what they appear to do. The problem was named \"Trojan Source\".[107] In response, code editors started highlighting marks to indicate forced text-direction changes.[108]\n\nUnicode was designed to provide code-point-by-code-point round-trip format conversion to and from any preexisting character encodings, so that text files in older character sets can be converted to Unicode and then back and get back the same file, without employing context-dependent interpretation. That has meant that inconsistent legacy architectures, such as combining diacritics and precomposed characters, both exist in Unicode, giving more than one method of representing some text. This is most pronounced in the three different encoding forms for Korean Hangul. Since version 3.0, any precomposed characters that can be represented by a combined sequence of already existing characters can no longer be added to the standard to preserve interoperability between software using different versions of Unicode.\n\nInjective mappings must be provided between characters in existing legacy character sets and characters in Unicode to facilitate conversion to Unicode and allow interoperability with legacy software. Lack of consistency in various mappings between earlier Japanese encodings such as Shift-JIS or EUC-JP and Unicode led to round-trip format conversion mismatches, particularly the mapping of the character JIS X 0208 '～' (1-33, WAVE DASH), heavily used in legacy database data, to either U+FF5E ～ FULLWIDTH TILDE (in Microsoft Windows) or U+301C 〜 WAVE DASH (other vendors).[109]\n\nSome Japanese computer programmers objected to Unicode because it requires them to separate the use of U+005C \\ REVERSE SOLIDUS (backslash) and U+00A5 ¥ YEN SIGN, which was mapped to 0x5C in JIS X 0201, and a lot of legacy code exists with this usage.[110] (This encoding also replaces tilde '~' 0x7E with macron '¯', now 0xAF.) The separation of these characters exists in ISO 8859-1, from long before Unicode.\n\nIndic scripts such as Tamil and Devanagari are each allocated only 128 code points, matching the ISCII standard. The correct rendering of Unicode Indic text requires transforming the stored logical order characters into visual order and the forming of ligatures (also known as conjuncts) out of components. Some local scholars argued in favor of assignments of Unicode code points to these ligatures, going against the practice for other writing systems, though Unicode contains some Arabic and other ligatures for backward compatibility purposes only.[111][112][113] Encoding of any new ligatures in Unicode will not happen, in part, because the set of ligatures is font-dependent, and Unicode is an encoding independent of font variations. The same kind of issue arose for the Tibetan script in 2003 when the Standardization Administration of China proposed encoding 956 precomposed Tibetan syllables,[114] but these were rejected for encoding by the relevant ISO committee (ISO/IEC JTC 1/SC 2).[115]\n\nThai alphabet support has been criticized for its ordering of Thai characters. The vowels เ, แ, โ, ใ, ไ that are written to the left of the preceding consonant are in visual order instead of phonetic order, unlike the Unicode representations of other Indic scripts. This complication is due to Unicode inheriting the Thai Industrial Standard 620, which worked in the same way, and was the way in which Thai had always been written on keyboards. This ordering problem complicates the Unicode collation process slightly, requiring table lookups to reorder Thai characters for collation.[96] Even if Unicode had adopted encoding according to spoken order, it would still be problematic to collate words in dictionary order. E.g., the word แสดง [sa dɛːŋ] \"perform\" starts with a consonant cluster \"สด\" (with an inherent vowel for the consonant \"ส\"), the vowel แ-, in spoken order would come after the ด, but in a dictionary, the word is collated as it is written, with the vowel following the ส.\n\nCharacters with diacritical marks can generally be represented either as a single precomposed character or as a decomposed sequence of a base letter plus one or more non-spacing marks. For example, ḗ (precomposed e with macron and acute above) and ḗ (e followed by the combining macron above and combining acute above) should be rendered identically, both appearing as an e with a macron (◌̄) and acute accent (◌́), but in practice, their appearance may vary depending upon what rendering engine and fonts are being used to display the characters. Similarly, underdots, as needed in the romanization of Indic languages, will often be placed incorrectly.[citation needed] Unicode characters that map to precomposed glyphs can be used in many cases, thus avoiding the problem, but where no precomposed character has been encoded, the problem can often be solved by using a specialist Unicode font such as Charis SIL that uses Graphite, OpenType ('gsub'), or AAT technologies for advanced rendering features.\n\nThe Unicode Standard has imposed rules intended to guarantee stability.[116] Depending on the strictness of a rule, a change can be prohibited or allowed. For example, a \"name\" given to a code point cannot and will not change. But a \"script\" property is more flexible, by Unicode's own rules. In version 2.0, Unicode changed many code point \"names\" from version 1. At the same moment, Unicode stated that, thenceforth, an assigned name to a code point would never change. This implies that when mistakes are published, these mistakes cannot be corrected, even if they are trivial (as happened in one instance with the spelling BRAKCET for BRACKET in a character name). In 2006 a list of anomalies in character names was first published, and, as of June 2021, there were 104 characters with identified issues,[117] for example:\n\nWhile Unicode defines the script designator (name) to be \"Phags_Pa\", in that script's character names, a hyphen is added: U+A840 ꡀ PHAGS-PA LETTER KA.[120][121] This, however, is not an anomaly, but the rule: hyphens are replaced by underscores in script designators.[120]",
		"url": "https://en.wikipedia.org/wiki/Unicode"
	},
	"es": {
		"content": "Unicode es un estándar de codificación de caracteres diseñado para facilitar el tratamiento informático, transmisión y visualización de textos de numerosos idiomas y disciplinas técnicas, además de textos clásicos de lenguas muertas. El término Unicode proviene de los tres objetivos perseguidos: universalidad, uniformidad, y unicidad.[1]​\n\nUnicode define cada carácter o símbolo mediante un nombre e identificador numérico, el punto de código (code point). Además incluye otras informaciones para el uso correcto de cada carácter, como sistema de escritura, categoría, direccionalidad, mayúsculas y otros atributos. Unicode trata los caracteres alfabéticos, ideográficos y símbolos de forma equivalente, lo que significa que se pueden mezclar en un mismo texto sin utilizar marcas o caracteres de control.[2]​\n\nEste estándar es mantenido por el Unicode Technical Committee (UTC), integrado en el Consorcio Unicode, del que forman parte con distinto grado de implicación empresas como: Microsoft, Apple, Adobe, IBM, Oracle, SAP, Google o Facebook, instituciones como la Universidad de Berkeley, o el Gobierno de la India y profesionales y académicos a título individual.[3]​ El Unicode Consortium mantiene estrecha relación con ISO/IEC, con la que mantiene desde 1991 el acuerdo de sincronizar sus estándares que contienen los mismos caracteres y puntos de código.[4]​\n\nLa creación de Unicode ha sido un proyecto de gran relevancia con el objetivo de reemplazar los esquemas de codificación de caracteres existentes, los cuales presentaban limitaciones significativas en tamaño y compatibilidad con entornos plurilingües. Unicode se ha convertido en el esquema de codificación de caracteres más completo y extenso, siendo el dominante en la internacionalización y adaptación local del software informático. Este estándar ha sido ampliamente adoptado en diversas tecnologías recientes, como XML, Java y sistemas operativos modernos.\n\nLa descripción exhaustiva del estándar y las tablas de caracteres están disponibles en el sitio web oficial de Unicode. Cada vez que se finaliza una nueva versión principal, se pública una referencia completa en formato de libro, también está disponible en su versión digital de manera gratuita. Las revisiones y adiciones se publican de manera independiente.\n\nUnicode engloba todos los caracteres de uso común en la actualidad. La versión 15.0, por ejemplo, cuenta con 149.186 caracteres provenientes de diversos alfabetos, sistemas ideográficos y colecciones de símbolos, como aquellos utilizados en matemáticas, tecnología, música e iconografía. Esta cifra continúa aumentando en cada nueva versión.[5]​\n\nUnicode abarca una amplia gama de sistemas de escritura modernos, como el alfabeto latino, así como escrituras históricas extintas, utilizadas con fines académicos, tales como el cuneiforme y el rúnico. Además de los caracteres alfabéticos, Unicode también incluye una variedad de caracteres no alfabéticos, como símbolos musicales y matemáticos, fichas de juegos como el dominó, flechas, iconos, etc.\n\nAdemás, Unicode incluye los signos diacríticos como caracteres individuales que pueden combinarse con otros caracteres, también ofrece versiones predefinidas de la mayoría de las letras con símbolos diacríticos utilizados en la actualidad, como las vocales acentuadas del español.\n\nUnicode es un estándar en constante evolución, y se agregan nuevos caracteres de forma continua. Sin embargo, también se descartan ciertos alfabetos propuestos por diversas razones, como es el caso del alfabeto klingon.[6]​\n\nUnicode está sincronizado con el estándar ISO/IEC conocido como UCS o Juego de Caracteres Universal. Desde un punto de vista técnico, Unicode incluye o es compatible con codificaciones previas como ASCII7 o ISO 8859-1, así como con estándares nacionales como ANSI Z39.64, KS X 1001, JIS X 0208, JIS X 0212, JIS X 0213, GB 2312, GB 18030, HKSCS y CNS 11643, codificaciones particulares de fabricantes de software como Apple, Adobe, Microsoft, IBM, etc. Además, Unicode reserva espacio para que los fabricantes de software puedan crear extensiones para su propio uso.[7]​\n\nEl elemento básico del estándar Unicode es el carácter. Se considera un carácter al elemento más pequeño de un sistema de escritura con significado. El estándar Unicode codifica los caracteres esenciales ―grafemas― definiéndolos de forma abstracta y deja la representación visual (tamaño, dimensión, fuente o estilo) al software que lo trate, como procesadores de texto o navegadores web. Se incluyen letras, signos diacríticos, caracteres de puntuación, ideogramas, caracteres silábicos, caracteres de control y otros símbolos. Los caracteres se agrupan en alfabetos o sistemas de escritura. Se considera que son diferentes los caracteres de alfabetos distintos, aunque compartan forma y significación.\n\nLos caracteres se identifican mediante un número o punto de código y su nombre o descripción. Cuando se ha asignado un código a un carácter, se dice que dicho carácter está codificado. El espacio para códigos tiene 1.114.112 posiciones posibles (0x10FFFF). Los puntos de código se representan utilizando notación hexadecimal agregando el prefijo U+. El valor hexadecimal se completa con ceros hasta 4 dígitos hexadecimales cuando es necesario; si es de longitud mayor que 4 dígitos no se agregan ceros.\n\nLos bloques del espacio de códigos contienen puntos con la siguiente información:[8]​\n\nUnicode incluye un mecanismo para formar caracteres y así extender el repertorio de compatibilidad con los símbolos existentes. Un carácter base se complementa con marcas: signos diacríticos, de puntuación o marcos. El tipo de cada carácter y sus atributos definen el papel que pueden jugar en una combinación. Por este motivo, puede haber varias opciones que representen el mismo carácter. Para facilitar la compatibilidad con codificaciones anteriores, se proporcionan caracteres precompuestos; en la definición de dichos caracteres se hace constar qué caracteres intervienen en la composición.\n\nUn grupo de caracteres consecutivos, independientemente de su tipo, forma una secuencia. En caso de que varias secuencias representen el mismo conjunto de caracteres esenciales, el estándar no define una de ellas como 'correcta', sino que las considera equivalentes. Para poder identificar dichas equivalencias, Unicode define los mecanismos de equivalencia canónica y de equivalencia de compatibilidad basados en la obtención de formas normalizadas de las cadenas a comparar.\n\nEn el estándar Unicode, los ideogramas de Asia oriental (popularmente llamados «caracteres chinos») se denominan «ideogramas han». Estos ideogramas se desarrollaron en China y fueron adaptados por culturas próximas para su propio uso.[9]​[10]​ Japón, Corea y Vietnam desarrollaron sus propios sistemas alfabéticos o silábicos para usar en combinación con los símbolos chinos: hiragana y katakana (en Japón), hangul (en Corea) y yi (en Vietnam). La evolución natural de los sistemas de escritura y los distintos momentos de entrada de los caracteres en las distintas culturas han marcado diferencias en los ideogramas utilizados. Unicode considera las distintas versiones de los ideogramas como variantes de un mismo carácter abstracto, es decir, como resultado de la aplicación de un tipo de letra diferente en cada caso y considera las variantes nacionales como pertenecientes a un mismo sistema de escritura. La versión original del estándar se desarrolló a partir de los estándares industriales existentes en los países afectados.\n\nEl organismo encargado de desarrollar el repertorio de caracteres es el Ideographic Rapporteur Group (IRG). IRG es un grupo de trabajo integrado en ISO/IEC JTC1/SC2/WG2, incluyendo a China, Hong Kong, Macao, Taipei Computer Association, Singapur, Japón, Corea del Sur, Corea del Norte, Vietnam y Estados Unidos de América.[9]​\n\nLa base de datos de caracteres CJK se denomina Unihan y contiene, además, información auxiliar sobre significado, conversiones, datos necesarios para utilizarlos en los diferentes lenguajes que los utilizan. A continuación se muestran los bloques que describen este repertorio. IRG define los caracteres de los siete grupos unificados; los dos grupos siguientes contienen caracteres para compatibilidad con estándares anteriores.\n\nSe admite que nunca se podrá finalizar la tarea de incluir ideogramas en el estándar debido, principalmente, a que la creación de nuevos ideogramas continúa. A fin de suplir eventuales carencias, Unicode ofrece un mecanismo que permite la representación de los símbolos que faltan denominado «secuencias de descripción ideográfica». Se basa en que en la práctica, la totalidad de los ideogramas se puede descomponer en piezas más pequeñas que, a su vez, son ideogramas. Aunque sea posible la representación de un símbolo mediante una secuencia, el estándar específica que siempre que exista una versión codificada su uso debe ser preferente. No hay un método para la «descomposición canónica» de ideogramas ni algoritmos de equivalencia por lo que las operaciones sobre el texto, como búsqueda u ordenación, pueden fallar.\n\nUnicode define 12 caracteres de control para la descripción de ideogramas representando distintas posibilidades de combinación espacial de otros caracteres han.\n\nEl estándar fue diseñado con los siguientes objetivos:\n\nEl conjunto de caracteres codificados por Unicode, es la UCD (unicode character database: base de datos de caracteres Unicode). Además de nombre y punto de código, incluye más información: alfabeto al que pertenece, nombre, clasificación, mayúsculas, orientación y otras formas de uso, variantes estandarizadas, reglas de combinación, etc.\n\nFormalmente la base de datos se divide en planos y estos a su vez en áreas y bloques. Con excepciones, los caracteres codificados se agrupan en el espacio de códigos siguiendo categorías como alfabeto o sistema de escritura, de forma que caracteres relacionados se encuentren cerca en las tablas de codificación.\n\nPor conveniencia se ha dividido el espacio de códigos en grandes grupos denominados planos. Cada plano contiene un máximo de 65 536 caracteres. Dado un punto de código expresado en hexadecimal, los 4 últimos dígitos determinan la posición del carácter en el plano.\n\nLos distintos planos se dividen en áreas de direccionamiento en función de los tipos generales que incluyen. Esta división es convencional, no reglada y puede variar con el tiempo. Las áreas se dividen, a su vez, en bloques. Los bloques están definidos normativamente y son rangos consecutivos del espacio de códigos. Los bloques se utilizan para formar las tablas impresas de caracteres pero no deben tomarse como definiciones de grupos significativos de caracteres.\n\nLos puntos de código de Unicode se identifican por un número entero. Según su arquitectura, un ordenador utilizará unidades de 8, 16 o 32 bits para representar dichos enteros. Las formas de codificación de Unicode reglamentan la forma en que los puntos de código se transformarán en unidades tratables por el computador.\n\nUnicode define tres formas de codificación bajo el nombre UTF (Unicode transformation format: formato de transformación Unicode):[11]​\n\nLas formas de codificación se limitan a describir el modo en que se representan los puntos de código en formato inteligible por la máquina. A partir de las 3 formas identificadas se definen 7 esquemas de codificación.\n\nLos esquemas de codificación tratan de la forma en que se serializa la información codificada.[11]​ La seguridad en los intercambios de información entre sistemas heterogéneos requiere la implementación de sistemas que permitan determinar el orden correcto de los bits y bytes y garantizar que la reconstrucción de la información es correcta. Una diferencia fundamental entre procesadores es el orden de disposición de los bytes en palabras de 16 y 32 bits, lo que se denomina endianness. Los esquemas de codificación deben garantizar que los extremos de una comunicación saben cómo interpretar la información recibida. A partir de las 3 formas de codificación se definen 7 esquemas. A pesar de que comparten nombres, no debe confundirse esquemas y formas de codificación.\n\nUnicode define una marca especial, la marca de orden de bytes (BOM, Byte Order Mark), al inicio de un fichero o una comunicación para hacer explícita la ordenación de bytes. Cuando un protocolo superior específica el orden de bytes, la marca no es necesaria y puede omitirse dando lugar a los esquemas de la lista anterior con sufijo BE o LE. En los esquemas UTF-16 y UTF-32, que admiten BOM, si este no se específica se asume que la ordenación de bytes es big-endian.\n\nLa unidad de codificación en UTF-8 es el byte por lo que no necesita una indicación de orden de byte. El estándar ni requiere ni recomienda la utilización de BOM, pero lo admite como marca de que el texto es Unicode o como resultado de la conversión de otros esquemas.\n\nEl proyecto Unicode se inició a finales de 1987, tras conversaciones entre Joe Becker, Lee Collins y Mark Davis (ingenieros de las empresas Apple y Xerox).[12]​ Como resultado de su colaboración, en agosto de 1988 se publicó el primer borrador de Unicode bajo el nombre de Unicode88.[13]​ En esta primera versión se consideraba que solo se codificarían los caracteres necesarios para el uso moderno, por lo que se utilizaron códigos de 16 bits.\n\nDurante el año 1989 se sumaron colaboradores de otras compañías como Microsoft o Sun Microsystems. El 3 de enero de 1991 se formó el Consorcio Unicode, y en octubre de 1991 se publicó la primera versión del estándar. La segunda versión, que ya incluía la escritura ideográfica han se publicó en junio de 1992. A continuación se muestra una tabla con las distintas versiones del Estándar Unicode con sus adiciones o modificaciones más importantes.\n\nISO/IEC 10646-2:2001\n\nISO/IEC 10646-2:2001\n\nLa versión 6.0 es la primera versión principal del estándar publicada exclusivamente en soporte electrónico. Agregados mandeo, batak y brahmi, ampliaciones de lenguajes africanos como tifinagh, etíope y bamúm. Otras adiciones importantes son: 222 ideogramas CJK, 1000 símbolos incluyendo los pictogramas emoji, el nuevo símbolo oficial para la rupia y símbolos alquímicos además de ampliaciones de los atributos de los caracteres y otras modificaciones normativas y algorítmicas.[18]​",
		"url": "https://es.wikipedia.org/wiki/Unicode"
	},
	"got": {
		"content": "𐌲𐌿𐍄𐌰𐍂𐌰𐌶𐌳𐌰, 𐌲𐌿𐍄𐍂𐌰𐌶𐌳𐌰 𐌰𐌹𐌸𐌸𐌰𐌿 𐌲𐌿𐍄𐌹𐍃𐌺𐌰 𐍂𐌰𐌶𐌳𐌰 𐌹𐍃𐍄 𐌲𐌰𐍃𐍅𐌿𐌻𐍄𐌰𐌽𐌰 𐌰𐌿𐍃𐍄𐍂𐌰𐌲𐌰𐌹𐍂𐌼𐌰𐌽𐌹𐍃𐌺𐌰 𐍂𐌰𐌶𐌳𐌰 𐍂𐍉𐌳𐌹𐌳𐌰 𐍆𐍂𐌰𐌼 𐌲𐌿𐍄𐌰𐌼. 𐍃𐌹 𐌹𐍃𐍄 𐌰𐌹𐌽𐌰𐌷𐍉 𐌰𐌿𐍃𐍄𐍂𐌰𐌲𐌰𐌹𐌼𐌰𐌽𐌹𐍃𐌺𐌰 𐍂𐌰𐌶𐌳𐌰 𐍃𐍉𐌴𐌹 𐌷𐌰𐌱𐌰𐌹𐌸 𐌲𐌰𐌼𐌴𐌻𐌴𐌹𐌽𐌹𐌽𐍃.\n\n𐌲𐌿𐍄𐌰𐍂𐌰𐌶𐌳𐌰 𐌹̈𐍃𐍄 𐌹̈𐌽𐌳𐍉𐌰𐌹𐍅𐍂𐍉𐍀𐌹𐍃𐌺𐌰 𐍂𐌰𐌶𐌳𐌰. 𐍅𐌰𐍂𐌸 𐍆𐌰𐌿𐍂𐌸𐌹𐍃 𐌲𐌰𐌼𐌴𐌻𐌹𐌳𐌰 𐌹𐌽 𐌺𐍉𐌳𐌰𐌹𐌺𐌰 𐌰𐍂𐌲𐌰𐌹𐌽𐍄𐌰𐌿 𐌼𐌹𐌸𐌸𐌰𐌽𐌴𐌹 𐍆𐌹𐌳𐍅𐍉𐍂𐌷𐌿𐌽𐌳𐌹𐌽 𐌾𐌴𐍂𐌰𐌷𐌿𐌽𐌳𐌰. 𐌳𐌿𐌲𐌰𐌽𐌽 𐌼𐌹𐌸𐌸𐌰𐌽𐌴𐌹 𐍃𐌰𐌹𐌷𐍃𐍄𐌹𐌽 𐍄𐌰𐌹𐌷𐌿𐌽𐍄𐌰𐌹𐌷𐌿𐌽𐌳𐌹𐌽 𐌾𐌴𐍂𐌰 𐌲𐌰𐍃𐍅𐌹 𐌹𐍃𐍄 𐍃𐍉 𐍆𐍂𐌿𐌼𐌹𐍃𐍄𐍉 𐌲𐌰𐌹𐍂𐌼𐌰𐌽𐌹𐍃𐌺𐍉 𐍂𐌰𐌶𐌳𐌰 𐌼𐌹𐌸 𐌲𐌰𐌼𐌴𐌻𐌴𐌹𐌽𐌹𐌼. 𐌷𐌰𐌱𐌰𐌹𐌸 𐍃𐍅𐌹𐌻𐍄𐌰𐌽, 𐌿𐌽𐍄𐌴 𐌲𐌿𐍄𐌰𐌽𐍃 𐍅𐌴𐍃𐌿𐌽 𐌱𐌹 𐌸𐌰𐌽𐍃 𐍆𐍂𐌰𐌲𐌺𐌰𐌽𐍃 𐌾𐌰𐌷 𐌹̈𐌽 𐌹̈𐍄𐌰𐌻𐌹𐌰𐌹 𐌱𐌻𐌰𐌿𐌸𐌹𐌳𐌰𐌽𐍃, 𐌹𐌽𐌿𐌷 𐌸𐌹𐍃 𐍂𐌰𐌶𐌳𐌰 𐌲𐌴𐍉𐌲𐍂𐌰𐍆𐌹𐍃𐌺𐌰𐌱𐌰 𐌲𐌰𐌱𐌿𐌽𐌳𐌰𐌽𐌰 𐍅𐌰𐍃. 𐍃𐍉 𐍂𐌰𐌶𐌳𐌰 𐌱𐌰𐌹𐍂𐌲𐌰𐌳𐌰 𐍅𐌰𐍂𐌸 𐌰𐌽𐌰 𐌷𐌹𐍃𐍀𐌰𐌽𐌾𐌰𐌹 𐌾𐌰𐌷 𐌻𐌿𐍃𐌹𐍄𐌰𐌿𐌾𐌰𐌹 𐌿𐌽𐌳 𐌰𐌷𐍄𐌿𐌳𐍉 𐍄𐌰𐌹𐌷𐌿𐌽𐍄𐌰𐌹𐌷𐌿𐌽𐌳𐍉 𐌾𐌴𐍂𐌰𐌷𐌿𐌽𐌳. 𐌺𐍂𐌴𐌹𐌼𐌲𐌿𐍄𐍂𐌰𐌶𐌳𐌰 𐍂𐍉𐌳𐌹𐌳𐌰 𐍅𐌰𐍃 𐌰𐌽𐌰 𐌺𐍂𐌴𐌹𐌼𐌰.\n\n𐌱𐍉𐌺𐍉𐍃 𐌾𐌰𐌷 𐌼𐌰𐌹𐌼𐌱𐍂𐌰𐌽𐌰 𐌸𐍉𐌴𐌹 𐍃𐌹𐌽𐌳 𐍆𐍂𐌰𐌻𐌿𐍃𐌰𐌽𐍉𐍃:\n\n𐌲𐌿𐍄𐍂𐌰𐌶𐌳𐌰 𐌹𐍃𐍄 𐌲𐌰𐍃𐍅𐌿𐌻𐍄𐌰𐌽𐌰 𐍂𐌰𐌶𐌳𐌰, 𐌰𐌺𐌴𐌹 𐌽𐌿 𐌲𐌰𐌲𐌲𐌹𐌸 𐌰𐍂𐌱𐌰𐌹𐌸𐍃 𐍆𐌰𐌿𐍂 𐌰𐍆𐍄𐍂𐌰𐌰𐌽𐌰𐌵𐌹𐍅𐌴𐌹𐌽 𐌸𐍉𐍃 𐍂𐌰𐌶𐌳𐍉𐍃. 𐍃𐍅𐌴 𐌹𐍃𐍄 𐌰𐌻𐌰𐍆𐌰𐌹𐍂𐍈𐌹𐍃𐌺𐌰 𐌲𐌰𐌼𐌰𐌹𐌽𐌳𐌿𐌸𐍃 𐌲𐌿𐍄𐍂𐌰𐌶𐌳𐍉𐍃 𐌾𐌰𐌷 𐌺𐌿𐌽𐌾𐌰𐌷𐌰𐌹𐌳𐌰𐌿𐍃 𐌲𐌿𐍄𐌹𐍃𐌺𐌰𐌹𐌶𐍉𐍃 - 𐌲𐌿𐍄𐌰𐍅𐌹𐌲𐍃, 𐍃𐍉𐌴𐌹 𐌿𐍃𐍅𐌰𐌿𐍂𐌺𐌴𐌹𐌸 𐌸𐌰𐍄𐌰. 𐍃𐌹𐌽𐌳 𐌾𐌰𐌷 𐌿𐍃𐍅𐌰𐌿𐍂𐌺𐌾𐌰𐌽𐌳𐌰𐌽𐍃 𐌸𐌰𐍄𐌰 𐍃𐌹𐌻𐌱𐌰𐌻𐌴𐌹𐌺𐍉.",
		"url": "https://got.wikipedia.org/wiki/𐌲𐌿𐍄𐍂𐌰𐌶𐌳𐌰"
	},
	"ja": {
		"content": "Unicode（ユニコード）は、符号化文字集合や文字符号化方式などを定めた、文字コードの業界標準規格。文字集合（文字セット）が単一の大規模文字セットであること（「Uni」という名はそれに由来する）などが特徴である。\n\n従来、各国の標準化団体あるいは各コンピュータメーカーによって独自に開発されていた個々の文字コードの間には互換性がなかった[1]。ISO/IEC 2022のように複数の文字コードを共存させる方法も考案されたが、例えば日本語の漢字と中国語の漢字のように、文字が重複する短所がある。一方Unicodeは、微細な差異はあっても本質的に同じ文字であれば一つの番号を当てる方針で各国・各社の文字コードの統合を図った規格である[1]。1980年代に、Starワークステーションの日本語化（J-Star）などを行ったゼロックスが提唱し、マイクロソフト、Apple、IBM、サン・マイクロシステムズ、ヒューレット・パッカード、ジャストシステムなどが参加するユニコードコンソーシアムにより作られた。国際規格のISO/IEC 10646とUnicode規格は同じ文字コード表になるように協調して策定されている[2]。\n\nUnicodeは世界で使われる全ての文字を共通の文字集合にて利用できるようにしようという考えで作られ、Unix、Windows、macOS、Plan 9[注釈 1]などの様々なオペレーティングシステムでサポートされている。Javaや.NETのようなプログラミング環境でも標準的にサポートされている。現代の文字だけでなく古代の文字や歴史的な文字、数学記号、絵文字なども含む[3]。\n\nUnicode以前の文字コードとの相互運用性もある程度考慮されており、歴史上・実用上の識別が求められる場合には互換領域がとられ、元のコード→Unicode→元のコードというような変換（ラウンドトリップ変換）において、元通りに戻るよう配慮されている文字もある。しかし、正規のJIS X 0208の範囲内であればトラブルは少ないが、複数の文字集合が混在していたり、文字集合の亜種ごとにマッピング（対応づけ）が異なる文字（機種依存文字）を含んでいたりする場合[注釈 2]、変換テーブルによるマッピングが不可逆変換となり文字化けを起こすことがある。\n\n文字コードは、Unicode文字符号化モデル[4]によると以下の4段階に分けられる：\n\nその後、バイト列を、gzipなどで圧縮したり、7ビット伝送路に通すためにBase64やQuoted-printableなどで変換したりすることがあるが、これらは文字コードの管轄範囲外である。\n\nUnicodeの文字集合の符号空間は0 - 10FFFF16で111万4,112の符号位置がある[7]。Unicode 12.1（2019年5月7日公表）では13万7,929個 (12%) の文字[注釈 3]が割り当てられ、65個を制御文字に使い、13万7,468符号位置 (12%) を私用文字として確保している。また、2,048文字分をUTF-16のための代用符号位置に使用しており、加えて66の特別な符号位置は使われない。残りの83万6,536符号位置 (75%) は未使用である[8]。\n\n文字を特定する場合にはUnicode符号位置や一意につけられた名前が使われる。例えば、アルファベット小文字の「a」はU+0061 (LATIN SMALL LETTER A)、八分音符「♪」はU+266A (EIGHTH NOTE) である。Unicode符号位置を文章中などに記す場合は \"U+\" の後に十六進法で符号位置を4桁から6桁続けることで表す。また、符号空間のうち代用符号位置を除く符号位置をUnicodeスカラ値という[9]。\n\n収録されている文字は、各国で標準として規定されている文字集合や実際に使用されている文字を持ち寄り、委員会により取捨選択されている。日本の文字については当初よりJIS X 0201、JIS X 0208、JIS X 0212を、Unicode 3.1からはJIS X 0213の内容も収録している。\n\nまた収録において、元の各文字集合内で分離されている文字は尊重するが、異なる文字集合に同一の文字が収録されているとみなされるものは、同じ符号位置に割り当てる方針を取っている。この際に集合が膨大であるという理由で、漢字について、中国、日本、韓国の各規格の漢字を統合しCJK統合漢字としたことは大きな議論となった。\n\n現在では独自創作の絵文字の追加等、当初の目的である「各国・各社の文字コードの統合」から外れた動きも進んでいる。\n\nUnicodeに収録されている文字については、「ブロックの一覧」を参照。\n\nUnicodeでは文字符号化形式としてUTF-8、UTF-16、UTF-32の3種類が定められている。\n\nUTF-8は1符号化文字を1〜4符号単位で表す可変幅文字符号化形式で、1符号単位は8ビットである。\n\nUTF-16は1符号化文字を1〜2符号単位で表す可変幅文字符号化形式で、1符号単位は16ビットである。基本多言語面の文字を符号単位一つで、その他の文字をサロゲートペア（代用対）という仕組みを使い符号単位二つで表現する。\n\nUTF-32は1符号化文字を1符号単位で表す固定幅文字符号化形式で、1符号単位は32ビットである。ただし、Unicodeの符号空間がU+10FFFFまでであるため、実際に使われるのは21ビットまでである。\n\nUnicodeでは文字符号化方式としてUTF-8、UTF-16、UTF-16BE、UTF-16LE、UTF-32、UTF-32BE、UTF-32LEの7種類が定められている。それぞれの符号化形式に対応する符号化方式は表の通り。\n\n文字符号化形式との違いは、文字符号化形式がプログラム内部で文字を扱う場合に符号なし整数として文字を表現する方法なのに対し、文字符号化方式は入出力時にバイト列として表現する方法である。UTF-8は符号単位が8ビットであるため区別する意味はない。\n\n以下はエイプリルフールに公開されたジョークRFCである (RFC 4042)。UTF-9に関しては同名の規格が実際に検討されていた（ただし、内容は大きく異なる）が、ドラフト段階で破棄されているため重複にはならない。\n\n以下はドラフト段階で破棄された規格案。\n\n1980年代の当初の構想では、Unicodeは16ビット固定長で、216 = 6万5,536 個の符号位置に必要な全ての文字を収録する、というもくろみであった。しかし、Unicode 1.0公表後、拡張可能な空き領域2万字分を巡り、各国から文字追加要求が起こった。その内容は中国、日本、台湾、ベトナム、シンガポールの追加漢字約1万5千字、古ハングル約5千字、未登録言語の文字などである。このようにしてUnicodeの、16ビットの枠内に全世界の文字を収録するという計画は早々に破綻し、1996年のUnicode 2.0の時点で既に、文字集合の空間を16ビットから広げることが決まった。この時、それまでの16ビットを前提としてすでに設計されていたシステム（たとえばJavaのchar型や、Windows NT・Windows 95のAPI）をなるべくそのままにしたまま、広げられた空間にある符号位置を表現する方法として、サロゲートペアが定義された。\n\nサロゲートペア（代用対）は16ビットUnicodeの領域1,024文字分を2つ使い（前半 U+D800 〜 U+DBFF、後半 U+DC00 〜 U+DFFF）、各々1個ずつからなるペアで1,024 × 1,024 = 1,048,576文字を表す。これはちょうど16面分であり、第1面〜第16面（U+010000 〜 U+10FFFF）の文字をこれで表すこととした。加えて第0面（基本多言語面）も使用可能なので、Unicodeには合計で 1,048,576 + 65,536 - 2,048 = 111万2,064文字分の空間が確保されたことになる。Unicodeの符号空間が10FFFF16まで（サロゲート領域を除いて111万2,064文字）とされているのはUTF-16が表現可能な限界だからである。\n\nサロゲートはUnicodeの符号位置の U+010000 〜 U+10FFFF の範囲を16ビットユニットのペア（2つ）で表現する集合で、最初の16ビットユニットを前半サロゲートもしくはハイサロゲート、二番目を後半サロゲートもしくはローサロゲートと称する。ハイサロゲートは U+D800 〜 U+DBFF の範囲、ローサロゲートは U+DC00 〜 U+DFFF の範囲である。\n\nサロゲートペアはUTF-16でのみ使われ[11]、UTF-8、UTF-32ではすべての符号位置を符号化できるためこのような特別な処理は必要ない。\n\nサロゲートのエンコーディングは、符号位置を C P {\\displaystyle CP} 、ハイサロゲートを H S G {\\displaystyle HSG} 、ローサロゲートを L S G {\\displaystyle LSG} とすると次の通りに計算する。\n\nデコーディングは、\n\nである。\n\n次の表は、この文字変換と他をまとめたものである。 色は、コードポイントからのビットがUTF-16バイトにどのように分配されるかを示した。 なお、UTF-16エンコーディングプロセスによって追加された追加ビットは黒で示されている。\n\n一つの面は6万5536個の符号位置がある。\n\n日本では2000年にJIS X 0208を拡張する目的でJIS X 0213（いわゆるJIS第3・第4水準）が制定されたが、この際、新たに採用された文字でUnicodeになかったものの一部は、BMPに収録できず、第2面への収録となった（Unicodeが最終的にJIS X 0213への対応を完了したのは2002年である）。このため、JIS X 0213収録文字をUnicodeで完全にサポートするには、追加漢字面をサポートしたOS、フォント、アプリケーションが必要となる。Shift_JISなど、Unicodeにて規定されるもの以外のエンコーディングを利用する場合であっても、JIS X 0213に対応するフォントやアプリケーションが必要である。\n\n常用漢字の2010年改定で追加された字のうち「𠮟」はU+20B9Fで、追加漢字面に含まれる。そのため、改定後の常用漢字完全サポートを謳う場合、Unicodeに対応していて更にこの拡張領域にも対応している必要があると言える。ただ、現状ではこの字は、JIS X 0208に含まれる（＝当然、Unicode策定当初からBMPに収録されている）異体字の「叱」(U+53F1) で代用されることが多い。\n\n1984年、ISOの文字コード規格委員会 (ISO/TC 97/SC2) は文字セットの切り替えを行わずに世界中の文字を単一の文字集合として扱える文字コード規格 (ISO 10646) を作成することを決定し、専門の作業グループ (ISO/TC 97/SC 2/WG 2) を設置し、作業を始めていた。1980年代後半にはこの作業グループにおいてさまざまな提案が検討されている。1990年になって出来あがったISO/TC 97/SC 2/WG 2作成のISO 10646の初版ドラフト（DIS 10646#DIS 10646第1版）では、漢字コードは32ビットで表現され、各国の漢字コードはそのまま入れることになった。しかし中国は漢字を各国でばらばらに符号化するのではなく、あくまで統一して扱うことを求めてこのドラフトには当初から反対しており、今後の漢字コードの方針を決めるため、WG 2は CJK-JRG (Joint Research Group) と呼ばれるグループを別途設置し、そこで引き続き検討することにした。\n\nこのような公的機関の動きとは別に、1987年頃からXeroxのJoe BeckerとLee Collinsは、後にUnicodeと呼ばれるようになる、世界中の文字を統一して扱える文字コードを開発していた。1989年9月には「Unicode Draft 1」が発表された。ここではその基本方針として、2オクテット（16ビット）固定長で全ての文字を扱えることを目指しており、そのために日本・中国・韓国の漢字を統一することで2万弱の漢字コードを入れ、さらに将来の拡張用に、3万程度の漢字の空き領域が別に用意されていた。このドラフトは少しずつ改良を加えられながら1990年4月にUnicode Draft 2、同年12月Unicode Final Draftとなった。さらに1991年1月にはこのUnicode Final Draftに賛同する企業によって、ユニコードコンソーシアムが設立された。\n\n1991年6月、ISO/IEC 10646による4オクテット固定長コードを主体としたドラフト「DIS 10646第1版」は、2オクテット固定長コードであるUnicodeとの一本化を求める各国により否決され、ISO 10646とUnicodeの一本化が図られることになった。また中国およびユニコードコンソーシアムの要請により、CJK-JRGにおいて、ISO 10646とUnicodeの一本化が図られることになった。CJK-JRGは各国の漢字コードに基づき独自の統合規準を定め、ISO 10646 / Unicode用の統合漢字コード表を作成することになった。CJK-JRGの会合は第1回が7月22日から24日にかけて東京で、第2回の会合が9月17日から19日にかけて北京で、第3回が11月25日から29日にかけて香港で開催された。これらの討議の結果、1991年末になって「ISO 10646＝Unicode」用の統合漢字コード表が Unified Repertoire and Ordering (URO) の第1版として完成した。\n\nUnicodeの最初に印刷されたドキュメントであるUnicode 1.0は、統合漢字表の完成に先行して漢字部分を除いたUnicode 1.0, Vol.1が1991年10月に出版され、後に1992年になって漢字部分だけのUnicode 1.0, Vol.2が出版された。\n\n1992年、CJK統合漢字URO第二版が完成し、これを取り込んだ（ただし、UROには若干の間違いが発見されており、それらの修正が行われている。）DIS 10646第2版が、5月30日の国際投票で可決された。\n\n1993年5月1日 「ISO/IEC 10646-1: 1993 Universal Multiple-Octet Coded Character Set (UCS) -- Part 1: Architecture and basic Multilingual Plane」が制定される。同年翌6月にUnicode 1.0は ISO/IEC 10646-1:1993にあわせた変更を行いUnicode 1.1となり、以後UnicodeとISO/IEC 10646とは歩調を合わせて改訂されていくことになる。\n\nUnicodeのバージョンは、メジャーバージョン (the major version)、マイナーバージョン (the minor version)、アップデートバージョン (the update version) の3つの部分から構成され、ピリオドでつなげて表示される[12]。ただし、マイナーバージョン及びアップデートバージョンについては0の場合には省略して表示されることもある。メジャーバージョンはレパートリーの追加のような重要な変更が行われたときに改定される。Unicodeのドキュメントは書籍形態と電子版ドキュメント形態の両方で公表され、どちらもUnicodeについての正式なドキュメントであるとされている。新たなバージョンがリリースされたときは新たなドキュメントが公表されるが、書籍として刊行されるのはメジャーバージョンが改定された場合および重要なマイナーバージョンの改定があった場合のみである。書籍版のバージョン1.0は、2巻に分けて刊行され、統合漢字部分を除いた第1巻は1991年10月に、統合漢字部分の第2巻は1992年6月に刊行された。そのため第1巻のみのものをUnicode 1.0.0、第2巻を含めたものをUnicode 1.0.1と呼ぶことがある。\n\nUnicodeのそれぞれのバージョン番号とその制定年月日、収録文字数他の特徴は以下の通りである。\n\n[13]\n\n[14]\n\n[15]\n\n[16]\n\n[17]\n\n[18]\n\n[19]\n\n[20]\n\n[21]\n\n[22]\n\n[23]\n\n[24]\n\n[25]\n\n[26]\n\n[27]\n\n[28]\n\n[29]\n\n[30]\n\n[31]\n\n[32]\n\n[33]\n\n[34]\n\n[35]\n\n[36]\n\n[37]\n\n[38]\n\n[39]\n\n[40]\n\n[41]\n\n[42]\n\n[43]\n\nUnicodeのバージョンには、上記のような「Unicodeの規格全体に付けられたバージョン」の他に「Unicodeを構成する個々の要素の規格に付けられたバージョン」が存在する。これに該当するものとしては、Unicodeを構成する各面ごとに付けられたバージョンや、Unicodeに収録されないこととされたスクリプトのリスト (NOR = Not The Roadmap) に付けられたバージョン、規格の一部を構成するUnicode Technical Note（Unicode技術ノート）、Unicode Technical Report（Unicode技術報告）、Unicode Technical Standard（Unicode技術標準）のバージョンなどが存在する。\n\n3.5[79]\n\nUnicodeは同一のコードでもバージョンが変わったとき完全に異なった文字を定義し直したことがある。\n\nそのうち最大のものがUnicode 2.0での「ハングルの大移動」である。これはUnicode 1.1までで定義されていたハングルの領域を破棄し、新しいハングルの領域を別の位置に設定し、破棄された領域には別の文字の領域を割り当てることとなった。その後、Unicode 3.0では、従来ハングルが割り当てられていた領域にCJK統合漢字拡張A、ついでUnicode 4.0で六十四卦が割り当てられた。このように、Unicode 1.1以前でハングルを記述した文書とUnicode 2.0以降でCJK統合漢字拡張Aを記述した文書には互換性がない[注釈 7]。JCS委員長の芝野耕司はUnicodeに日本語の漢字を収録させる議論の中で、ハングル大移動について「韓国のとった滅茶苦茶な行動」と述べている[297]。\n\nShift JIS では JIS X 0201 における（日本や中国の通貨の）円記号 \"¥\" が 0x5C に置かれている。これを Unicode のマッピングに合わせると YEN SIGN (U+00A5) にマップされる。しかし、0x5C は ASCII ではバックスラッシュ \"\\\" に相当し、C言語などでエスケープ文字として使われる事から、この文字のコードを変更すると問題が起きる。極端な例として、0x5C が円記号とエスケープ文字の両方の目的で使われているケース（たとえばC言語のprintf関数で printf(\"¥¥%d¥n\", price); など）も考えられる。\n\nそのため、Unicode を利用するアプリケーションでは、U+007F 以下のコードに関しては移動させないという暗黙のルールができている。\n\nそうなると、Unicode 環境では円記号がバックスラッシュの表示に変わってしまうように思われるが、これは日本語用のフォントデータの 0x5C の位置には円記号の字形を当ててしまうことで対処している。これによって、日本語環境での表示上は 0x5C の位置で円記号を用いることができる。\n\nこの問題は日本語環境に限ったことではない。もともと ISO 646 上では、0x5C を含む数種の文字は自由領域（バリアント）として各国での定義を認めていた。そのため、日本語以外でも ASCII でバックスラッシュに相当するコードに異なる記号を当てているケースが多い。例えば、韓国では通貨のウォン記号 (WON SIGN, U+20A9, \"₩\")、デンマークやノルウェーではストローク付きO (LATIN CAPITAL LETTER O WITH STROKE, U+00D8, \"Ø\") などである。（後者は後の時代には、0x5C はバックスラッシュのままとし、ISO 8859 シリーズを用いることが一般化した。）\n\nJIS X 0221 規定の JIS X 0208 と JIS X 0221 の対応表では、波ダッシュは WAVE DASH (U+301C, \"〜\") に対応させている。\n\nしかし、マイクロソフトは Windows の Shift_JIS と Unicode の変換テーブルを作成する際に、JIS X 0208 において 1 区 33 点に割り当てられている波ダッシュ \"〜\" を、Unicode における全角チルダ (FULLWIDTH TILDE, U+FF5E, \"～\") に割り当てたため不整合が生じた。\n\nこの結果、macOS 等の JIS X 0221 準拠の Shift_JIS ⇔ Unicode 変換テーブルをもつ処理系と Windows との間で Unicode データをやり取りする場合、文字化けを起こすことになる。そこで Windows 以外の OS 上で動くアプリケーションの中には、CP932 という名前でマイクロソフト仕様の Shift_JIS コード体系を別途用意して対応しているケースが多い。この原因とされている Unicode 仕様書の例示字形の問題に関しては、波ダッシュ#Unicodeに関連する問題を参照すること。\n\n上記に加え、マイクロソフト仕様は変換時にも問題が起こる文字を以下に示す。\n\nこのうちセント・ポンド・否定については、IBMのメインフレームではShift_JISを拡張してこれらの半角版をコードポイント 0xFD-0xFF に割り当て、別途JIS X 0208からマップされた位置に全角版を収録していたため、WindowsをIBMメインフレームの端末として用いるケースを想定したといわれている[要出典]。\n\nなお、Windows Vista や Microsoft Office 2007 に付属する IME パッドの文字一覧における JIS X 0213 の面区点の表示は、上記の文字についても JIS で規定されているものと同じマッピングを使用している[要出典]。\n\n0000-0FFF 1000-1FFF 2000-2FFF 3000-3FFF 4000-4FFF 5000-5FFF 6000-6FFF 7000-7FFF 8000-8FFF 9000-9FFF A000-AFFF B000-BFFF C000-CFFF D000-DFFF E000-EFFF F000-FFFF\n\n10000-10FFF 11000-11FFF 12000-12FFF 13000-13FFF 14000-14FFF 15000-15FFF 16000-16FFF 17000-17FFF 18000-18FFF 19000-19FFF 1A000-1AFFF 1B000-1BFFF 1C000-1CFFF 1D000-1DFFF 1E000-1EFFF 1F000-1FFFF\n\n20000-20FFF 21000-21FFF 22000-22FFF 23000-23FFF 24000-24FFF 25000-25FFF 26000-26FFF 27000-27FFF 28000-28FFF 29000-29FFF 2A000-2AFFF 2B000-2BFFF 2C000-2CFFF 2D000-2DFFF 2E000-2EFFF 2F000-2FFFF\n\n30000-30FFF 31000-31FFF 32000-32FFF 33000-33FFF 34000-34FFF 35000-35FFF 36000-36FFF 37000-37FFF 38000-38FFF 39000-39FFF 3A000-3AFFF 3B000-3BFFF 3C000-3CFFF 3D000-3DFFF 3E000-3EFFF 3F000-3FFFF\n\nE0000-E0FFF",
		"url": "https://ja.wikipedia.org/wiki/Unicode"
	},
	"lorem": {
		"content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n\n[1] Non eram nescius, Brute, cum, quae summis ingeniis exquisitaque doctrina philosophi Graeco sermone tractavissent, ea Latinis litteris mandaremus, fore ut hic noster labor in varias reprehensiones incurreret. nam quibusdam, et iis quidem non admodum indoctis, totum hoc displicet philosophari. quidam autem non tam id reprehendunt, si remissius agatur, sed tantum studium tamque multam operam ponendam in eo non arbitrantur. erunt etiam, et ii quidem eruditi Graecis litteris, contemnentes Latinas, qui se dicant in Graecis legendis operam malle consumere. postremo aliquos futuros suspicor, qui me ad alias litteras vocent, genus hoc scribendi, etsi sit elegans, personae tamen et dignitatis esse negent.\n\n[2] Contra quos omnis dicendum breviter existimo. Quamquam philosophiae quidem vituperatoribus satis responsum est eo libro, quo a nobis philosophia defensa et collaudata est, cum esset accusata et vituperata ab Hortensio. qui liber cum et tibi probatus videretur et iis, quos ego posse iudicare arbitrarer, plura suscepi veritus ne movere hominum studia viderer, retinere non posse. Qui autem, si maxime hoc placeat, moderatius tamen id volunt fieri, difficilem quandam temperantiam postulant in eo, quod semel admissum coerceri reprimique non potest, ut propemodum iustioribus utamur illis, qui omnino avocent a philosophia, quam his, qui rebus infinitis modum constituant in reque eo meliore, quo maior sit, mediocritatem desiderent.\n\n[3] Sive enim ad sapientiam perveniri potest, non paranda nobis solum ea, sed fruenda etiam [sapientia] est; sive hoc difficile est, tamen nec modus est ullus investigandi veri, nisi inveneris, et quaerendi defatigatio turpis est, cum id, quod quaeritur, sit pulcherrimum. etenim si delectamur, cum scribimus, quis est tam invidus, qui ab eo nos abducat? sin laboramus, quis est, qui alienae modum statuat industriae? nam ut Terentianus Chremes non inhumanus, qui novum vicinum non vult 'fodere aut arare aut aliquid ferre denique' -- non enim illum ab industria, sed ab inliberali labore deterret --, sic isti curiosi, quos offendit noster minime nobis iniucundus labor.\n\n[4] Iis igitur est difficilius satis facere, qui se Latina scripta dicunt contemnere. in quibus hoc primum est in quo admirer, cur in gravissimis rebus non delectet eos sermo patrius, cum idem fabellas Latinas ad verbum e Graecis expressas non inviti legant. quis enim tam inimicus paene nomini Romano est, qui Ennii Medeam aut Antiopam Pacuvii spernat aut reiciat, quod se isdem Euripidis fabulis delectari dicat, Latinas litteras oderit?\n\nSynephebos ego, inquit, potius Caecilii aut Andriam Terentii quam utramque Menandri legam?\n\n[5] A quibus tantum dissentio, ut, cum Sophocles vel optime scripserit Electram, tamen male conversam Atilii mihi legendam putem, de quo Lucilius: 'ferreum scriptorem', verum, opinor, scriptorem tamen, ut legendus sit. rudem enim esse omnino in nostris poetis aut inertissimae segnitiae est aut fastidii delicatissimi. mihi quidem nulli satis eruditi videntur, quibus nostra ignota sunt. an 'Utinam ne in nemore . . .' nihilo minus legimus quam hoc idem Graecum, quae autem de bene beateque vivendo a Platone disputata sunt, haec explicari non placebit Latine?\n\n[6] Quid? si nos non interpretum fungimur munere, sed tuemur ea, quae dicta sunt ab iis quos probamus, eisque nostrum iudicium et nostrum scribendi ordinem adiungimus, quid habent, cur Graeca anteponant iis, quae et splendide dicta sint neque sint conversa de Graecis? nam si dicent ab illis has res esse tractatas, ne ipsos quidem Graecos est cur tam multos legant, quam legendi sunt. quid enim est a Chrysippo praetermissum in Stoicis? legimus tamen Diogenem, Antipatrum, Mnesarchum, Panaetium, multos alios in primisque familiarem nostrum Posidonium. quid? Theophrastus mediocriterne delectat, cum tractat locos ab Aristotele ante tractatos? quid? Epicurei num desistunt de isdem, de quibus et ab Epicuro scriptum est et ab antiquis, ad arbitrium suum scribere? quodsi Graeci leguntur a Graecis isdem de rebus alia ratione compositis, quid est, cur nostri a nostris non legantur?\n\n[7] Quamquam, si plane sic verterem Platonem aut Aristotelem, ut verterunt nostri poetae fabulas, male, credo, mererer de meis civibus, si ad eorum cognitionem divina illa ingenia transferrem. sed id neque feci adhuc nec mihi tamen, ne faciam, interdictum puto. locos quidem quosdam, si videbitur, transferam, et maxime ab iis, quos modo nominavi, cum inciderit, ut id apte fieri possit, ut ab Homero Ennius, Afranius a Menandro solet. Nec vero, ut noster Lucilius, recusabo, quo minus omnes mea legant. utinam esset ille Persius, Scipio vero et Rutilius multo etiam magis, quorum ille iudicium reformidans Tarentinis ait se et Consentinis et Siculis scribere. facete is quidem, sicut alia; sed neque tam docti tum erant, ad quorum iudicium elaboraret, et sunt illius scripta leviora, ut urbanitas summa appareat, doctrina mediocris.\n\n[8] Ego autem quem timeam lectorem, cum ad te ne Graecis quidem cedentem in philosophia audeam scribere? quamquam a te ipso id quidem facio provocatus gratissimo mihi libro, quem ad me de virtute misisti. Sed ex eo credo quibusdam usu venire; ut abhorreant a Latinis, quod inciderint in inculta quaedam et horrida, de malis Graecis Latine scripta deterius. quibus ego assentior, dum modo de isdem rebus ne Graecos quidem legendos putent. res vero bonas verbis electis graviter ornateque dictas quis non legat? nisi qui se plane Graecum dici velit, ut a Scaevola est praetore salutatus Athenis Albucius.\n\n[9] Quem quidem locum comit multa venustate et omni sale idem Lucilius, apud quem praeclare Scaevola:\n\n[10] Sed iure Mucius. ego autem mirari [satis] non queo unde hoc sit tam insolens domesticarum rerum fastidium. non est omnino hic docendi locus; sed ita sentio et saepe disserui, Latinam linguam non modo non inopem, ut vulgo putarent, sed locupletiorem etiam esse quam Graecam. quando enim nobis, vel dicam aut oratoribus bonis aut poetis, postea quidem quam fuit quem imitarentur, ullus orationis vel copiosae vel elegantis ornatus defuit? Ego vero, quoniam forensibus operis, laboribus, periculis non deseruisse mihi videor praesidium, in quo a populo Romano locatus sum, debeo profecto, quantumcumque possum, in eo quoque elaborare, ut sint opera, studio, labore meo doctiores cives mei, nec cum istis tantopere pugnare, qui Graeca legere malint, modo legant illa ipsa, ne simulent, et iis servire, qui vel utrisque litteris uti velint vel, si suas habent, illas non magnopere desiderent.\n\n[11] Qui autem alia malunt scribi a nobis, aequi esse debent, quod et scripta multa sunt, sic ut plura nemini e nostris, et scribentur fortasse plura, si vita suppetet; et tamen, qui diligenter haec, quae de philosophia litteris mandamus, legere assueverit, iudicabit nulla ad legendum his esse potiora. quid est enim in vita tantopere quaerendum quam cum omnia in philosophia, tum id, quod his libris quaeritur, qui sit finis, quid extremum, quid ultimum, quo sint omnia bene vivendi recteque faciendi consilia referenda, quid sequatur natura ut summum ex rebus expetendis, quid fugiat ut extremum malorum? qua de re cum sit inter doctissimos summa dissensio, quis alienum putet eius esse dignitatis, quam mihi quisque tribuat, quid in omni munere vitae optimum et verissimum sit, exquirere?\n\n[12] An, partus ancillae sitne in fructu habendus, disseretur inter principes civitatis, P. Scaevolam Maniumque Manilium, ab iisque M. Brutus dissentiet -- quod et acutum genus est et ad usus civium non inutile, nosque ea scripta reliquaque eiusdem generis et legimus libenter et legemus -- haec, quae vitam omnem continent, neglegentur? nam, ut sint illa vendibiliora, haec uberiora certe sunt. quamquam id quidem licebit iis existimare, qui legerint. nos autem hanc omnem quaestionem de finibus bonorum et malorum fere a nobis explicatam esse his litteris arbitramur, in quibus, quantum potuimus, non modo quid nobis probaretur, sed etiam quid a singulis philosophiae disciplinis diceretur, persecuti sumus.\n\n[13] Ut autem a facillimis ordiamur, prima veniat in medium Epicuri ratio, quae plerisque notissima est. quam a nobis sic intelleges eitam, ut ab ipsis, qui eam disciplinam probant, non soleat accuratius explicari; verum enim invenire volumus, non tamquam adversarium aliquem convincere. accurate autem quondam a L. Torquato, homine omni doctrina erudito, defensa est Epicuri sententia de voluptate, a meque ei responsum, cum C. Triarius, in primis gravis et doctus adolescens, ei disputationi interesset.\n\n[14] Nam cum ad me in Cumanum salutandi causa uterque venisset, pauca primo inter nos de litteris, quarum summum erat in utroque studium, deinde Torquatus: Quoniam nacti te, inquit, sumus aliquando otiosum, certe audiam, quid sit, quod Epicurum nostrum non tu quidem oderis, ut fere faciunt, qui ab eo dissentiunt, sed certe non probes, eum quem ego arbitror unum vidisse verum maximisque erroribus animos hominum liberavisse et omnia tradidisse, quae pertinerent ad bene beateque vivendum. sed existimo te, sicut nostrum Triarium, minus ab eo delectari, quod ista Platonis, Aristoteli, Theophrasti orationis ornamenta neglexerit. nam illud quidem adduci vix possum, ut ea, quae senserit ille, tibi non vera videantur.\n\n[15] Vide, quantum, inquam, fallare, Torquate. oratio me istius philosophi non offendit; nam et complectitur verbis, quod vult, et dicit plane, quod intellegam; et tamen ego a philosopho, si afferat eloquentiam, non asperner, si non habeat, non admodum flagitem. re mihi non aeque satisfacit, et quidem locis pluribus. sed quot homines, tot sententiae; falli igitur possumus.\n\nQuam ob rem tandem, inquit, non satisfacit? te enim iudicem aequum puto, modo quae dicat ille bene noris.\n\n[16] Nisi mihi Phaedrum, inquam, tu mentitum aut Zenonem putas, quorum utrumque audivi, cum mihi nihil sane praeter sedulitatem probarent, omnes mihi Epicuri sententiae satis notae sunt. atque eos, quos nominavi, cum Attico nostro frequenter audivi, cum miraretur ille quidem utrumque, Phaedrum autem etiam amaret, cotidieque inter nos ea, quae audiebamus, conferebamus, neque erat umquam controversia, quid ego intellegerem, sed quid probarem.\n\n[17] Quid igitur est? inquit; audire enim cupio, quid non probes. Principio, inquam, in physicis, quibus maxime gloriatur, primum totus est alienus. Democritea dicit perpauca mutans, sed ita, ut ea, quae corrigere vult, mihi quidem depravare videatur. ille atomos quas appellat, id est corpora individua propter soliditatem, censet in infinito inani, in quo nihil nec summum nec infimum nec medium nec ultimum nec extremum sit, ita ferri, ut concursionibus inter se cohaerescant, ex quo efficiantur ea, quae sint quaeque cernantur, omnia, eumque motum atomorum nullo a principio, sed ex aeterno tempore intellegi convenire.\n\n[18] Epicurus autem, in quibus sequitur Democritum, non fere labitur. quamquam utriusque cum multa non probo, tum illud in primis, quod, cum in rerum natura duo quaerenda sint, unum, quae materia sit, ex qua quaeque res efficiatur, alterum, quae vis sit, quae quidque efficiat, de materia disseruerunt, vim et causam efficiendi reliquerunt. sed hoc commune vitium, illae Epicuri propriae ruinae: censet enim eadem illa individua et solida corpora ferri deorsum suo pondere ad lineam, hunc naturalem esse omnium corporum motum.\n\n[19] Deinde ibidem homo acutus, cum illud ocurreret, si omnia deorsus e regione ferrentur et, ut dixi, ad lineam, numquam fore ut atomus altera alteram posset attingere itaque ** attulit rem commenticiam: declinare dixit atomum perpaulum, quo nihil posset fieri minus; ita effici complexiones et copulationes et adhaesiones atomorum inter se, ex quo efficeretur mundus omnesque partes mundi, quaeque in eo essent. Quae cum tota res (est) ficta pueriliter, tum ne efficit [quidem], quod vult. nam et ipsa declinatio ad libidinem fingitur -- ait enim declinare atomum sine causa; quo nihil turpius physico, quam fieri quicquam sine causa dicere, -- et illum motum naturalem omnium ponderum, ut ipse constituit, e regione inferiorem locum petentium sine causa eripuit atomis nec tamen id, cuius causa haec finxerat, assecutus est.\n\n[20] Nam si omnes atomi declinabunt, nullae umquam cohaerescent, sive aliae declinabunt, aliae suo nutu recte ferentur, primum erit hoc quasi, provincias atomis dare, quae recte, quae oblique ferantur, deinde eadem illa atomorum, in quo etiam Democritus haeret, turbulenta concursio hunc mundi ornatum efficere non poterit. ne illud quidem physici, credere aliquid esse minimum, quod profecto numquam putavisset, si a Polyaeno, familiari suo, geometrica discere maluisset quam illum etiam ipsum dedocere. Sol Democrito magnus videtur, quippe homini erudito in geometriaque perfecto, huic pedalis fortasse; tantum enim esse censet, quantus videtur, vel paulo aut maiorem aut minorem.\n\n[21] Ita, quae mutat, ea corrumpit, quae sequitur sunt tota Democriti, atomi, inane, imagines, quae eidola nominant, quorum incursione non solum videamus, sed etiam cogitemus; infinitio ipsa, quam apeirian vocant, tota ab illo est, tum innumerabiles mundi, qui et oriantur et intereant cotidie. Quae etsi mihi nullo modo probantur, tamen Democritum laudatum a ceteris ab hoc, qui eum unum secutus esset, nollem vituperatum.\n\n[22] Iam in altera philosophiae parte. quae est quaerendi ac disserendi, quae logikh dicitur, iste vester plane, ut mihi quidem videtur, inermis ac nudus est. tollit definitiones, nihil de dividendo ac partiendo docet, non quo modo efficiatur concludaturque ratio tradit, non qua via captiosa solvantur ambigua distinguantur ostendit; iudicia rerum in sensibus ponit, quibus si semel aliquid falsi pro vero probatum sit, sublatum esse omne iudicium veri et falsi putat.\n\n[23] Confirmat autem illud vel maxime, quod ipsa natura, ut ait ille, sciscat et probet, id est voluptatem et dolorem. ad haec et quae sequamur et quae fugiamus refert omnia. quod quamquam Aristippi est a Cyrenaicisque melius liberiusque defenditur, tamen eius modi esse iudico, ut nihil homine videatur indignius. ad maiora enim quaedam nos natura genuit et conformavit, ut mihi quidem videtur. ac fieri potest, ut errem, sed ita prorsus existimo, neque eum Torquatum, qui hoc primus cognomen invenerit, aut torquem illum hosti detraxisse, ut aliquam ex eo perciperet corpore voluptatem, aut cum Latinis tertio consulatu conflixisse apud Veserim propter voluptatem; quod vero securi percussit filium, privavisse se etiam videtur multis voluptatibus, cum ipsi naturae patrioque amori praetulerit ius maiestatis atque imperii.\n\n[24] quid? T. Torquatus, is qui consul cum Cn. Octavio fuit, cum illam severitatem in eo filio adhibuit, quem in adoptionem D. Silano emancipaverat, ut eum Macedonum legatis accusantibus, quod pecunias praetorem in provincia cepisse arguerent, causam apud se dicere iuberet reque ex utraque parte audita pronuntiaret eum non talem videri fuisse in imperio, quales eius maiores fuissent, et in conspectum suum venire vetuit, numquid tibi videtur de voluptatibus suis cogitavisse?\n\nSed ut omittam pericula, labores, dolorem etiam, quem optimus quisque pro patria et pro suis suscipit, ut non modo nullam captet, sed etiam praetereat omnes voluptates, dolores denique quosvis suscipere malit quam deserere ullam officii partem, ad ea, quae hoc non minus declarant, sed videntur leviora, veniamus.\n\n[25] Quid tibi, Torquate, quid huic Triario litterae, quid historiae cognitioque rerum, quid poetarum evolutio, quid tanta tot versuum memoria voluptatis affert? nec mihi illud dixeris: 'Haec enim ipsa mihi sunt voluptati, et erant illa Torquatis.' Numquam hoc ita defendit Epicurus neque Metrodorus aut quisquam eorum, qui aut saperet aliquid aut ista didicisset. et quod quaeritur saepe, cur tam multi sint Epicurei, sunt aliae quoque causae, sed multitudinem haec maxime allicit, quod ita putant dici ab illo, recta et honesta quae sint, ea facere ipsa per se laetitiam, id est voluptatem. homines optimi non intellegunt totam rationem everti, si ita res se habeat. nam si concederetur, etiamsi ad corpus nihil referatur, ista sua sponte et per se esse iucunda, per se esset et virtus et cognitio rerum, quod minime ille vult expetenda.\n\n[26] Haec igitur Epicuri non probo, inquam. De cetero vellem equidem aut ipse doctrinis fuisset instructior -- est enim, quod tibi ita videri necesse est, non satis politus iis artibus, quas qui tenent, eruditi appellantur -- aut ne deterruisset alios a studiis. quamquam te quidem video minime esse deterritum.\n\nQuae cum dixissem, magis ut illum provocarem quam ut ipse loquerer, tum Triarius leniter arridens: Tu quidem, inquit, totum Epicurum paene e philosophorum choro sustulisti. Quid ei reliquisti, nisi te, quoquo modo loqueretur, intellegere, quid diceret? Aliena dixit in physicis nec ea ipsa, quae tibi probarentur; si qua in iis corrigere voluit, deteriora fecit. disserendi artem nullam habuit. voluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; nam ante Aristippus, et ille melius. addidisti ad extremum etiam indoctum fuisse.\n\n[27] Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. quid enim me prohiberet Epicureum esse, si probarem, quae ille diceret? cum praesertim illa perdiscere ludus esset. Quam ob rem dissentientium inter se reprehensiones non sunt vituperandae, maledicta, contumeliae, tum iracundiae, contentiones concertationesque in disputando pertinaces indignae philosophia mihi videri solent.\n\n[28] Tum Torquatus: Prorsus, inquit, assentior; neque enim disputari sine reprehensione nec cum iracundia aut pertinacia recte disputari potest. sed ad haec, nisi molestum est, habeo quae velim. An me, inquam, nisi te audire vellem, censes haec dicturum fuisse? Utrum igitur percurri omnem Epicuri disciplinam placet an de una voluptate quaeri, de qua omne certamen est? Tuo vero id quidem, inquam, arbitratu. Sic faciam igitur, inquit: unam rem explicabo, eamque maximam, de physicis alias, et quidem tibi et declinationem istam atomorum et magnitudinem solis probabo et Democriti errata ab Epicuro reprehensa et correcta permulta. nunc dicam de voluptate, nihil scilicet novi, ea tamen, quae te ipsum probaturum esse confidam.\n\n[29] Certe, inquam, pertinax non ero tibique, si mihi probabis ea, quae dices, libenter assentiar. Probabo, inquit, modo ista sis aequitate, quam ostendis. sed uti oratione perpetua malo quam interrogare aut interrogari. Ut placet, inquam. Tum dicere exorsus est. Primum igitur, inquit, sic agam, ut ipsi auctori huius disciplinae placet: constituam, quid et quale sit id, de quo quaerimus, non quo ignorare vos arbitrer, sed ut ratione et via procedat oratio. quaerimus igitur, quid sit extremum et ultimum bonorum, quod omnium philosophorum sententia tale debet esse, ut ad id omnia referri oporteat, ipsum autem nusquam. hoc Epicurus in voluptate ponit, quod summum bonum esse vult, summumque malum dolorem, idque instituit docere sic:\n\n[30] Omne animal, simul atque natum sit, voluptatem appetere eaque gaudere ut summo bono, dolorem aspernari ut summum malum et, quantum possit, a se repellere, idque facere nondum depravatum ipsa natura incorrupte atque integre iudicante. itaque negat opus esse ratione neque disputatione, quam ob rem voluptas expetenda, fugiendus dolor sit. sentiri haec putat, ut calere ignem, nivem esse albam, dulce mel. quorum nihil oportere exquisitis rationibus confirmare, tantum satis esse admonere. interesse enim inter argumentum conclusionemque rationis et inter mediocrem animadversionem atque admonitionem. altera occulta quaedam et quasi involuta aperiri, altera prompta et aperta iudicari. etenim quoniam detractis de homine sensibus reliqui nihil est, necesse est, quid aut ad naturam aut contra sit, a natura ipsa iudicari. ea quid percipit aut quid iudicat, quo aut petat aut fugiat aliquid, praeter voluptatem et dolorem?\n\n[31] Sunt autem quidam e nostris, qui haec subtilius velint tradere et negent satis esse, quid bonum sit aut quid malum, sensu iudicari, sed animo etiam ac ratione intellegi posse et voluptatem ipsam per se esse expetendam et dolorem ipsum per se esse fugiendum. itaque aiunt hanc quasi naturalem atque insitam in animis nostris inesse notionem, ut alterum esse appetendum, alterum aspernandum sentiamus. Alii autem, quibus ego assentior, cum a philosophis compluribus permulta dicantur, cur nec voluptas in bonis sit numeranda nec in malis dolor, non existimant oportere nimium nos causae confidere, sed et argumentandum et accurate disserendum et rationibus conquisitis de voluptate et dolore disputandum putant.\n\n[32] Sed ut perspiciatis, unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam eaque ipsa, quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt, explicabo. nemo enim ipsam voluptatem, quia voluptas sit, aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos, qui ratione voluptatem sequi nesciunt, neque porro quisquam est, qui dolorem ipsum, quia dolor sit, amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt, ut labore et dolore magnam aliquam quaerat voluptatem. ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? quis autem vel eum iure reprehenderit, qui in ea voluptate velit esse, quam nihil molestiae consequatur, vel illum, qui dolorem eum fugiat, quo voluptas nulla pariatur?\n\n[33] At vero eos et accusamus et iusto odio dignissimos ducimus, qui blanditiis praesentium voluptatum deleniti atque corrupti, quos dolores et quas molestias excepturi sint, obcaecati cupiditate non provident, similique sunt in culpa, qui officia deserunt mollitia animi, id est laborum et dolorum fuga. et harum quidem rerum facilis est et expedita distinctio. nam libero tempore, cum soluta nobis est eligendi optio, cumque nihil impedit, quo minus id, quod maxime placeat, facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet, ut et voluptates repudiandae sint et molestiae non recusandae. itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.\n\n[34] Hanc ego cum teneam sententiam, quid est cur verear, ne ad eam non possim accommodare Torquatos nostros? quos tu paulo ante cum memoriter, tum etiam erga nos amice et benivole collegisti, nec me tamen laudandis maioribus meis corrupisti nec segniorem ad respondendum reddidisti. quorum facta quem ad modum, quaeso, interpretaris? sicine eos censes aut in armatum hostem impetum fecisse aut in liberos atque in sanguinem suum tam crudelis fuisse, nihil ut de utilitatibus, nihil ut de commodis suis cogitarent? at id ne ferae quidem faciunt, ut ita ruant itaque turbent, ut earum motus et impetus quo pertineant non intellegamus, tu tam egregios viros censes tantas res gessisse sine causa?\n\n[35] Quae fuerit causa, mox videro; interea hoc tenebo, si ob aliquam causam ista, quae sine dubio praeclara sunt, fecerint, virtutem iis per se ipsam causam non fuisse. -- Torquem detraxit hosti. -- Et quidem se texit, ne interiret. -- At magnum periculum adiit. -- In oculis quidem exercitus. -- Quid ex eo est consecutus? -- Laudem et caritatem, quae sunt vitae sine metu degendae praesidia firmissima. -- Filium morte multavit. -- Si sine causa, nollem me ab eo ortum, tam inportuno tamque crudeli; sin, ut dolore suo sanciret militaris imperii disciplinam exercitumque in gravissimo bello animadversionis metu contineret, saluti prospexit civium, qua intellegebat contineri suam. atque haec ratio late patet.\n\n[36] In quo enim maxime consuevit iactare vestra se oratio, tua praesertim, qui studiose antiqua persequeris, claris et fortibus viris commemorandis eorumque factis non emolumento aliquo, sed ipsius honestatis decore laudandis, id totum evertitur eo delectu rerum, quem modo dixi, constituto, ut aut voluptates omittantur maiorum voluptatum adipiscendarum causa aut dolores suscipiantur maiorum dolorum effugiendorum gratia.\n\n[37] Sed de clarorum hominum factis illustribus et gloriosis satis hoc loco dictum sit. erit enim iam de omnium virtutum cursu ad voluptatem proprius disserendi locus. nunc autem explicabo, voluptas ipsa quae qualisque sit, ut tollatur error omnis imperitorum intellegaturque ea, quae voluptaria, delicata, mollis habeatur disciplina, quam gravis, quam continens, quam severa sit. Non enim hanc solam sequimur, quae suavitate aliqua naturam ipsam movet et cum iucunditate quadam percipitur sensibus, sed maximam voluptatem illam habemus, quae percipitur omni dolore detracto, nam quoniam, cum privamur dolore, ipsa liberatione et vacuitate omnis molestiae gaudemus, omne autem id, quo gaudemus, voluptas est, ut omne, quo offendimur, dolor, doloris omnis privatio recte nominata est voluptas. ut enim, cum cibo et potione fames sitisque depulsa est, ipsa detractio molestiae consecutionem affert voluptatis, sic in omni re doloris amotio successionem efficit voluptatis.\n\n[38] Itaque non placuit Epicuro medium esse quiddam inter dolorem et voluptatem; illud enim ipsum, quod quibusdam medium videretur, cum omni dolore careret, non modo voluptatem esse, verum etiam summam voluptatem. quisquis enim sentit, quem ad modum sit affectus, eum necesse est aut in voluptate esse aut in dolore. omnis autem privatione doloris putat Epicurus terminari summam voluptatem, ut postea variari voluptas distinguique possit, augeri amplificarique non possit.\n\n[39] At etiam Athenis, ut e patre audiebam facete et urbane Stoicos irridente, statua est in Ceramico Chrysippi sedentis porrecta manu, quae manus significet illum in hae esse rogatiuncula delectatum: 'Numquidnam manus tua sic affecta, quem ad modum affecta nunc est, desiderat?' -- Nihil sane. -- 'At, si voluptas esset bonum, desideraret.' -- Ita credo. -- 'Non est igitur voluptas bonum.' Hoc ne statuam quidem dicturam pater aiebat, si loqui posset. conclusum est enim contra Cyrenaicos satis acute, nihil ad Epicurum. nam si ea sola voluptas esset, quae quasi titillaret sensus, ut ita dicam, et ad eos cum suavitate afflueret et illaberetur, nec manus esse contenta posset nec ulla pars vacuitate doloris sine iucundo motu voluptatis. sin autem summa voluptas est, ut Epicuro placet, nihil dolere, primum tibi recte, Chrysippe, concessum est nihil desiderare manum, cum ita esset affecta, secundum non recte, si voluptas esset bonum, fuisse desideraturam. idcirco enim non desideraret, quia, quod dolore caret, id in voluptate est.\n\n[40] Extremum autem esse bonorum voluptatem ex hoc facillime perspici potest: Constituamus aliquem magnis, multis, perpetuis fruentem et animo et corpore voluptatibus nullo dolore nec impediente nec inpendente, quem tandem hoc statu praestabiliorem aut magis expetendum possimus dicere? inesse enim necesse est in eo, qui ita sit affectus, et firmitatem animi nec mortem nec dolorem timentis, quod mors sensu careat, dolor in longinquitate levis, in gravitate brevis soleat esse, ut eius magnitudinem celeritas, diuturnitatem allevatio consoletur.\n\n[41] Ad ea cum accedit, ut neque divinum numen horreat nec praeteritas voluptates effluere patiatur earumque assidua recordatione laetetur, quid est, quod huc possit, quod melius sit, accedere? Statue contra aliquem confectum tantis animi corporisque doloribus, quanti in hominem maximi cadere possunt, nulla spe proposita fore levius aliquando, nulla praeterea neque praesenti nec expectata voluptate, quid eo miserius dici aut fingi potest? quodsi vita doloribus referta maxime fugienda est, summum profecto malum est vivere cum dolore, cui sententiae consentaneum est ultimum esse bonorum eum voluptate vivere. nec enim habet nostra mens quicquam, ubi consistat tamquam in extremo, omnesque et metus et aegritudines ad dolorem referuntur, nec praeterea est res ulla, quae sua natura aut sollicitare possit aut angere.\n\n[42] Praeterea et appetendi et refugiendi et omnino rerum gerendarum initia proficiscuntur aut a voluptate aut a dolore. quod cum ita sit, perspicuum est omnis rectas res atque laudabilis eo referri, ut cum voluptate vivatur. quoniam autem id est vel summum bonorum vel ultimum vel extremum -- quod Graeci telos nominant --, quod ipsum nullam ad aliam rem, ad id autem res referuntur omnes, fatendum est summum esse bonum iucunde vivere.\n\nId qui in una virtute ponunt et splendore nominis capti quid natura postulet non intellegunt, errore maximo, si Epicurum audire voluerint, liberabuntur: istae enim vestrae eximiae pulchraeque virtutes nisi voluptatem efficerent, quis eas aut laudabilis aut expetendas arbitraretur? ut enim medicorum scientiam non ipsius artis, sed bonae valetudinis causa probamus, et gubernatoris ars, quia bene navigandi rationem habet, utilitate, non arte laudatur, sic sapientia, quae ars vivendi putanda est, non expeteretur, si nihil efficeret; nunc expetitur, quod est tamquam artifex conquirendae et comparandae voluptatis --\n\n[43] Quam autem ego dicam voluptatem, iam videtis, ne invidia verbi labefactetur oratio mea --. nam cum ignoratione rerum bonarum et malarum maxime hominum vita vexetur, ob eumque errorem et voluptatibus maximis saepe priventur et durissimis animi doloribus torqueantur, sapientia est adhibenda, quae et terroribus cupiditatibusque detractis et omnium falsarum opinionum temeritate derepta certissimam se nobis ducem praebeat ad voluptatem. sapientia enim est una, quae maestitiam pellat ex animis, quae nos exhorrescere metu non sinat. qua praeceptrice in tranquillitate vivi potest omnium cupiditatum ardore restincto. cupiditates enim sunt insatiabiles, quae non modo singulos homines, sed universas familias evertunt, totam etiam labefactant saepe rem publicam.\n\n[44] Ex cupiditatibus odia, discidia, discordiae, seditiones, bella nascuntur, nec eae se foris solum iactant nec tantum in alios caeco impetu incurrunt, sed intus etiam in animis inclusae inter se dissident atque discordant, ex quo vitam amarissimam necesse est effici, ut sapiens solum amputata circumcisaque inanitate omni et errore naturae finibus contentus sine aegritudine possit et sine metu vivere.\n\n[45] Quae est enim aut utilior aut ad bene vivendum aptior partitio quam illa, qua est usus Epicurus? qui unum genus posuit earum cupiditatum, quae essent et naturales et necessariae, alterum, quae naturales essent nec tamen necessariae, tertium, quae nec naturales nec necessariae. quarum ea ratio est, ut necessariae nec opera multa nec impensa expleantur; ne naturales quidem multa desiderant, propterea quod ipsa natura divitias, quibus contenta sit, et parabilis et terminatas habet; inanium autem cupiditatum nec modus ullus nec finis inveniri potest.\n\n[46] Quodsi vitam omnem perturbari videmus errore et inscientia, sapientiamque esse solam, quae nos a libidinum impetu et a formidinum terrore vindicet et ipsius fortunae modice ferre doceat iniurias et omnis monstret vias, quae ad quietem et ad tranquillitatem ferant, quid est cur dubitemus dicere et sapientiam propter voluptates expetendam et insipientiam propter molestias esse fugiendam?\n\n[47] Eademque ratione ne temperantiam quidem propter se expetendam esse dicemus, sed quia pacem animis afferat et eos quasi concordia quadam placet ac leniat. temperantia est enim, quae in rebus aut expetendis aut fugiendis ut rationem sequamur monet. nec enim satis est iudicare quid faciendum non faciendumve sit, sed stare etiam oportet in eo, quod sit iudicatum. plerique autem, quod tenere atque servare id, quod ipsi statuerunt, non possunt, victi et debilitati obiecta specie voluptatis tradunt se libidinibus constringendos nec quid eventurum sit provident ob eamque causam propter voluptatem et parvam et non necessariam et quae vel aliter pararetur et qua etiam carere possent sine dolore tum in morbos gravis, tum in damna, tum in dedecora incurrunt, saepe etiam legum iudiciorumque poenis obligantur.\n\n[48] Qui autem ita frui volunt voluptatibus, ut nulli propter eas consequantur dolores, et qui suum iudicium retinent, ne voluptate victi faciant id, quod sentiant non esse faciendum, ii voluptatem maximam adipiscuntur praetermittenda voluptate. idem etiam dolorem saepe perpetiuntur, ne, si id non faciant, incidant in maiorem. ex quo intellegitur nec intemperantiam propter se esse fugiendam temperantiamque expetendam, non quia voluptates fugiat, sed quia maiores consequatur.\n\n[49] Eadem fortitudinis ratio reperietur. nam neque laborum perfunctio neque perpessio dolorum per se ipsa allicit nec patientia nec assiduitas nec vigiliae nec ea ipsa, quae laudatur, industria, ne fortitudo quidem, sed ista sequimur, ut sine cura metuque vivamus animumque et corpus, quantum efficere possimus, molestia liberemus. ut enim mortis metu omnis quietae vitae status perturbatur, et ut succumbere doloribus eosque humili animo inbecilloque ferre miserum est, ob eamque debilitatem animi multi parentes, multi amicos, non nulli patriam, plerique autem se ipsos penitus perdiderunt, sic robustus animus et excelsus omni est liber cura et angore, cum et mortem contemnit, qua qui affecti sunt in eadem causa sunt, qua ante quam nati, et ad dolores ita paratus est, ut meminerit maximos morte finiri, parvos multa habere intervalla requietis, mediocrium nos esse dominos, ut, si tolerabiles sint, feramus, si minus, animo aequo e vita, cum ea non placeat, tamquam e theatro exeamus. quibus rebus intellegitur nec timiditatem ignaviamque vituperari nec fortitudinem patientiamque laudari suo nomine, sed illas reici, quia dolorem pariant, has optari, quia voluptatem.\n\n[50] Iustitia restat, ut de omni virtute sit dictum. sed similia fere dici possunt. ut enim sapientiam, temperantiam, fortitudinem copulatas esse docui cum voluptate, ut ab ea nullo modo nec divelli nec distrahi possint, sic de iustitia iudicandum est, quae non modo numquam nocet cuiquam, sed contra semper afficit cum vi sua atque natura, quod tranquillat animos, tum spe nihil earum rerum defuturum, quas natura non depravata desiderat. [et] quem ad modum temeritas et libido et ignavia semper animum excruciant et semper sollicitant turbulentaeque sunt, sic [inprobitas si] cuius in mente consedit, hoc ipso, quod adest, turbulenta est; si vero molita quippiam est, quamvis occulte fecerit, numquam tamen id confidet fore semper occultum. plerumque improborum facta primo suspicio insequitur, dein sermo atque fama, tum accusator, tum iudex;\n\n[51] Multi etiam, ut te consule, ipsi se indicaverunt. quodsi qui satis sibi contra hominum conscientiam saepti esse et muniti videntur, deorum tamen horrent easque ipsas sollicitudines, quibus eorum animi noctesque diesque exeduntur, a diis inmortalibus supplicii causa importari putant. quae autem tanta ex improbis factis ad minuendas vitae molestias accessio potest fieri, quanta ad augendas, cum conscientia factorum, tum poena legum odioque civium? et tamen in quibusdam neque pecuniae modus est neque honoris neque imperii nec libidinum nec epularum nec reliquarum cupiditatum, quas nulla praeda umquam improbe parta minuit, [sed] potius inflammat, ut coercendi magis quam dedocendi esse videantur.\n\n[52] Invitat igitur vera ratio bene sanos ad iustitiam, aequitatem, fidem, neque homini infanti aut inpotenti iniuste facta conducunt, qui nec facile efficere possit, quod conetur, nec optinere, si effecerit, et opes vel fortunae vel ingenii liberalitati magis conveniunt, qua qui utuntur, benivolentiam sibi conciliant et, quod aptissimum est ad quiete vivendum, caritatem, praesertim cum omnino nulla sit causa peccandi.\n\n[53] Quae enim cupiditates a natura proficiscuntur, facile explentur sine ulla iniuria, quae autem inanes sunt, iis parendum non est. nihil enim desiderabile concupiscunt, plusque in ipsa iniuria detrimenti est quam in iis rebus emolumenti, quae pariuntur iniuria. Itaque ne iustitiam quidem recte quis dixerit per se ipsam optabilem, sed quia iucunditatis vel plurimum afferat. nam diligi et carum esse iucundum est propterea, quia tutiorem vitam et voluptatem pleniorem efficit. itaque non ob ea solum incommoda, quae eveniunt inprobis, fugiendam inprobitatem putamus, sed multo etiam magis, quod, cuius in animo versatur, numquam sinit eum respirare, numquam adquiescere.\n\n[54] Quodsi ne ipsarum quidem virtutum laus, in qua maxime ceterorum philosophorum exultat oratio, reperire exitum potest, nisi derigatur ad voluptatem, voluptas autem est sola, quae nos vocet ad se et alliciat suapte natura, non potest esse dubium, quin id sit summum atque extremum bonorum omnium, beateque vivere nihil aliud sit nisi cum voluptate vivere.\n\n[55] Huic certae stabilique sententiae quae sint coniuncta explicabo brevi. nullus in ipsis error est finibus bonorum et malorum, id est in voluptate aut in dolore, sed in his rebus peccant, cum e quibus haec efficiantur ignorant. animi autem voluptates et dolores nasci fatemur e corporis voluptatibus et doloribus -- itaque concedo, quod modo dicebas, cadere causa, si qui e nostris aliter existimant, quos quidem video esse multos, sed imperitos --, quamquam autem et laetitiam nobis voluptas animi et molestiam dolor afferat, eorum tamen utrumque et ortum esse e corpore et ad corpus referri, nec ob eam causam non multo maiores esse et voluptates et dolores animi quam corporis. nam corpore nihil nisi praesens et quod adest sentire possumus, animo autem et praeterita et futura. ut enim aeque doleamus animo, cum corpore dolemus, fieri tamen permagna accessio potest, si aliquod aeternum et infinitum impendere malum nobis opinemur. quod idem licet transferre in voluptatem, ut ea maior sit, si nihil tale metuamus.\n\n[56] Iam illud quidem perspicuum est, maximam animi aut voluptatem aut molestiam plus aut ad beatam aut ad miseram vitam afferre momenti quam eorum utrumvis, si aeque diu sit in corpore. Non placet autem detracta voluptate aegritudinem statim consequi, nisi in voluptatis locum dolor forte successerit, at contra gaudere nosmet omittendis doloribus, etiamsi voluptas ea, quae sensum moveat, nulla successerit, eoque intellegi potest quanta voluptas sit non dolere.\n\n[57] Sed ut iis bonis erigimur, quae expectamus, sic laetamur iis, quae recordamur. stulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant. est autem situm in nobis ut et adversa quasi perpetua oblivione obruamus et secunda iucunde ac suaviter meminerimus. sed cum ea, quae praeterierunt, acri animo et attento intuemur, tum fit ut aegritudo sequatur, si illa mala sint, laetitia, si bona.\n\nO praeclaram beate vivendi et apertam et simplicem et directam viam! Cum enim certe nihil homini possit melius esse quam vacare omni dolore et molestia perfruique maximis et animi et corporis voluptatibus, videtisne quam nihil praetermittatur quod vitam adiuvet, quo facilius id, quod propositum est, summum bonum consequamur? clamat Epicurus, is quem vos nimis voluptatibus esse deditum dicitis; non posse iucunde vivi, nisi sapienter, honeste iusteque vivatur, nec sapienter, honeste, iuste, nisi iucunde.\n\n[58] Neque enim civitas in seditione beata esse potest nec in discordia dominorum domus; quo minus animus a se ipse dissidens secumque discordans gustare partem ullam liquidae voluptatis et liberae potest. atqui pugnantibus et contrariis studiis consiliisque semper utens nihil quieti videre, nihil tranquilli potest.\n\n[59] Quodsi corporis gravioribus morbis vitae iucunditas impeditur, quanto magis animi morbis impediri necesse est! animi autem morbi sunt cupiditates inmensae et inanes divitiarum, gloriae, dominationis, libidinosarum etiam voluptatum. accedunt aegritudines, molestiae, maerores, qui exedunt animos conficiuntque curis hominum non intellegentium nihil dolendum esse animo, quod sit a dolore corporis praesenti futurove seiunctum. nec vero quisquam stultus non horum morborum aliquo laborat, nemo igitur est non miser.\n\n[60] Accedit etiam mors, quae quasi saxum Tantalo semper impendet, tum superstitio, qua qui est imbutus quietus esse numquam potest. praeterea bona praeterita non meminerunt, praesentibus non fruuntur, futura modo expectant, quae quia certa esse non possunt, conficiuntur et angore et metu maximeque cruciantur, cum sero sentiunt frustra se aut pecuniae studuisse aut imperiis aut opibus aut gloriae. nullas enim consequuntur voluptates, quarum potiendi spe inflammati multos labores magnosque susceperant.\n\n[61] ecce autem alii minuti et angusti aut omnia semper desperantes aut malivoli, invidi, difficiles, lucifugi, maledici, monstruosi, alii autem etiam amatoriis levitatibus dediti, alii petulantes, alii audaces, protervi, idem intemperantes et ignavi, numquam in sententia permanentes, quas ob causas in eorum vita nulla est intercapedo molestiae. igitur neque stultorum quisquam beatus neque sapientium non beatus. Multoque hoc melius nos veriusque quam Stoici. illi enim negant esse bonum quicquam nisi nescio quam illam umbram, quod appellant honestum non tam solido quam splendido nomine, virtutem autem nixam hoc honesto nullam requirere voluptatem atque ad beate vivendum se ipsa esse contentam.\n\n[62] Sed possunt haec quadam ratione dici non modo non repugnantibus, verum etiam approbantibus nobis. sic enim ab Epicuro sapiens semper beatus inducitur: finitas habet cupiditates, neglegit mortem, de diis inmortalibus sine ullo metu vera sentit, non dubitat, si ita melius sit, migrare de vita. his rebus instructus semper est in voluptate. neque enim tempus est ullum, quo non plus voluptatum habeat quam dolorum. nam et praeterita grate meminit et praesentibus ita potitur, ut animadvertat quanta sint ea quamque iucunda, neque pendet ex futuris, sed expectat illa, fruitur praesentibus ab iisque vitiis, quae paulo ante collegi, abest plurimum et, cum stultorum vitam cum sua comparat, magna afficitur voluptate. dolores autem si qui incurrunt, numquam vim tantam habent, ut non plus habeat sapiens, quod gaudeat, quam quod angatur.\n\n[63] Optime vero Epicurus, quod exiguam dixit fortunam intervenire sapienti maximasque ab eo et gravissimas res consilio ipsius et ratione administrari neque maiorem voluptatem ex infinito tempore aetatis percipi posse, quam ex hoc percipiatur, quod videamus esse finitum. In dialectica autem vestra nullam existimavit esse nec ad melius vivendum nec ad commodius disserendum viam. In physicis plurimum posuit. ea scientia et verborum vis et natura orationis et consequentium repugnantiumve ratio potest perspici. omnium autem rerum natura cognita levamur superstitione, liberamur mortis metu, non conturbamur ignoratione rerum, e qua ipsa horribiles existunt saepe formidines. denique etiam morati melius erimus, cum didicerimus quid natura desideret. tum vero, si stabilem scientiam rerum tenebimus, servata illa, quae quasi delapsa de caelo est ad cognitionem omnium, regula, ad quam omnia iudicia rerum dirigentur, numquam ullius oratione victi sententia desistemus.\n\n[64] Nisi autem rerum natura perspecta erit, nullo modo poterimus sensuum iudicia defendere. quicquid porro animo cernimus, id omne oritur a sensibus; qui si omnes veri erunt, ut Epicuri ratio docet, tum denique poterit aliquid cognosci et percipi. quos qui tollunt et nihil posse percipi dicunt, ii remotis sensibus ne id ipsum quidem expedire possunt, quod disserunt. praeterea sublata cognitione et scientia tollitur omnis ratio et vitae degendae et rerum gerendarum. sic e physicis et fortitudo sumitur contra mortis timorem et constantia contra metum religionis et sedatio animi omnium rerum occultarum ignoratione sublata et moderatio natura cupiditatum generibusque earum explicatis, et, ut modo docui, cognitionis regula et iudicio ab eadem illa constituto veri a falso distinctio traditur.\n\n[65] Restat locus huic disputationi vel maxime necessarius de amicitia, quam, si voluptas summum sit bonum, affirmatis nullam omnino fore. de qua Epicurus quidem ita dicit, omnium rerum, quas ad beate vivendum sapientia comparaverit, nihil esse maius amicitia, nihil uberius, nihil iucundius. nec vero hoc oratione solum, sed multo magis vita et factis et moribus comprobavit. quod quam magnum sit fictae veterum fabulae declarant, in quibus tam multis tamque variis ab ultima antiquitate repetitis tria vix amicorum paria reperiuntur, ut ad Orestem pervenias profectus a Theseo. at vero Epicurus una in domo, et ea quidem angusta, quam magnos quantaque amoris conspiratione consentientis tenuit amicorum greges! quod fit etiam nunc ab Epicureis. sed ad rem redeamus; de hominibus dici non necesse est.\n\n[66] Tribus igitur modis video esse a nostris de amicitia disputatum. alii cum eas voluptates, quae ad amicos pertinerent, negarent esse per se ipsas tam expetendas, quam nostras expeteremus, quo loco videtur quibusdam stabilitas amicitiae vacillare, tuentur tamen eum locum seque facile, ut mihi videtur, expediunt. ut enim virtutes, de quibus ante dictum est, sic amicitiam negant posse a voluptate discedere. nam cum solitudo et vita sine amicis insidiarum et metus plena sit, ratio ipsa monet amicitias comparare, quibus partis confirmatur animus et a spe pariendarum voluptatum seiungi non potest.\n\n[67] Atque ut odia, invidiae, despicationes adversantur voluptatibus, sic amicitiae non modo fautrices fidelissimae, sed etiam effectrices sunt voluptatum tam amicis quam sibi, quibus non solum praesentibus fruuntur, sed etiam spe eriguntur consequentis ac posteri temporis. quod quia nullo modo sine amicitia firmam et perpetuam iucunditatem vitae tenere possumus neque vero ipsam amicitiam tueri, nisi aeque amicos et nosmet ipsos diligamus, idcirco et hoc ipsum efficitur in amicitia, et amicitia cum voluptate conectitur. nam et laetamur amicorum laetitia aeque atque nostra et pariter dolemus angoribus.\n\n[68] Quocirca eodem modo sapiens erit affectus erga amicum, quo in se ipsum, quosque labores propter suam voluptatem susciperet, eosdem suscipiet propter amici voluptatem. quaeque de virtutibus dicta sunt, quem ad modum eae semper voluptatibus inhaererent, eadem de amicitia dicenda sunt. praeclare enim Epicurus his paene verbis: 'Eadem', inquit, 'scientia confirmavit animum, ne quod aut sempiternum aut diuturnum timeret malum, quae perspexit in hoc ipso vitae spatio amicitiae praesidium esse firmissimum.'\n\n[69] Sunt autem quidam Epicurei timidiores paulo contra vestra convicia, sed tamen satis acuti, qui verentur ne, si amicitiam propter nostram voluptatem expetendam putemus, tota amicitia quasi claudicare videatur. itaque primos congressus copulationesque et consuetudinum instituendarum voluntates fieri propter voluptatem; cum autem usus progrediens familiaritatem effecerit, tum amorem efflorescere tantum, ut, etiamsi nulla sit utilitas ex amicitia, tamen ipsi amici propter se ipsos amentur. etenim si loca, si fana, si urbes, si gymnasia, si campum, si canes, si equos, si ludicra exercendi aut venandi consuetudine adamare solemus, quanto id in hominum consuetudine facilius fieri poterit et iustius?\n\n[70] Sunt autem, qui dicant foedus esse quoddam sapientium, ut ne minus amicos quam se ipsos diligant. quod et posse fieri intellegimus et saepe etiam videmus, et perspicuum est nihil ad iucunde vivendum reperiri posse, quod coniunctione tali sit aptius. Quibus ex omnibus iudicari potest non modo non impediri rationem amicitiae, si summum bonum in voluptate ponatur, sed sine hoc institutionem omnino amicitiae non posse reperiri.\n\n[71] Quapropter si ea, quae dixi, sole ipso illustriora et clariora sunt, si omnia dixi hausta e fonte naturae, si tota oratio nostra omnem sibi fidem sensibus confirmat, id est incorruptis atque integris testibus, si infantes pueri, mutae etiam bestiae paene loquuntur magistra ac duce natura nihil esse prosperum nisi voluptatem, nihil asperum nisi dolorem, de quibus neque depravate iudicant neque corrupte, nonne ei maximam gratiam habere debemus, qui hac exaudita quasi voce naturae sic eam firme graviterque comprehenderit, ut omnes bene sanos in viam placatae, tranquillae, quietae, beatae vitae deduceret? Qui quod tibi parum videtur eruditus, ea causa est, quod nullam eruditionem esse duxit, nisi quae beatae vitae disciplinam iuvaret.\n\n[72] An ille tempus aut in poetis evolvendis, ut ego et Triarius te hortatore facimus, consumeret, in quibus nulla solida utilitas omnisque puerilis est delectatio, aut se, ut Plato, in musicis, geometria, numeris, astris contereret, quae et a falsis initiis profecta vera esse non possunt et, si essent vera, nihil afferrent, quo iucundius, id est quo melius viveremus, eas ergo artes persequeretur, vivendi artem tantam tamque et operosam et perinde fructuosam relinqueret? non ergo Epicurus ineruditus, sed ii indocti, qui, quae pueros non didicisse turpe est, ea putant usque ad senectutem esse discenda.\n\nQuae cum dixisset, Explicavi, inquit, sententiam meam, et eo quidem consilio, tuum iudicium ut cognoscerem, quoniam mihi ea facultas, ut id meo arbitratu facerem, ante hoc tempus numquam est data.",
		"url": "https://la.wikisource.org/wiki/De_finibus_bonorum_et_malorum/Liber_Primus"
	},
	"ru": {
		"content": "Юнико́д[1] (англ. Unicode) — стандарт кодирования символов, включающий в себя знаки почти всех письменных языков мира[2]. В настоящее время стандарт является преобладающим в Интернете.\n\nСтандарт предложен в 1991 году некоммерческой организацией «Консорциум Юникода» (англ. Unicode Consortium, Unicode Inc.)[3][4]. Применение этого стандарта позволяет закодировать очень большое число символов из разных систем письменности: в документах, закодированных по стандарту Юникод, могут соседствовать китайские иероглифы, математические символы, буквы греческого алфавита, латиницы и кириллицы, символы музыкальной нотной нотации, при этом становится ненужным переключение кодовых страниц[5].\n\nСтандарт состоит из двух основных частей: универсального набора символов (англ. Universal character set, UCS) и семейства кодировок (англ. Unicode transformation format, UTF). Универсальный набор символов перечисляет допустимые по стандарту Юникод символы и присваивает каждому символу код в виде неотрицательного целого числа, записываемого обычно в шестнадцатеричной форме с префиксом U+, например, U+040F. Семейство кодировок определяет способы преобразования кодов символов для передачи в потоке или в файле.\n\nКоды в стандарте Юникод разделены на несколько областей. Область с кодами от U+0000 до U+007F содержит символы набора ASCII, и коды этих символов совпадают с их кодами в ASCII. Далее расположены области символов других систем письменности, знаки пунктуации и технические символы. Часть кодов зарезервирована для использования в будущем[6]. Под символы кириллицы выделены области знаков с кодами от U+0400 до U+052F, от U+2DE0 до U+2DFF, от U+A640 до U+A69F (см. Кириллица в Юникоде)[7].\n\nUnicode — это уникальный код для любого символа, независимо от платформы, независимо от программы, независимо от языка.Консорциум Юникода[8]\n\nК концу 1980-х годов стандартом стали 8-битные кодировки, их существовало уже большое множество, и постоянно появлялись новые. Это объяснялось как расширением круга поддерживаемых языков, так и стремлением создавать кодировки, частично совместимые между собой (характерный пример — появление альтернативной кодировки для русского языка, обусловленное эксплуатацией западных программ, созданных для кодировки CP437). В результате появилось несколько проблем:\n\nПроблема неправильной раскодировки вызывала появление в документе символов иностранных языков, не предполагавшихся в документе, или появление не предполагавшихся псевдографических символов, прозванных русскоязычными пользователями «кракозябрами». Проблема во многом была вызвана отсутствием стандартизированной формы указания кодировки для файла или потока. Проблему можно было решить либо последовательным внедрением стандарта указания кодировки, либо внедрением общей для всех языков кодировки.[5]\n\nПроблема ограниченности набора символов[5]. Проблему можно было решить либо переключением шрифтов внутри документа, либо внедрением «широкой» кодировки. Переключение шрифтов издавна практиковалось в текстовых процессорах, причём часто использовались шрифты с нестандартной кодировкой, т. н. «dingbat fonts». В итоге при попытке переноса документа в другую систему все нестандартные символы превращались в «кракозябры».\n\nПроблема преобразования одной кодировки в другую. Проблему можно было решить либо составлением таблиц перекодировки для каждой пары кодировок, либо использованием промежуточного преобразования в третью кодировку, включающую все символы всех кодировок[9].\n\nПроблема дублирования шрифтов. Для каждой кодировки создавался свой шрифт, даже если наборы символов в кодировках совпадали частично или полностью. Проблему можно было решить путём создания «больших» шрифтов, из которых впоследствии выбирались бы нужные для данной кодировки символы. Однако это требовало создания единого реестра символов, чтобы определять, чему что соответствует.\n\nБыла признана необходимость создания единой «широкой» кодировки. Кодировки с переменной длиной символа, широко использующиеся в Восточной Азии, были признаны слишком сложными в использовании, поэтому было решено использовать символы фиксированной ширины. Использование 32-битных символов казалось слишком расточительным, поэтому было решено использовать 16-битные.\n\nПервая версия Юникода представляла собой кодировку с фиксированным размером символа в 16 бит, то есть общее число кодов было 216 (65 536). С тех пор символы стали обозначать четырьмя шестнадцатеричными цифрами (например, U+04F0). При этом в Юникоде планировалось кодировать не все существующие символы, а только те, которые необходимы в повседневном обиходе. Редко используемые символы должны были размещаться в «области пользовательских символов» (private use area), которая первоначально занимала коды U+D800…U+F8FF. Чтобы использовать Юникод также и в качестве промежуточного звена при преобразовании разных кодировок друг в друга, в него включили все символы, представленные во всех наиболее известных кодировках.\n\nВ дальнейшем, однако, было принято решение кодировать все символы и в связи с этим значительно расширить кодовую область. Одновременно с этим коды символов стали рассматриваться не как 16-битные значения, а как абстрактные числа, которые в компьютере могут представляться множеством разных способов (см. способы представления).\n\nПоскольку в ряде компьютерных систем (например, Windows NT[10]) фиксированные 16-битные символы уже использовались в качестве кодировки по умолчанию, было решено все наиболее важные знаки кодировать только в пределах первых 65 536 позиций (так называемая англ. Basic Multilingual Plane, BMP). Остальное пространство используется для «дополнительных символов» (англ. supplementary characters): систем письма вымерших языков или очень редко используемых китайских иероглифов, математических и музыкальных символов.\n\nДля совместимости со старыми 16-битными системами была изобретена система UTF-16, где первые 65 536 позиций, за исключением позиций из интервала U+D800…U+DFFF, отображаются непосредственно как 16-битные числа, а остальные представляются в виде «суррогатных пар» (первый элемент пары из области U+D800…U+DBFF, второй элемент пары из области U+DC00…U+DFFF). Для суррогатных пар была использована часть кодового пространства (2048 позиций), отведённого «для частного использования».\n\nПоскольку в UTF-16 можно отобразить только 220+216−2048 (1 112 064) символов, то это число и было выбрано в качестве окончательной величины кодового пространства Юникода (диапазон кодов: 0x000000-0x10FFFF).\n\nХотя кодовая область Юникода была расширена за пределы 216 уже в версии 2.0, первые символы в «верхней» области были размещены только в версии 3.1.\n\nРоль этой кодировки в веб-секторе постоянно растёт. На начало 2010 доля веб-сайтов, использующих Юникод, составила около 50 %[11].\n\nРабота по доработке стандарта продолжается. Новые версии выпускаются по мере изменения и пополнения таблиц символов. Параллельно выпускаются новые документы ISO/IEC 10646.\n\nПервый стандарт выпущен в 1991 году, последней версией на данный момент является ​15.1.0 (12 сентября 2023)[12]. Версии стандарта 1.0—5.0 публиковались как книги и имеют ISBN[13][14].\n\nНомер версии стандарта составлен из трёх цифр (например, 3.1.1). Третью цифру меняют при внесении в стандарт небольших изменений, не добавляющих новых символов (исключение — версия 1.0.1, в которой добавлены унифицированные идеограммы китайского, японского и корейского письма)[15].\n\nБаза данных символов Юникода (Unicode Character Database) доступна для всех версий на официальном сайте как в простом текстовом, так и в XML-формате. Файлы распространяются под BSD-подобной лицензией.\n\nISO/IEC 10646-2:2001\n\nISO/IEC 10646-2:2001\n\n\n\nХотя форма записи UTF-8 позволяет кодировать до 221 (2 097 152) кодовых позиций, было принято решение использовать лишь 1 112 064 для совместимости с UTF-16. Впрочем, даже и этого в данный момент более чем достаточно — в версии 15.1 используется всего 149 878 кодовых позиций.\n\nКодовое пространство разбито на 17 плоскостей (англ. planes) по 216 (65 536) символов. Нулевая плоскость (plane 0) называется базовой (basic) и содержит символы наиболее употребительных письменностей. Остальные плоскости — дополнительные (supplementary). Первая плоскость (plane 1) используется в основном для исторических письменностей, вторая (plane 2) — для редко используемых иероглифов китайского письма (ККЯ), третья (plane 3) зарезервирована для архаичных китайских иероглифов[66]. Плоскость 14 отведена для символов, используемых по особому назначению. Плоскости 15 и 16 выделены для частного употребления[6].\n\nДля обозначения символов Unicode используется запись вида «U+xxxx» (для кодов 0…FFFF), или «U+xxxxx» (для кодов 10000…FFFFF), или «U+xxxxxx» (для кодов 100000…10FFFF), где xxx — шестнадцатеричные цифры. Например, символ «я» (U+044F) имеет код 044F16 = 110310.\n\nУниверсальная система кодирования (Юникод) представляет собой набор графических символов и способ их кодирования для компьютерной обработки текстовых данных.\n\nГрафические или печатаемые символы — это символы, имеющие видимое изображение. Графическим символам противопоставляются управляющие и форматирующие символы.\n\nГрафические символы включают в себя следующие группы:\n\nЮникод — это система для линейного представления текста. Символы, имеющие дополнительные над- или подстрочные элементы, могут быть представлены в виде построенной по определённым правилам последовательности кодов (составной вариант, composite character) или в виде единого символа (монолитный вариант, precomposed character). С 2014 года считается, что все буквы крупных письменностей в Юникод внесены, и если символ доступен в составном варианте, дублировать его в монолитном виде не нужно.\n\nКонсорциум не создаёт нового, а констатирует сложившийся порядок вещей[68]. Например, картинки «эмодзи» были добавлены потому, что японские операторы мобильной связи широко их использовали. Для этого добавление символа проходит через сложный процесс[68]. И, например, символ российского рубля прошёл его за три месяца, как только получил официальный статус, причём до этого он много лет де-факто использовался и его отказывались включить в Юникод.\n\nТоварные знаки кодируют только в порядке исключения. Так, в Юникоде нет флага Windows или яблока Apple.\n\nЭмодзи не вводятся в Юникод, если:[69]\n\nСимволы в Юникоде подразделяются на базовые (англ. base characters) и комбинирующие (англ. combining marks). Метки обычно следуют за базовым символом и изменяют его отображение определённым образом. К комбинирующим символам, например, относятся диакритические знаки, знаки ударения. Например, русскую букву «Й» в Юникоде можно записать в виде базового символа «И» (U+0418) и комбинирующего символа « ̆» (U+0306), отображаемого над базовым.\n\nКомбинирующие символы помечены в таблицах символов Юникода особыми категориями:\n\nОсобый тип комбинирующих символов — селекторы варианта начертания (англ. variation selectors). Они действуют только на те базовые символы, для которых такие варианты определены. К примеру, в версии Юникода 5.0 варианты начертания определены для ряда математических символов, для символов традиционного монгольского алфавита и для символов монгольского квадратного письма.\n\nИз-за наличия в Юникоде комбинирующих символов одни и те же знаки письменности можно представить различными кодами. Так, например, букву «Й» в примере выше можно записать как отдельным символом, так и сочетанием базового и комбинированного. Из-за этого сравнение строк байт за байтом становится невозможным. Алгоритмы нормализации (англ. normalization forms) решают эту проблему, выполняя приведение символов к определённому стандартному виду. Приведение осуществляется путём замены символов на эквивалентные с использованием таблиц и правил. «Декомпозицией» называется замена (разложение) одного символа на несколько составляющих символов, а «композицией», наоборот, — замена (соединение) нескольких составляющих символов на один символ.\n\nВ стандарте Юникода определены четыре алгоритма нормализации текста: NFD, NFC, NFKD и NFKC.\n\nNFD, англ. normalization form D («D» от англ. decomposition), форма нормализации D — каноническая декомпозиция — алгоритм, согласно которому выполняется рекурсивное разложение составных символов (англ. precomposed characters) на последовательность из одного или нескольких простых символов в соответствии с таблицами декомпозиции. Рекурсивное потому, что в процессе разложения составной символ может быть разложен на несколько других, некоторые из которых тоже являются составными, и к которым применяется дальнейшее разложение.\n\nПримеры:\n\nNFC, англ. normalization form C («C» от англ. composition), форма нормализации C — алгоритм, согласно которому последовательно выполняются каноническая декомпозиция и каноническая композиция. Сначала каноническая декомпозиция (алгоритм NFD) приводит текст к форме D. Затем каноническая композиция — операция, обратная NFD, обрабатывает текст от начала к концу с учётом следующих правил:\n\nПример:\n\nNFKD, англ. normalization form KD, форма нормализации KD — совместимая декомпозиция — алгоритм, согласно которому последовательно выполняются каноническая декомпозиция и замены символов текста по таблицам совместимой декомпозиции. Таблицы совместимой декомпозиции предусматривают замену на почти эквивалентные символы[70]:\n\nПримеры:\n\nNFKC, англ. normalization form KC, форма нормализации KC — алгоритм, согласно которому последовательно выполняются совместимая декомпозиция (алгоритм NFKD) и каноническая композиция (алгоритм NFC).\n\nСтандарт Юникод поддерживает письменности языков как с направлением написания слева направо (англ. left-to-right, LTR), так и с написанием справа налево (англ. right-to-left, RTL) — например, арабское и еврейское письмо. В обоих случаях символы хранятся в «естественном» порядке; их отображение с учётом нужного направления письма обеспечивается приложением.\n\nКроме того, Юникод поддерживает комбинированные тексты, сочетающие фрагменты с разным направлением письма. Данная возможность называется двунаправленность (англ. bidirectional text, BiDi). Некоторые упрощённые обработчики текста (например, в сотовых телефонах) могут поддерживать Юникод, но не иметь поддержки двунаправленности. Все символы Юникода поделены на несколько категорий: пишущиеся слева направо, пишущиеся справа налево, и пишущиеся в любом направлении. Символы последней категории (в основном это знаки пунктуации) при отображении принимают направление окружающего их текста.\n\nЮникод включает практически все современные письменности, в том числе:\n\nи другие.\n\nС академическими целями добавлены многие исторические письменности, в том числе: германские руны, древнетюркские руны, древнегреческая письменность, египетские иероглифы, клинопись, письменность майя, этрусский алфавит.\n\nВ Юникоде представлен широкий набор математических и музыкальных символов, а также пиктограмм.\n\nГосударственные флаги не включены в Юникод напрямую. Для их кодирования используются пары из 26 буквенных символов, предназначенных для представления двухбуквенных кодов стран по стандарту ISO 3166-1 alpha-2. Эти буквы закодированы в диапазоне от U+1F1E6 🇦 regional indicator symbol letter a (HTML &#127462;) до U+1F1FF 🇿 regional indicator symbol letter z (HTML &#127487;).\n\nВ Юникод принципиально не включаются логотипы компаний и продуктов, хотя они и встречаются в шрифтах (например, логотип Apple в кодировке MacRoman (0xF0) или логотип Windows в шрифте Wingdings (0xFF)). В юникодовских шрифтах логотипы должны размещаться только в области пользовательских символов. Существуют свободные бесплатные шрифты, включающие в себя логотипы компаний, программных продуктов и другие товарные знаки (например, Шрифт Awesome[англ.][71]).\n\nКонсорциум Юникода работает в тесной связи с рабочей группой ISO/IEC/JTC1/SC2/WG2, которая занимается разработкой международного стандарта 10646 (ISO/IEC 10646). Между стандартом Юникода и ISO/IEC 10646 установлена синхронизация, хотя каждый стандарт использует свою терминологию и систему документации.\n\nСотрудничество Консорциума Юникода с Международной организацией по стандартизации (англ. International Organization for Standardization, ISO) началось в 1991 году. В 1993 году ISO выпустила стандарт DIS 10646.1. Для синхронизации с ним Консорциум утвердил стандарт Юникода версии 1.1, в который были внесены дополнительные символы из DIS 10646.1. В результате значения закодированных символов в Unicode 1.1 и DIS 10646.1 полностью совпали.\n\nВ дальнейшем сотрудничество двух организаций продолжилось. В 2000 году стандарт Unicode 3.0 был синхронизирован с ISO/IEC 10646-1:2000. Предстоящая третья версия ISO/IEC 10646 будет синхронизирована с Unicode 4.0. Возможно, эти спецификации даже будут опубликованы как единый стандарт.\n\nАналогично форматам UTF-16 и UTF-32 в стандарте Юникода, стандарт ISO/IEC 10646 также имеет две основные формы кодирования символов: UCS-2 (2 байта на символ, аналогично UTF-16) и UCS-4 (4 байта на символ, аналогично UTF-32). UCS значит универсальный набор кодированных символов (англ. universal coded character set). UCS-2 можно считать подмножеством UTF-16 (UTF-16 без суррогатных пар), а UCS-4 является синонимом для UTF-32.\n\nРазличия стандартов Юникод и ISO/IEC 10646:\n\nЮникод имеет несколько форм представления (англ. Unicode transformation format, UTF): UTF-8, UTF-16 (UTF-16BE, UTF-16LE) и UTF-32 (UTF-32BE, UTF-32LE). Была разработана также форма представления UTF-7 для передачи по семибитным каналам, но из-за несовместимости с ASCII она не получила распространения и не включена в стандарт. 1 апреля 2005 года были предложены две шуточные формы представления: UTF-9 и UTF-18 (RFC 4042).\n\nВ Microsoft Windows NT и основанных на ней системах Windows 2000 и Windows XP в основном используется форма UTF-16LE. В UNIX-подобных операционных системах GNU/Linux, BSD и Mac OS X принята форма UTF-8 для файлов и UTF-32 или UTF-8 для обработки символов в оперативной памяти.\n\nPunycode — другая форма кодирования последовательностей Unicode-символов в так называемые ACE-последовательности, которые состоят только из алфавитно-цифровых символов, как это разрешено в доменных именах.\n\nUTF-8 — представление Юникода, обеспечивающее наибольшую компактность и обратную совместимость с 7-битной системой ASCII; текст, состоящий только из символов с номерами меньше 128, при записи в UTF-8 превращается в обычный текст ASCII и может быть отображён любой программой, работающей с ASCII; и наоборот, текст, закодированный 7-битной ASCII может быть отображён программой, предназначенной для работы с UTF-8. Остальные символы Юникода изображаются последовательностями длиной от 2 до 4 байт, в которых первый байт всегда имеет маску 11xxxxxx, а остальные — 10xxxxxx. В UTF-8 не используются суррогатные пары.\n\nФормат UTF-8 был изобретён 2 сентября 1992 года Кеном Томпсоном и Робом Пайком и реализован в ОС Plan 9[72]. Сейчас стандарт UTF-8 официально закреплён в документах RFC 3629 и ISO/IEC 10646 Annex D.\n\nUTF-16 — кодировка, позволяющая записывать символы Юникода в диапазонах U+0000…U+D7FF и U+E000…U+10FFFF (общим количеством 1 112 064). При этом каждый символ записывается одним или двумя словами (суррогатная пара). Кодировка UTF-16 описана в приложении Q к международному стандарту ISO/IEC 10646, а также ей посвящён документ IETF RFC 2781 под названием «UTF-16, an encoding of ISO 10646».\n\nUTF-32 — способ представления Юникода, при котором каждый символ занимает ровно 4 байта. Главное преимущество UTF-32 перед кодировками переменной длины заключается в том, что символы Юникод в ней непосредственно индексируемы, поэтому найти символ по номеру его позиции в файле можно чрезвычайно быстро, и получение любого символа n-й позиции при этом является операцией, занимающей всегда одинаковое время. Это также делает замену символов в строках UTF-32 очень простой. Напротив, кодировки с переменной длиной требуют последовательного доступа к символу n-й позиции, что может быть очень затратной по времени операцией. Главный недостаток UTF-32 — это неэффективное использование пространства, так как для хранения любого символа используется четыре байта. Символы, лежащие за пределами нулевой (базовой) плоскости кодового пространства, редко используются в большинстве текстов. Поэтому удвоение, в сравнении с UTF-16, занимаемого строками в UTF-32 пространства, зачастую не оправдано.\n\nВ потоке данных UTF-16 младший байт может записываться либо перед старшим (англ. UTF-16 little-endian, UTF-16LE), либо после старшего (англ. UTF-16 big-endian, UTF-16BE). Аналогично существует два варианта четырёхбайтной кодировки — UTF-32LE и UTF-32BE.\n\nДля указания на использование Юникода, в начале текстового файла или потока может передаваться Маркер последовательности байтов (англ. byte order mark (BOM)) — символ U+FEFF (неразрывный пробел нулевой ширины). По его виду можно легко различить как формат представления Юникода, так и последовательность байтов. Маркер последовательности байтов может принимать следующий вид:\n\nВнедрение Юникода привело к изменению подхода к традиционным 8-битным кодировкам. Если раньше такая кодировка всегда задавалась непосредственно, то теперь она может задаваться таблицей соответствия между данной кодировкой и Юникодом. Фактически почти все 8-битные кодировки теперь можно рассматривать как форму представления некоторого подмножества Юникода. И это намного упростило создание программ, которые должны работать с множеством разных кодировок: теперь, чтобы добавить поддержку ещё одной кодировки, надо всего лишь добавить ещё одну таблицу перекодировки символов в Юникод.\n\nКроме того, многие форматы данных позволяют вставлять любые символы Юникода, даже если документ записан в старой 8-битной кодировке. Например, в HTML можно использовать коды с амперсандом.\n\nБольшинство современных операционных систем в той или иной степени обеспечивает поддержку Юникода.\n\nВ операционных системах семейства Windows NT для внутреннего представления имён файлов и других системных строк используется двухбайтовая кодировка UTF-16LE. Системные вызовы, принимающие строковые параметры, существуют в однобайтном и двухбайтном вариантах. Подробнее см. в статье Юникод в операционных системах семейства Microsoft Windows.\n\nUNIX-подобные операционные системы, в том числе GNU/Linux, BSD, OS X, используют для представления Юникода кодировку UTF-8. Большинство программ может работать с UTF-8 как с традиционными однобайтными кодировками, не обращая внимания на то, что символ представляется как несколько последовательных байт. Для работы с отдельными символами строки обычно перекодируются в UCS-4, так что каждому символу соответствует машинное слово.\n\nОдной из первых успешных коммерческих реализаций Юникода стала среда программирования Java. В ней принципиально отказались от 8-битного представления символов в пользу 16-битного. Это решение увеличило расход памяти, но позволило вернуть в программирование важную абстракцию: произвольный одиночный символ (тип char). В частности, программист мог работать со строкой, как с простым массивом. Успех не был окончательным, Юникод перерос ограничение в 16 бит и к версии J2SE 5.0 произвольный символ снова стал занимать переменное число единиц памяти — один char или два (см. суррогатная пара).\n\nСейчас[когда?] большинство[сколько?] языков программирования поддерживает строки Юникода, хотя их представление может различаться в зависимости от реализации.\n\nПоскольку ни одна раскладка клавиатуры не может позволить вводить все символы Юникода одновременно, от операционных систем и прикладных программ требуется поддержка альтернативных методов ввода произвольных символов Юникода.\n\nНачиная с Windows 2000, служебная программа «Таблица символов» (charmap.exe) поддерживает символы Юникода и позволяет копировать их в буфер обмена. Реализована поддержка только базовой плоскости (коды символов U+0000…U+FFFF); символы с кодами от U+10000 «Таблица символов» не отображает. Похожая таблица есть в Microsoft Word.\n\nИногда можно набрать шестнадцатеричный код, нажать Alt+X, и код будет заменён на соответствующий символ, например, в WordPad, Microsoft Word. В редакторах Alt+X выполняет и обратное преобразование. В программах, работающих в среде Windows, чтобы получить символ Unicode, нужно при нажатой клавише Alt набрать десятичное значение кода символа на цифровой клавиатуре: например, комбинации Alt+0171 и Alt+0187 выводят левую и правую кавычки-ёлочки, соответственно, Alt+0151 — длинное тире, Alt+0769 — знак ударения, Alt+0133 — многоточие и пр.\n\nВ Mac OS 8.5 и более поздних версиях поддерживается метод ввода, называемый «Unicode Hex Input». При зажатой клавише Option требуется набрать четырёхзначный шестнадцатеричный код требуемого символа. Этот метод позволяет вводить символы с кодами, большими U+FFFD, используя пары суррогатов; такие пары операционной системой будут автоматически заменены на одиночные символы. Этот метод ввода перед использованием нужно активизировать в соответствующем разделе системных настроек и затем выбрать как текущий метод ввода в меню клавиатуры.\n\nНачиная с Mac OS X 10.2, существует также приложение «Character Palette», позволяющее выбирать символы из таблицы, в которой можно выделять символы определённого блока или символы, поддерживаемые конкретным шрифтом.\n\nВ GNOME также есть утилита «Таблица символов» (ранее gucharmap), позволяющая отображать символы определённого блока или системы письма и предоставляющая возможность поиска по названию или описанию символа. Когда код нужного символа известен, его можно ввести в соответствии со стандартом ISO 14755: при зажатых клавишах Ctrl+⇧ Shift ввести шестнадцатеричный код (начиная с некоторой версии GTK+, ввод кода нужно предварить нажатием клавиши «U»). Вводимый шестнадцатеричный код может иметь до 32 бит в длину, позволяя вводить любые символы Юникода без использования суррогатных пар.\n\nВсе приложения X Window, включая GNOME и KDE, поддерживают ввод при помощи клавиши Compose. Для клавиатур, на которых нет отдельной клавиши Compose, для этой цели можно назначить любую клавишу — например, ⇪ Caps Lock.\n\nКонсоль GNU/Linux также допускает ввод символа Юникода по его коду — для этого десятичный код символа нужно ввести цифрами расширенного блока клавиатуры при зажатой клавише Alt. Можно вводить символы и по их шестнадцатеричному коду: для этого нужно зажать клавишу AltGr, и для ввода цифр A—F использовать клавиши расширенного блока клавиатуры от NumLock до ↵ Enter (по часовой стрелке). Поддерживается также и ввод в соответствии с ISO 14755. Для того чтобы перечисленные способы могли работать, нужно включить в консоли режим Юникода вызовом unicode_start(1) и выбрать подходящий шрифт вызовом setfont(8).\n\nMozilla Firefox для Linux поддерживает ввод символов по ISO 14755.\n\nВ Юникоде английское «a» и польское «a» — один и тот же символ. Точно так же одним и тем же символом (но отличающимся от «a» латинского) считаются русское «а» и сербское «а». Такой принцип кодирования не универсален; по-видимому, решения «на все случаи жизни» вообще не может существовать.\n\nНекоторые недостатки связаны не с самим Юникодом, а с возможностями обработчиков текста.\n\nНекоторые редкие системы письма всё ещё не представлены должным образом в Юникоде. Изображение «длинных» надстрочных символов, простирающихся над несколькими буквами, как, например, в церковнославянском языке, пока не реализовано.\n\n«Unicode» — одновременно и имя собственное (или часть имени, например, Unicode Consortium), и имя нарицательное, происходящее из английского языка.\n\nНа первый взгляд предпочтительнее использовать написание «Уникод». В русском языке уже есть морфемы «уни-» (слова с латинским элементом «uni-» традиционно переводились и писались через «уни-»: универсальный, униполярный, унификация, униформа) и «код». Напротив, торговые марки, заимствованные из английского языка, обычно передаются посредством практической транскрипции, в которой деэтимологизированное сочетание букв «uni-» записывается в виде «юни-» («Юнилевер», «Юникс» и т. п.), то есть точно так же, как в случае с побуквенными сокращениями, вроде UNICEF «United Nations International Children’s Emergency Fund» — ЮНИСЕФ.\n\nНа сайте Консорциума есть специальная страница, где рассматриваются проблемы передачи слова «Unicode» в различных языках и системах письма. Для русской кириллицы указан вариант «Юникод»[1]. В MS Windows также используется вариант «Юникод».\n\nВ Википедии на русском языке используется вариант «Юникод» как наиболее распространённый.",
		"url": "https://ru.wikipedia.org/wiki/Unicode"
	},
	"th": {
		"content": "\n\nคอมพิวเตอร์ (อังกฤษ: computer) หรือศัพท์บัญญัติราชบัณฑิตยสภาว่า คณิตกรณ์[2][3] เป็นเครื่องจักรแบบสั่งการได้ที่ออกแบบมาเพื่อดำเนินการกับลำดับตัวดำเนินการทางตรรกศาสตร์หรือคณิตศาสตร์ โดยอนุกรมนี้อาจเปลี่ยนแปลงได้เมื่อพร้อม ส่งผลให้คอมพิวเตอร์สามารถแก้ปัญหาได้มากมาย\n\nคอมพิวเตอร์ถูกประดิษฐ์ออกมาให้ประกอบไปด้วยความจำรูปแบบต่าง ๆ เพื่อเก็บข้อมูล อย่างน้อยหนึ่งส่วนที่มีหน้าที่ดำเนินการคำนวณเกี่ยวกับตัวดำเนินการทางตรรกศาสตร์ และตัวดำเนินการทางคณิตศาสตร์ และส่วนควบคุมที่ใช้เปลี่ยนแปลงลำดับของตัวดำเนินการโดยยึดสารสนเทศที่ถูกเก็บไว้เป็นหลัก อุปกรณ์เหล่านี้จะยอมให้นำเข้าข้อมูลจากแหล่งภายนอก และส่งผลจากการคำนวณตัวดำเนินการออกไป\n\nหน่วยประมวลผลของคอมพิวเตอร์มีหน้าที่ดำเนินการกับคำสั่งต่าง ๆ ที่คอยสั่งให้อ่าน ประมวล และเก็บข้อมูลไว้ คำสั่งต่าง ๆ ที่มีเงื่อนไขจะแปลงชุดคำสั่งให้ระบบและสิ่งแวดล้อมรอบ ๆ เป็นฟังก์ชันที่สถานะปัจจุบัน\n\nคอมพิวเตอร์อิเล็กทรอนิกส์เครื่องแรกถูกพัฒนาขึ้นในช่วงกลางคริสต์ศตวรรษที่ 20 (ค.ศ. 1940 – ค.ศ. 1945) แรกเริ่มนั้น คอมพิวเตอร์มีขนาดเท่ากับห้องขนาดใหญ่ ซึ่งใช้พลังงานมากเท่ากับเครื่องคอมพิวเตอร์ส่วนบุคคล (พีซี) สมัยใหม่หลายร้อยเครื่องรวมกัน[4]\n\nคอมพิวเตอร์ในสมัยใหม่นี้ผลิตขึ้นโดยใช้วงจรรวม หรือวงจรไอซี (Integrated circuit) โดยมีความจุมากกว่าสมัยก่อนล้านถึงพันล้านเท่า และขนาดของตัวเครื่องใช้พื้นที่เพียงเศษส่วนเล็กน้อยเท่านั้น คอมพิวเตอร์อย่างง่ายมีขนาดเล็กพอที่จะถูกบรรจุไว้ในอุปกรณ์โทรศัพท์มือถือ และคอมพิวเตอร์มือถือนี้ใช้พลังงานจากแบตเตอรี่ขนาดเล็ก และหากจะมีคนพูดถึงคำว่า \"คอมพิวเตอร์\" มักจะหมายถึงคอมพิวเตอร์ส่วนบุคคลซึ่งถือเป็นสัญลักษณ์ของยุคสารสนเทศ อย่างไรก็ดี ยังมีคอมพิวเตอร์ชนิดฝังอีกมากมายที่พบได้ตั้งแต่ในเครื่องเล่นเอ็มพีสามจนถึงเครื่องบินบังคับ และของเล่นชนิดต่าง ๆ จนถึงหุ่นยนต์อุตสาหกรรม\n\nมีการบันทึกไว้ว่า ครั้งแรกที่มีการใช้คำว่า \"คอมพิวเตอร์\" คือเมื่อ ค.ศ. 1613 ซึ่งหมายถึงบุคคลที่ทำหน้าที่คาดการณ์ หรือคิดคำนวณ และมีความหมายเช่นนี้เรื่อยมาจนถึงกลางคริสต์ศตวรรษที่ 20 และตั้งแต่ปลายคริสต์ศตวรรษที่ 19 มา ความหมายของคำว่าคอมพิวเตอร์นี้เริ่มมีใช้กับเครื่องจักรที่ทำหน้าที่คิดคำนวณมากขึ้น[5]\n\nประวัติของคอมพิวเตอร์สมัยใหม่นั้นเริ่มต้นจากเทคโนโลยีสองชนิดที่แตกต่างกัน ได้แก่ การคำนวณโดยอัตโนมัติ กับการคำนวณที่สามารถโปรแกรมได้ (หมายถึงสร้างวิธีการทำงานและปรับแต่งได้) แต่ระบุแน่ชัดไม่ได้ว่าเทคโนโลยีชนิดใดเกิดขึ้นก่อน ส่วนหนึ่งเป็นเพราะการคำนวณแต่ละชนิดนั้นไม่มีความสอดคล้องกัน อุปกรณ์บางชนิดก็มีความสำคัญที่จะเอ่ยถึง อย่างเช่นเครื่องมือเชิงกลเพื่อการคำนวณบางชนิดที่ประสบความสำเร็จและยังใช้กันอยู่หลายศตวรรษก่อนที่จะมีเครื่องคิดเลขอิเล็กทรอนิกส์ อาทิลูกคิดของชาวสุเมเรียนที่ถูกออกแบบขึ้นราว 2,500 ปีก่อนคริสตกาล[6] ชนะการแข่งขันความเร็วในการคำนวณต่อเครื่องคำนวณตั้งโต๊ะเมื่อ ค.ศ. 1946 ที่ประเทศญี่ปุ่น[7] ต่อมาในคริสต์ทศวรรษ 1620 มีการประดิษฐ์สไลด์รูล ซึ่งถูกนำขึ้นยานอวกาศในภารกิจของโครงการอะพอลโลถึง 5 ครั้ง รวมถึงเมื่อครั้งที่สำรวจดวงจันทร์ด้วย[8] นอกจากนี้ยังมี เครื่องทำนายตำแหน่งดาวฤกษ์ (Astrolabe) และ กลไกอันติคือเธรา ซึ่งเป็นเครื่องคำนวณ (คอมพิวเตอร์) เกี่ยวกับดาราศาสตร์ยุคโบราณที่ชาวกรีกเป็นผู้สร้างขึ้นราว 80 ปีก่อนคริสตกาล[9] ที่มาของระบบการสั่งการโปรแกรมเกิดขึ้นเมื่อ ฮีโรแห่งอเล็กซานเดรีย (c.10-70 AD) นักคณิตศาสตร์ชาวกรีกสร้างโรงละครที่ประกอบด้วยเครื่องจักร ใช้แสดงละครความยาว 10 นาที และทำงานโดยมีกลไกเชือกและอิฐบล็อกทรงกระบอกที่ซับซ้อน ซึ่งสามารถตัดสินใจเลือกได้ว่าจะชิ้นส่วนกลไกใดใช้ในการแสดงฉากใดและเมื่อใด[10]\n\nราว ๆ ปลายศตวรรษที่ 10 สมเด็จพระสันตะปาปาซิลเวสเตอร์ที่ 2 นักบวชชาวฝรั่งเศส ได้นำลิ้นชักบรรจุอุปกรณ์ชนิดหนึ่งที่จะตอบคำถามได้ว่าใช่ หรือ ไม่ใช่ เมื่อถูกถามคำถาม (ด้วยเลขฐานสอง) [11] ซึ่งชาวมัวร์ประดิษฐ์ไว้กลับมาจากประเทศสเปน ในศตวรรษที่ 13 นักบุญอัลแบร์ตุส มาญุส และโรเจอร์ เบคอน นักปราชญ์ชาวอังกฤษ ได้สร้างหุ่นยนต์แอนดรอยด์ (android) พูดได้ โดยไม่ได้พัฒนาใด ๆ ต่ออีก (นักบุญอัลแบร์ตุส มาญุส บ่นออกมาว่าเขาเสียเวลาเปล่าไป 40 ปีในชีวิต เมื่อนักบุญโทมัส อควีนาสตกใจกับเครื่องนี้และได้ทำลายมันเสีย) [12]\n\nในปี ค.ศ. 1642 แห่งยุคฟื้นฟูศิลปวิทยา มีการประดิษฐ์เครื่องคำนวณของปาสคาลซึ่งเป็นเครื่องคำนวณตัวเลขเชิงกล[13] เป็นอุปกรณ์ที่จะสามารถคำนวณโดยใช้ตัวดำเนินการทางคณิตศาสตร์โดยไม่ต้องพึ่งสติปัญญามนุษย์[14] เครื่องคำนวณเชิงกลนี้ยังถือเป็นรากฐานของการพัฒนาคอมพิวเตอร์ในสองทาง แรกเริ่มนั้น ความพยายามที่จะพัฒนาเครื่องคำนวณที่มีสมรรถภาพสูงและยืดหยุ่น[15] ซึ่งทฤษฎีนี้ถูกสร้างโดยชาร์ลส แบบเบจ[16][17] และได้รับการพัฒนาในเวลาต่อมา[18] นำไปสู่การพัฒนาเมนเฟรมคอมพิวเตอร์ (คอมพิวเตอร์ขนาดใหญ่) ขึ้นในคริสต์ทศวรรษ 1960 และในขณะเดียวกัน อินเทล ก็สามารถประดิษฐ์ ไมโครโพรเซสเซอร์ ซึ่งถือเป็นจุดกำเนิดการเปลี่ยนแปลงครั้งใหญ่ของคอมพิวเตอร์ส่วนบุคคล และเป็นหัวใจสำคัญของระบบคอมพิวเตอร์หากไม่คำนึงถึงขนาดและวัตถุประสงค์[19] ขึ้นได้โดยบังเอิญ[20] ระหว่างการพัฒนาเครื่องคำนวณอิเล็กทรอนิกส์ บิซิคอม ที่พัฒนาสืบต่อจากเครื่องคำนวณเชิงกลโดยตรง\n\nในปัจจุบัน คอมพิวเตอร์ได้ใช้วงจรเบ็ดเสร็จขนาดใหญ่มาก (very large scale integrated circuit) ซึ่งสามารถบรรจุทรานซิสเตอร์ได้มากกว่าสิบล้านตัว เราสามารถแบ่งคอมพิวเตอร์ในรุ่นปัจจุบันออกเป็น 4 ประเภทดังต่อไปนี้\n\nซูเปอร์คอมพิวเตอร์ ถือได้ว่าเป็นคอมพิวเตอร์ที่มีความเร็วมาก และมีประสิทธิภาพสูงสุดเมื่อเปรียบเทียบกับคอมพิวเตอร์ชนิดอื่น ๆ เครื่องซูเปอร์คอมพิวเตอร์มีราคาแพงมาก มีขนาดใหญ่ สามารถคำนวณทางคณิตศาสตร์ได้หลายแสนล้านครั้งต่อวินาที และได้รับการออกแบบ เพื่อให้ใช้แก้ปัญหาขนาดใหญ่มากทางวิทยาศาสตร์และทางวิศวกรรมศาสตร์ได้อย่างรวดเร็ว เช่น การพยากรณ์อากาศล่วงหน้าเป็นเวลาหลายวัน การศึกษาผลกระทบของมลพิษกับสภาวะแวดล้อมซึ่งหากใช้คอมพิวเตอร์ชนิดอื่น ๆ แก้ไขปัญหาประเภทนี้ อาจจะต้องใช้เวลาในการคำนวณหลายปีกว่าจะเสร็จสิ้น ในขณะที่ซูเปอร์คอมพิวเตอร์สามารถแก้ไขปัญหาได้ภายในเวลาไม่กี่ชั่วโมงเท่านั้น เนื่องจากการแก้ปัญหาใหญ่ ๆ จะต้องใช้หน่วยความจำสูง ดังนั้น ซูเปอร์คอมพิวเตอร์จึงมีหน่วยความจำที่ใหญ่มาก ซูเปอร์คอมพิวเตอร์มีหลายประเภท ตั้งแต่รุ่นที่มีหน่วยประมวลผล (processing unit) 1 หน่วย จนถึงรุ่นที่มีหน่วยประมวลผลหลายหมื่นหน่วยซึ่งสามารถทำงานหลายอย่างได้พร้อม ๆ กัน\n\nเมนเฟรมคอมพิวเตอร์ มีสมรรถภาพที่ต่ำกว่าซูเปอร์คอมพิวเตอร์มาก แต่ยังมีความเร็วสูง และมีประสิทธิภาพสูงกว่ามินิคอมพิวเตอร์หรือไมโครคอมพิวเตอร์ เมนเฟรมคอมพิวเตอร์สามารถให้บริการผู้ใช้จำนวนหลายร้อยคนพร้อม ๆ กัน ฉะนั้น จึงสามารถใช้โปรแกรมจำนวนนับร้อยแบบในเวลาเดียวกันได้ โดยเฉพาะถ้าต่อเครื่องเข้าเครือข่ายคอมพิวเตอร์ ผู้ใช้สามารถใช้ได้จากทั่วโลก ปัจจุบัน องค์กรใหญ่ ๆ เช่น ธนาคาร จะใช้คอมพิวเตอร์ประเภทนี้ในการทำบัญชีลูกค้า หรือการให้บริการจากเครื่องฝากและถอนเงินแบบอัตโนมัติ (automatic teller machine) เนื่องจากเครื่องเมนเฟรมคอมพิวเตอร์ได้ถูกใช้งานมากในการบริการผู้ใช้พร้อม ๆ กัน เมนเฟรมคอมพิวเตอร์จึงต้องมีหน่วยความจำที่ใหญ่มาก\n\nมินิคอมพิวเตอร์ คือ เมนเฟรมคอมพิวเตอร์ขนาดเล็ก ๆ ซึ่งสามารถบริการผู้ใช้งานได้หลายคนพร้อม ๆ กัน แต่จะไม่มีสมรรถภาพเพียงพอที่จะบริการผู้ใช้ในจำนวนที่เทียบเท่าเมนเฟรมคอมพิวเตอร์ได้ จึงทำให้มินิคอมพิวเตอร์เหมาะสำหรับองค์กรขนาดกลาง หรือสำหรับแผนกหนึ่งหรือสาขาหนึ่งขององค์กรขนาดใหญ่เท่านั้น\n\nไมโครคอมพิวเตอร์ คือ คอมพิวเตอร์ขนาดเล็กแบบขนาดตั้งโต๊ะ (desktop computer) หรือขนาดเล็กกว่านั้น เช่น ขนาดสมุดบันทึก (notebook computer) และขนาดฝ่ามือ (palmtop computer) ไมโครคอมพิวเตอร์ได้เริ่มมีขึ้นในปีพ.ศ. 2518 ถึงแม้ว่าในระยะหลัง เครื่องชนิดนี้จะมีประสิทธิภาพที่สูง แต่เนื่องจากมีราคาไม่แพงและมีขนาดกะทัดรัด ไมโครคอมพิวเตอร์จึงยังเหมาะสำหรับใช้ส่วนตัว ไมโครคอมพิวเตอร์ได้ถูกออกแบบสำหรับใช้ที่บ้าน โรงเรียน และสำนักงานสำหรับที่บ้าน เราสามารถใช้ไมโครคอมพิวเตอร์ในการทำงบประมาณรายรับรายจ่ายของครอบครัวช่วยทำการบ้านของลูก ๆ การค้นคว้าข้อมูลและข่าวสาร การส่งจดหมายอิเล็กทรอนิกส์ (electronic mail หรือ email) หรือโทรศัพท์ทางอินเทอร์เน็ต (internet phone) ในการติดต่อทั้งในและนอกประเทศ หรือแม้กระทั่งทางบันเทิง เช่น การเล่นเกมบนเครื่องไมโครคอมพิวเตอร์ สำหรับที่โรงเรียน เราสามารถใช้ไมโครคอมพิวเตอร์ในการช่วยสอนนักเรียนในการค้นคว้าข้อมูลจากทั่วโลกสำหรับที่สำนักงาน เราสามารถใช้ไมโครคอมพิวเตอร์ในการช่วยพิมพ์จดหมายและข้อมูลอื่น ๆ เก็บและค้นข้อมูล วิเคราะห์และทำนายยอดซื้อขายล่วงหน้า\n\nโน้ตบุ๊ค คือ คอมพิวเตอร์ที่มีขนาดเล็กกว่าไมโครคอมพิวเตอร์ ถูกออกแบบไว้เพื่อนำติดตัวไปใช้ตามที่ต่าง ๆ มีขนาดเล็ก และน้ำหนักเบา ในปัจจุบันมีขนาดพอ ๆ กับสมุดที่ทำด้วยกระดาษ\n\nเน็ตบุ๊ค คือ คอมพิวเตอร์ที่มีขนาดเล็กกว่าคอมพิวเตอร์โน๊ตบุ๊คและเล็กกว่าโน้ตบุ๊ค ไม่มีไดรฟ์สำหรับอ่านและเขียนแผ่น และใช้ฮาร์ดดิสแบบ SSD ทำให้น้ำหนักเบา ถูกออกแบบไว้เพื่อนำติดตัวไปใช้ตามที่ต่าง ๆ มีขนาดเล็ก และน้ำหนักเบา ปัจจุบันไม่ได้รับความนิยม\n\nอัลตร้าบุ๊ค คือ คอมพิวเตอร์ที่มีขนาดเล็กกว่าไมโครคอมพิวเตอร์และมีขนาดเท่ากับโน้ตบุ๊ค ถูกออกแบบไว้เพื่อนำติดตัวไปใช้ตามที่ต่าง ๆ และน้ำหนักเบากว่าโน้ตบุ๊ค และเน้นความสวยงาม ทันสมัย แปลกใหม่\n\nแท็บเล็ต คอมพิวเตอร์ หรือเรียกสั้น ๆ ว่า แท็บเล็ต คือเครื่องคอมพิวเตอร์ที่สามารถใช้ในขณะเคลื่อนที่ได้ ขนาดกลางและใช้หน้าจอสัมผัสในการทำงานเป็นอันดับแรก มีคีย์บอร์ดเสมือนจริงหรือปากกาดิจิทัลในการใช้งานแทนที่แป้นพิมพ์คีย์บอร์ด และมีความหมายครอบคลุมถึงโน๊คบุ๊คแบบ convertible ที่มีหน้าจอแบบสัมผัสและมีแป้นพิมพ์คีย์บอร์ดติดมาด้วยไม่ว่าจะเป็นแบบหมุนหรือแบบสไลด์ก็ตาม [21]\n\nคอมพิวเตอร์มีประโยชน์กับเรามากมาย เช่น\n\nบทความนี้ยังเป็นโครง คุณสามารถช่วยวิกิพีเดียได้โดยการเพิ่มเติมข้อมูล\n\nหมายเหตุ: ขอแนะนำให้จัดหมวดหมู่โครงให้เข้ากับเนื้อหาของบทความ (ดูเพิ่มที่ วิกิพีเดีย:โครงการจัดหมวดหมู่โครงที่ยังไม่สมบูรณ์)",
		"url": "https://th.wikipedia.org/wiki/คอมพิวเตอร์"
	},
	"tr": {
		"content": "Unicode (Evrensel Kod) Unicode Consortium organizasyonu tarafından geliştirilen ve her karaktere bir sayı değeri karşılığı atayan bir endüstri standardıdır. Sistemin amacı farklı karakter kodlama sistemlerinin birbiriyle tutarlı çalışmasını ve dünyadaki tüm yazım sistemlerinden metinlerin bilgisayar ortamında tek bir standart altında temsil edilebilmesini sağlamaktır. Evrensel Karakter Kümesi (UCS) olarak bilinen ISO/IEC 10646 standardı ise, her iki organizasyonun işbirliği ile aynı sayısal karşılıkları taşımaktadır. Unicode, son sürümü itibarıyla 129 farklı modern ve tarihî yazım sistemine ait 120.000'den fazla karakteri ve emoji gibi çeşitli sembol kümelerini kapsamaktadır.\n\nStandardın içinde karakterler ve karakterlere atanmış sayı değerlerinin tablolaştırılmış hali, bu sayılarının kodlanmasıyla ilgili kurum tarafından önerilen standart kodlama sistemleri ve bunların yanı sıra eşdeğer karakterler, karakterin bileşenlerine ayrılış bilgileri, sıralama kuralı, büyük-küçük harf bilgisi, yazılış yönü bilgisi gibi karakterin ekranda doğru gösterilebilmesi için yazılımların ihtiyaç duyduğu ek bilgiler bulunmaktadır.[1] Haziran 2015 tarihi itibarıyla standardın en son sürümü olan Unicode 8.0 ile birlikte 7.716 yeni karakter eklemesi yapılmıştır[2]\n\nUnicode kodlarından oluşan karakter dizilerini (metinleri) bilgisayarda verimli bir biçimde saklayabilmek amacıyla çeşitli karakter kodlamaları geliştirilmiştir. Bunlardan en bilinenleri UTF-8, UTF-16 ve artık kullanımdan kalkmış olan UCS-2'dir.\n\nHarflerin bilgisayar ortamında saklanması ve taşınması ile ilgili geliştirilen ilk sistem ASCII sistemi olmuştur. Bu sistemde Amerikan İngilizcesi alfabesinde bulunan harflerin her birine bir sayı atanmış ve harfler bilgisayar ortamında sayı olarak saklanmıştır. Harflerin kodlanması için 7 bitlik baytlar kullanılmıştır. 7 bit ile yazılabilen en büyük sayı 127'dir ve 128 adet sayı (0 ile 127 arası) temel noktalama işaretleri ve Amerikan İngilizcesi harflerini kodlamak için yeterli olmuştur. Örneğin a harfi ASCII kodlamasında on tabanında 97 sayısıyla eşleştirilmiştir. Harflerin eşleştirildiği bu sayılar daha sonra Unicode tarafından kod noktası olarak adlandırılacaktır.\n\nASCII kodlaması kullanılmaya başladıktan sonra Latin alfabesi kullanan Avrupa ülkelerinin alfabelerindeki ü, ö veya ç gibi harflerin de kodlanması ihtiyacı ortaya çıktığından Genişletilmiş ASCII denilen yeni bir sistem kullanılmıştır. Bu sistemde harfleri kodlamak için kullanılan yedi bite bir bit daha eklenerek 8 bitlik baytlar kullanılmış ve dolayısıyla kodlanabilecek harf sayısı 27 = 128'den 28 = 256'ya çıkarılmıştır. İlk çıkarılan genişletilmiş ASCII kümesi olan ISO 8859-1 (Latin-1 olarak da bilinir), Latin alfabesi kullanan Batı Avrupa dillerinin büyük çoğunluğunun harflerini karşılamaktadır. Daha sonra farklı dillerin alfabelerinde bulunan o dile özgü karakterler için farklı genişletilmiş ASCII kümeleri çıkarılmıştır. Bu farklı kümelere kod sayfası da denmektedir.\n\nAncak genişletilmiş ASCII sistemi de sorunu kökten çözmeyi başaramamıştır. Genişletilmiş ASCII'nin eksiklikleri şu iki maddeyle özetlenebilir:\n\nBirbirinden farklı ASCII kümelerinde bulunan dillerin karakterlerinin aynı metinde bulunması mümkün değildir. Çünkü 128 ve sonrasındaki sayılar her genişletilmiş ASCII kümesinde farklı karakterlere karşılık gelebilmektedirler. Eğer belgedeki kodlamanın hangi kod sayfasına göre yapıldığı belirtilmemişse metnin doğru okunması mümkün olmamaktadır. Örneğin İnternet üzerinde kodlama türünün belirtilmemiş olması veya yanlış belirtilmiş olması nedeniyle Türkçe karakterlerden ı, ş karakterlerinin ý, þ olarak görünmesinin nedeni Türkçe için üretilmiş olan Latin-5 kod sayfasında bu karakterlere karşılık gelen sayıların, İnternet tarayıcılarının kodlama türü belirtilmediğinde varsayılan olarak kullandığı Latin-1 kodlamasında İzlanda alfabesinden ý, þ karakterlerine karşılık gelmesi yüzündendir. Dolayısıyla diğer dillerin alfabeleri için bir çözüm olarak öne sürülen genişletilmiş ASCII sistemi de tam çözüm olamamıştır.\n\nFarklı kod sayfaları arasında karışıklığa yol açması bir yana içinde binlerce farklı karakter barındıran Çince veya Japonca gibi dillerin harfleri için fazladan gelen 128 karakterlik kapasitenin de yeterli olması olanaksızdır. Bu yüzden ASCII sistemi yerini Unicode'a bırakmıştır. Ancak yine de yıllardan beri kullanılmış olması nedeniyle bu kodlamayla üretilmiş birçok yazılım, web sayfası bulunmaktadır. Unicode sistemi teşvik edilse de işletim sistemleri eski yazılım ve web sayfaları nedeniyle ASCII kodlamasını da Unicode ile birlikte hâlâ mecburen desteklemektedir.\n\nUnicode'un kökeni 1987'ye dek uzanmaktadır. Bu tarihte Xerox çalışanı Joe Becker ve Apple çalışanı Mark Davis evrensel bir karakter kümesi oluşturmanın sağlayacağı faydalar üzerinde çalışma yapmaktaydı.[3] Ağustos 1988 tarihinde, Joe Becker \"şimdilik Unicode olarak adlandırılan uluslararası/çokdilli metin karakteri kodlama sistemi\" için bir teklif taslağı hazırladı. Unicode ismini eşsiz, tek ve evrensel bir kodlama sistemini çağrıştırması amacıyla seçtiğini belirtmiştir.[4]\n\nUnicode 88 başlıklı belgede Becker 16-bitlik bir karakter modeli taslağı hazırlamıştır:[4]\n\nUnicode, güvenilir ve üzerinde çalışılabilir dünya geneli bir metin kodlaması ihtiyacına cevap verme niyetiyle ortaya çıkmıştır. Unicode kabaca, dünyadaki bütün yaşayan dillerin tüm karakterlerini kapsayabilmesi amacıyla uzunluğu 16 bite çıkarılmış \"geniş gövdeli bir ASCII\" olarak tanımlanabilir. Mühendisliği düzgünce yapılan bir tasarımla, karakter başına 16 bit bu amaç için fazlasıyla yeterli olacaktır.\n\nOrijinal 16-bitlik tasarım sadece günlük kullanıma dahil olan karakterlerin ve yazı türlerinin kodlanmasına ihtiyaç duyulacağını varsaymasından kaynaklanıyordu:[4]\n\nUnicode eskiden beri gelenleri korumaktansa geleceğe dönük yararlılığı garanti altına almayı yüksek öncelik edinmiştir. Unicode ilk olarak çağdaş metinlerde kullanılan karakterleri (yani 1988 yılında dünyada basılan gazete ve dergilerin kullandığı karakterlerin tümünü) hedeflemektedir ki bu karakterlerin sayısı şüphesiz 214 = 16.384'ü geçmez. Çağdaş kullanım dahilinde olan karakterlerin dışında kalan diğer karakterlerin eskimiş olduğu veya ender kullanıldığı kabul edilebilir. Çoğunlukla kullanışlı karakterleri barındıran Unicode'u bu eski ve nadir karakterlerle kalabalıklaştırmak yerine bu karakterleri özel kullanım kayıtlarına dahil etmek daha iyidir.\n\n1989 yılının başlarında Unicode çalışma grubu Metaphor'dan Ken Whistler ve Mike Kernaghan, Research Libraries Group'tan Karen Smith-Yoshimura ve Joan Aliprand ve Sun Microsystems'tan Glenn Wright da bulunacak şekilde genişletildi. 1990 yılında Microsoft'tan Michel Suignard ve Asmus Freytag ve NeXT'ten Rick McGowan da gruba katıldı. 1990'ın sonlarına doğru mevcut kodlama standartlarındaki karakterlerin eşlenmesi çalışmaları tamamlanmış ve Unicode'un gözden geçirilecek son taslağı hazırlanmıştı.\n\nUnicode Consortium 3 Ocak 1991'de California'da kuruldu. Ekim 1991'de Unicode standardının ilk cildi yayımlandı. Han ideograflarını içeren ikinci cildi de Haziran 1992'de yayımlandı.\n\n1996'da Unicode 2.0'da yedek karakter mekanizması uygulanmaya başladı. Bu sayede Unicode artık 16 bit ile sınırlanmamış oldu. Yedek karakter çiftlerinin kullanılmasıyla Unicode kod alanı bir milyon kod noktasını bulundurarak kadar genişledi. Böylece Unicode, Mısır hiyeroglifleri veya Göktürk alfabesi gibi birçok tarihi yazı sistemlerini ve kodlamasına ihtiyaç duyulmayacağı tahmin edilen, artık kullanılmayan eskimiş karakterleri destekler hale geldi. Unicode tarafından desteklenmesi ilk başta planlanmamış olan karakterler arasında nadiren kullanılan Kanji karakterleri ve Çince karakterler de bulunmaktadır. Bu karakterler artık kullanılmayan kişi veya yer isimlerinin parçası olduklarından nadiren kullanılıyordu ancak orijinal Unicode mimarisinde düşünülenden çok daha önemli karakterlerdir.[5]\n\nUnicode, 016 ile 10FFFF16 arasındaki sayılara karşılık gelen 1.114.112 adet kod noktasından oluşan bir kod alanı tanımlamıştır.[6] Kod noktası Unicode tarafından her bir karaktere atanan sayıdır ve bu sayı genelde on altı tabanında, hekzadesimal gösterim adı verilen, 10-15 arası rakamların A-F arası harflerle temsil edildiği bir gösterim yöntemiyle yazılır. Normalde Unicode kod noktaları \"U+\" ve sonrasında kod noktasının on altı tabanındaki karşılığı ile ifade edilir. Kod noktalarının beşinci ve altıncı haneleri birlikte düzlem (plane) numarasını ifade etmektedir, toplamda on yedi adet düzlem bulunmaktadır (016'dan 1016'a kadar). İlk düzlem olan Temel Çokdilli Düzlem (Basic Multilingual Plane, kısaca BMP) sıfırıncı düzlem olduğundan bu düzlemdeki karakterlerin kod noktaları yazılırken beşinci hanedeki 0 yazılmadan dört haneli şekilde yazılırlar (mesela Unicode tanımı latin capital letter x olan X harfinin kod noktası U+000058 yerine kısaca U+0058 olarak ifade edilir), BMP dışındaki düzlemlerde bulunan kod noktaları başında düzlem numarası yazılarak ifade edilir. Dolayısıyla toplamda beş veya altı haneli olarak yazılırlar. (örneğin language tag adlı karakterinin kod noktası U+E0001, Emoji karakterlerinden grinning face adlı 😀 karakterinin kod noktası U+1F600 olarak ifade edilmektedir). Unicode standardının eski sürümlerinde de benzer yazım şekilleri kullanılıyordu ancak ufak farklılıklar bulunmaktaydı. Örneğin Unicode 3.0 standardında kod noktaları yazılırken sekiz haneli kod noktasının önüne \"U-\" getirilirdi. Kod birimleri yazılırken önüne \"U+\" konularak yazılırdı.[7]\n\nUnicode kod alanı 0'dan 16'ya kadar numaralanmış on yedi adet düzleme (plane) ayrılmıştır:\n\n0000–​0FFF 1000–​1FFF 2000–​2FFF 3000–​3FFF 4000–​4FFF 5000–​5FFF 6000–​6FFF 7000–​7FFF\n\n8000–​8FFF 9000–​9FFF A000–​AFFF B000–​BFFF C000–​CFFF D000–​DFFF E000–​EFFF F000–​FFFF\n\n10000–​10FFF 11000–​11FFF 12000–​12FFF 13000–​13FFF 16000–​16FFF\n\n1B000–​1BFFF 1D000–​1DFFF 1E000–​1EFFF 1F000–​1FFFF\n\n20000–​20FFF 21000–​21FFF 22000–​22FFF 23000–​23FFF 24000–​24FFF 25000–​25FFF 26000–​26FFF 27000–​27FFF\n\n28000–​28FFF 29000–​29FFF 2A000–​2AFFF 2B000–​2BFFF 2F000–​2FFFF\n\nE0000–​E0FFF\n\n15: PUA-A F0000–​FFFFF 16: PUA-B100000–​10FFFF\n\nBMP düzleminde bulunan tüm kod noktaları UTF-16 kodlamasında tek kod birimiyle ve UTF-8 kodlamasında da değişken uzunlukta kod birimleriyle (bir, iki veya üç) kodlanabilir. UTF-16 kodlamasında kod birimleri 16-bit (iki bayta karşılık gelir), UTF-8'de 8-bit (bir bayta karşılık gelir) uzunluğundadır. 1'den 16'ya kadar olan düzlemlerdeki kod noktaları (tamamlayıcı düzlemler, supplementary plane) UTF-8 kodlamasında dört kod birimi ile ve UTF-16'da yedek çiftler veya yer tutucu çiftler (surrogate pairs) denilen bir sistem ile iki kod birimi halinde kodlanırlar.\n\nHer düzlem, blok adı verilen bölümlere ayrılmıştır ve her blokta o blokla ilgili karakterler bulunur. Blokların kapsadığı kod noktalarının sayısı (yani blokların büyüklüğü) değişken olmakla birlikte bu sayı her zaman 16'nın ve genelde de 128'in katıdır. Aynı yazı türünde bulunan karakterler farklı bloklara dağılmış olabilir.\n\nHer kod noktasının Genel Kategori adlı bir niteliği bulunmaktadır. Temel kategoriler şunlardır: Harf (Letter), İm (Mark), Sayı (Number), Noktalama (Punctuation), Sembol (Symbol), Ayırıcı (Seperator) ve Diğer (Other). Bu kategorilerin alt bölümleri de bulunmaktadır. Genel Kategori niteliği her zaman kullanışlı değildir; çünkü eski kodlama sistemleri tek kod noktasına birden fazla özellik yüklemiştir. Örneğin U+000A <control-000A> LINE FEED (SATIR ATLATMA) karakteri ASCII sisteminde hem kontrol hem de düzen ayırıcısı karakteri olarak kullanılmıştır, Unicode'da bu karakterin Genel Kategori niteliği \"Diğer, Kontrol\" olarak tanımlanmıştır. Bir kod noktasının özelliklerinin ve davranışının tam olarak belirlenebilmesi için diğer niteliklerin de dikkate alınması gerekmektedir. Mevcut Genel Kategoriler şunlardır:\n\nU+D800..U+DBFF aralığında bulunan 1.024 kod noktası üst yedek kod noktaları olarak ve U+DC00..U+DFFF aralığında 1.024 kod noktası da alt yedek kod noktaları olarak bilinir. Üst yedek kod noktası (veya öndeki kod noktası) ve sonrasında gelen alt kod noktasının (veya arkadaki kod noktası) oluşturduğu çifte yedek çifti denir bu yedek çiftleri UTF-16 kodlaması tarafından BMP'nin dışında kalan kod noktalarını kodlamak için kullanılır. UTF-16 kodlaması kod noktalarını iki bayttan oluşan birimler halinde kodlamaktadır, iki bayt ile kodlanabilecek en büyük sayı FFFF16 (6553510) olduğundan U+FFFF'nin sonrasındaki kod noktalarını ifade etmek için U+D800'den U+DFFF'ye kadar olan kod noktaları Unicode tarafından yedek kod noktaları olarak kullanılmak üzere ayrılmıştır. Bu kod noktaları çiftler halinde kullanılarak BMP dışında kalan 1.048.560 karakteri UTF-16 ile kodlamaya fazlasıyla yetmektedir (1.024 * 1.024 = 1.048.576 adet yedek çifti kombinasyonu vardır). Üst ve alt yedek kod noktaları tek başlarına anlamsız ve geçersizdir. Çiftler halinde kullanıldıkları zaman bir karakteri temsil ederler. Dolayısıyla, yedek çifti olarak kullanılmak üzere ayrılmış olan U+D800..U+DFFF aralığındaki kod noktaları U+10000..U+10FFFF aralığındaki kod noktalarının yerini tutmak amacıyla kullanıldığından, karakter olarak kullanılabilecek kod noktaları yedek kod noktaları dışında kalan kod noktalarıdır (U+0000..U+D7FF ve U+E000..U+10FFFF aralığında kalan 1.112.064 kod noktası). Karakter olarak kullanılabilecek kod noktalarına aynı zamanda karakterin skaler değeri de denmektedir. Örneğin UTF-16'da yedek çiftleri kullanılarak (D803 DC00)16 olarak kodlanan U+10C00 𐰀 OLD TURKIC LETTER ORKHON A karakterinin skaler değeri U+10C00'dır denir. Çünkü (D803 DC00)16 yedek çifti 10C0016 sayısını temsil eder.\n\nUnicode bazı kod noktalarına ne şimdi ne de gelecekte herhangi bir karakter atamayacağını duyurmuştur. Karakter dışı kod noktaları Unicode şartnamesinde noncharacter olarak adlandırılmaktadır. Karakter dışı kod noktaları yazılımların iç yapısında herhangi bir amaçla kullanılabilir. Toplamda 66 adet karakter dışı kod noktası bulunmaktadır. U+FDD0..U+FDEF aralığındaki kod noktaları ve her düzlemin son iki kod noktası (yani U+FFFE, U+FFFF, U+1FFFE, U+1FFFF, ... U+10FFFE, U+10FFFF). Karakter dışı kod noktaları sabittir, ileride yeni karakter dışı kod noktası tanımlanmayacaktır.[8]\n\nAyırtılmış (reserved) kod noktaları da ileride kodlama karakteri olarak kullanılacak olan ama henüz Unicode tarafından kendilerine karakter atanmamış kod noktalarıdır.\n\nÖzel kullanım kod noktaları da karakter olarak kullanılabileceği belirtilen ancak hangi karakteri temsilen kullanılacakları Unicode tarafından belirtilmeyip kullanıcıya bırakılmış olan karakterlerdir[9]. Yani bu karakterler yazılı bilgi alışverişi esnasında taraflar arasında önceden yapılmış bir anlaşamaya göre yorumlanabilecek karakterlerdir. Unicode kod alanında üç adet özel kullanım bölgesi vardır:\n\nÇizgesel karakterler (graphic characters), Unicode tarafından tanımlanan, belirli bir anlamsal değeri olan ve görünür bir glif şekline sahip veya görünür bir boşluğu temsil eden karakterlerdir. Unicode 7.0 itibarıyla 112.804 çizgesel karakter vardır.\n\nBiçimlendirme karakterleri ise görünür bir şekilleri olmayan ancak komşu karakterlerin görünüşünü veya davranışını etkileyen karakterlerdir. Örneğin U+200C ZERO WIDTH NON-JOINER karakteri ve U+200D ZERO WIDTH JOINER karakteri bitişebilen karakterlerin bitişme durumunu belirlemek için kullanılabilir. Unicode 7.0'da 152 biçimlendirme karakteri bulunmaktadır. Başka bir örnek vermek gerekirse metin dosyalarında bilgisayarlarda ↵ Enter tuşuna basarak yapılan alt satıra geçme işlemi sırasında metne U+000A <control-000A> ({{{no}}}) ve U+000D <control-000D> ({{{no}}}) karakterleri işlenir, bu karakterler metin üzerinde görünür bir yer kaplamadığı halde kendilerinden sonra gelen karakterlerin alt satırda görüntülenmesini sağladıkları için kontrol işlevi görür ve çizgesel karakter sınıfına girmez.\n\nAltmış altı kod noktası (U+0000..U+001F ve U+007F.. U+009F aralıkları) kontrol kodları olarak ayrılmıştır ve ISO/IEC 6429 standardıyla belirlenen C0 ve C1 kontrol kodlarına karşılık gelir. Unicode'la uyumlu kodlamalarda bu kontrol karakterlerinden U+0009 <control-0009> ({{{no}}}), U+000A <control-000A> ({{{no}}}) ve U+000D <control-000D> ({{{no}}}) karakterleri sıklıkla kullanılır.\n\nÇizgesel karakterler, biçimlendirme karakterleri ve kontrol kodu karakterleri hep beraber atanmış karakterler olarak bilinir.\n\nSoyut karakter, metinsel bir verinin düzenlenmesi, denetlenmesi veya temsil edilmesi için kullanılan bilgi birimi olarak tanımlanmıştır. Soyut karakterin bir kod noktasıyla eşleştirilmiş haline kodlu karakter denir ve kodlu karakterler genelde kısaca karakter olarak anılır. Soyut karakterler, karakterlerin temsil ettikleri bilginin soyut, yani şekil almamış halini temsil eder. Unicode standardında tek bir kod noktasıyla ifade edilebilen bir soyut karakter aynı zamanda birden fazla kod noktasının yan yana kullanılmasıyla da alternatif olarak temsil edilebilir.[10] Örneğin â harfi, Unicode tanımı U+00E2 â LATIN SMALL LETTER A WITH CIRCUMFLEX kod noktasıyla ifade edilebileceği gibi U+0061 a LATIN SMALL LETTER A ve U+0302 ̂ COMBINING CIRCUMFLEX ACCENT kod noktalarının yan yana kullanılmasıyla da ifade edilebilir. İki farklı şekilde de aynı görüntü ortaya çıkacaktır: U+00E2 â ve U+0061 U+0302 â. Bu karakterler kodlamaları itibarıyla farklı olmalarına rağmen aynı soyut karakteri temsil etmektedir. Birisi tek bir kod noktasından oluşurken diğeri harfin parçalarına karşılık gelen kod noktalarının birleştirilmesiyle ifade edilir. Ancak bazı soyut karakterlerin Unicode standardında tek kod noktasıyla karşılığı yoktur. Bu yüzden bu soyut karakterler birden fazla kod noktasının art arda kullanılmasıyla ifade edilebilirler. Unicode yalnızca iki kod noktasının kombinasyonuyla ifade edilebilen soyut karakterlerin bir listesini sitesinde barındırmaktadır.[11]\n\nAyrıca, diğer standartlarla uyumluluk sağlamak için bazen aynı soyut karakter birden çok tekil kodlanmış karaktere karşılık gelebilir. Bu durum aynı karakterin birden fazla kod noktasının birleşimi şeklinde gösterilmesinden farklıdır. Mesela Å karakteri Unicode standardında U+00C5 Å LATIN CAPITAL LETTER A WITH RING ABOVE kod noktasıyla temsil edilmektedir. Ancak aynı soyut karakteri temsil eden U+212B Å ANGSTROM SIGN adlı bir kod noktası da bulunmaktadır.\n\nTüm çizgesel karakterler, biçimlendirme karakterleri ve özel kullanım karakterlerinin eşsiz ve değişmez bir tanımlayıcı adı vardır. İsimlerin değişmezliği Unicode sürüm 2.0'dan itibaren İsim Kararlılığı Politikası ile garanti altına alınmıştır [8]. Tanımlayıcı adın ciddi derecede sorunlu ve yanıltıcı olduğu veya önemli yazım hataları içerdiği durumlarda resmi bir ikincil ad tanımlanabilir ve uygulamaların resmi karakter adı yerine resmi ikincil adı kullanmaları tavsiye edilir. Örneğin Yi yazısına ait karakterlerden yanlış adlandırılan U+A015 ꀕ YI SYLLABLE WU karakterine daha sonra yi syllable iteration mark (yi hece yineleme işareti) şeklinde ikincil bir ad tanımlanmıştır. U+FE18 ︘ PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRAKCET ({{{no}}}) karakteri daha sonra ikincil ad olarak presentation form for vertical right white lenticular bracket adını almıştır.[12]\n\nEskiden karakter kodlaması kavramı hem her karaktere birer sayı atama hem de bu sayıları doğrudan iki tabanına dönüştürüp bilgisayarda kullanılabilir hale getirme işlemine verilen isimdi. Bu yüzden Unicode'dan önce kullanılan ASCII ve tüm genişletilmiş ASCII sistemleri birer karakter kodlamasıdır. Unicode'dan sonra ise karakter kodlaması kavramı değişmiştir. Unicode, karakterlere sayı atanmasıyla bu sayıların iki tabanına dönüştürülmesi işlemini birbirinden ayırmıştır. Çünkü Unicode ile beraber karakterlere atanan sayıları (kod noktalarını) doğrudan iki tabanına dönüştürmektense farklı yöntemler kullanarak dönüştürme ihtiyacı ortaya çıkmıştır. Dolayısıyla Unicode'dan sonra karakter kodlaması kavramı kod noktalarını iki tabanında temsil etme yöntemini karşılar olmuştur.\n\nTanımda meydana gelen bu değişikliğin nedeni, Unicode'un genişletilmiş ASCII kümelerinin karakterlere atamak için kullandığı [0, 255] olan sayı aralığını [0, 1.114.111]'e çıkarmasında yatmaktadır. Unicode'dan önceki sistemlerde karakterlere atanan sayılar 255'i aşmadığı için, bu sayılar doğrudan iki tabanına çevrilip karakterleri temsil etmek için kullanılabiliyordu. Örneğin ASCII sistemlerinde a harfi 9710 sayısıyla eşleştirilmişti ve herhangi bir metin belgesinde a harfi kodlanacağı zaman 9710 iki tabanına dönüştürülüp 11000012 şeklinde 8-bitlik bir bayt olarak depolanıyordu. Sayılar 255'i aşmadığı için her karakter 1 bayta sığmaktaydı. Ancak Unicode sisteminde 255'ten büyük sayılar iki tabanında 8 biti aşmaktadır ve eğer 1.114.111 sayının hepsi eşit uzunlukta olacak şekilde doğrudan iki tabanına dönüştürülüp kullanılmak istenirse her karakterin 32 bit uzunluğunda olması, iki tabanına dönüştürüldüğünde 32 bitten kısa olan sayıların da başına sıfır eklenerek 32 bite tamamlanması gerekir. Çünkü her karakterin uzunluğu sabit olmazsa kodlamada bir karakterin nerede bitip diğerinin nerede başladığı anlaşılamaz. Bu durum genişletilmiş ASCII'ye göre kodlanmış bir metne göre Unicode ile kodlanmış bir metnin dört kat kadar fazla yer kaplamasına neden olacaktır.\n\nHer karakterin eşit uzunlukta bitlerle temsil edilmesi zorunluluğundan kurtulmak istenirse karakterlerin kod noktalarını iki tabanına dönüştürürken araya karakterin başlangıcını veya sonunu belli edecek işaretler eklenerek kodlama yapılması gerekecektir. Bu işlem için geliştirilen birkaç farklı kodlama sistemi bulunmaktadır. Bunlara örnek olarak UTF-8, UTF-16 ve UTF-32 gösterilebilir. Bunların her biri, karakterlerin kod noktalarını (yani Unicode tarafından kendilerine atanan sayıları) bilgisayar ortamında saklamak ve üzerinde işlem yapmak amacıyla iki tabanına dönüştürürken farklı yöntemler kullanırlar. Her birinin farklı avantajları ve dezavantajları mevcuttur. Günümüzde en yaygın kullanılan kodlama UTF-8 kodlamasıdır.",
		"url": "https://tr.wikipedia.org/wiki/Unicode"
	},
	"vi": {
		"content": "Unicode (hay gọi là mã thống nhất; mã đơn nhất) là bộ mã chuẩn quốc tế được thiết kế để dùng làm bộ mã duy nhất cho tất cả các ngôn ngữ khác nhau trên thế giới, kể cả các ngôn ngữ sử dụng ký tự tượng hình phức tạp như chữ Hán của tiếng Trung Quốc, tiếng Nhật, chữ Nôm của tiếng Việt, v.v. Vì những điểm ưu việt đó, Unicode đã và đang từng bước thay thế các bộ mã truyền thống, kể cả bộ mã tiêu chuẩn ISO 8859. Unicode đang được sử dụng trên rất nhiều phần mềm cũng như các trình ứng dụng, chẳng hạn Windows.\n\nPhiên bản mới nhất là Unicode® 15.0 công bố ngày 13 tháng 9 năm 2022.\n\nHiệp hội Unicode ở California xuất bản phiên bản đầu tiên của The Unicode Standard (Tiêu chuẩn Unicode) vào năm 1991, và vẫn liên tục hoàn thiện chuẩn. Các phiên bản mới được viết dựa trên các phiên bản đã có, nhờ vậy đảm bảo được tính tương thích. Cũng xin lưu ý rằng Unicode và tiêu chuẩn ISO 10646 là hai khái niệm hoàn toàn độc lập. Khi nói đến ISO 10646 tức là người ta đang nói đến tiêu chuẩn quốc tế chính thức, còn Unicode thì được Unicode Consortium (tập hợp đại diện các công ty tin học lớn) soạn ra. Kể từ năm 1991, khi Nhóm làm việc ISO và Liên đoàn Unicode quyết định hợp tác chặt chẽ với nhau trong quá trình nâng cấp và mở rộng chuẩn để đảm bảo tính tương thích (cụ thể là vị trí của các ký tự trên cả hai đều y hệt nhau – chẳng hạn chữ ơ là 01A1). Còn với Unicode thì lại khác, chuẩn này được phát triển bởi Liên đoàn Unicode. Liên đoàn Unicode là một tổ chức phi lợi nhuận tập hợp bởi một số công ty, trong đó có cả những công ty đa quốc gia khổng lồ có ảnh hưởng lớn như Microsoft, Adobe Systems, IBM, Novell, Sun Microsystems, Lotus Software, Symantec và Unisys. (Danh sách đầy đủ tại: [1]). Tuy nhiên, chuẩn Unicode không chỉ quy định bộ mã, mà còn cả cách dựng hình, cách mã hóa (sử dụng 1, 2, 3 hay 4 byte để biểu diễn một ký tự (UTF-8 là một ví dụ), sự tương quan (collation) giữa các ký tự, và nhiều đặc tính khác của các ký tự, hỗ trợ cả những ngôn ngữ từ phải sang trái như tiếng Ả Rập chẳng hạn.\n\nUnicode chiếm trước 1.114.112 (= 220+216) mã chữ, và hiện nay đã gán ký hiệu cho hơn 96000 mã chữ. 256 mã đầu tiên phù hợp với ISO 8859-1, là cách mã hóa ký tự phổ biến nhất trong \"thế giới phương Tây\"; do đó, 128 ký tự đầu tiên còn được định danh theo ASCII.\n\nKhông gian mã Unicode cho các ký tự được chia thành 17 mặt phẳng (plane) và mỗi mặt phẳng có 65536 code point. Mặt phẳng đầu tiên (plane 0), \"Mặt phẳng đa ngôn ngữ căn bản\" (Basic Multilingual Plane - BMP), là nơi mà đa số các ký hiệu được gán mã. BMP chứa các ký hiệu cho hầu hết các ngôn ngữ hiện đại, và một số lượng lớn các ký tự đặc biệt. Đa số các code point được phân bố trong BMP được dùng để mã hóa các ngôn ngữ CJKV (Hán-Nhật-Hàn-Việt).\n\nHai mặt phẳng tiếp theo được dùng cho các ký tự \"đồ họa\". Mặt phẳng 1, \"Mặt phẳng đa ngôn ngữ bổ sung\" (Supplementary Multilingual Plane - SMP), được dùng chủ yếu cho các loại chữ viết cổ, ví dụ Egyptian hieroglyph (chưa được mã hóa), nhưng cũng còn được dùng cho các ký hiệu âm nhạc. Mặt phẳng 2, (Supplementary Ideographic Plane - SIP), được dùng cho khoảng 40000 chữ Trung Quốc ít gặp mà đa số là các ký hiệu cổ, ngoài ra cũng có một số ký hiệu hiện đại. Mặt phẳng 14 hiện chứa một số các ký tự thẻ ngôn ngữ không được khuyến khích và một số ký hiệu lựa chọn biến thể. Mặt phẳng 15 và Mặt phẳng 16 được mở cho các sử dụng cá nhân.\n\nVẫn còn nhiều tranh luận giữa các chuyên gia về ngôn ngữ CJK (Hoa-Nhật-Hàn), đặc biệt là các chuyên gia người Nhật, về nhu cầu và lợi ích kỹ thuật của việc \"thống nhất chữ Hoa\", tức là việc chuyển những bộ chữ Hoa và chữ Nhật vào trong một bộ chữ hợp nhất. (Xem thêm mã hóa chữ Hoa)\n\nKho ≈220 điểm mã bảo đảm sự tương thích với bộ mã UTF-16. Việc mới chỉ dùng hết có 10% kho chữ cho thấy rằng kho chữ cỡ ≈20 bit này khó bị đầy trong một tương lai gần.\n\nĐọc từ đầu tới giờ, chúng ta chỉ mới biết rằng Unicode là một cách để đánh số duy nhất cho tất cả các ký tự được dùng bởi con người trong ngôn ngữ viết. Nhưng những con số đó được ghi trong các hệ thống xử lý văn bản lại là những vấn đề khác; những vấn đề đó là hậu quả của việc phần lớn các phần mềm ở phương Tây chỉ biết tới các hệ thống mã hóa 8-bit, và việc đưa Unicode vào các phần mềm chỉ mới diễn ra chậm chạp trong những năm gần đây.\n\nCác chương trình 8-bit cũ chỉ nhận biết các ký tự 8 bit, và không thể dùng nhiều hơn 256 điểm mã nếu không có những cách giải quyết đặc biệt. Do đó người ta phải đề ra nhiều cơ chế để dùng Unicode; tùy thuộc vào khả năng lưu trữ, sự tương thích với chương trình nguồn và sự tương tác với các hệ thống khác mà mỗi người chọn một cơ chế.\n\nCách đơn giản nhất để lưu trữ tất cả các 220+216 Unicode code points là sử dụng 32 bit cho mỗi ký tự, nghĩa là, 4 byte – do đó, cách mã hóa này được Unicode gọi là UTF-32 và ISO/IEC 10646 gọi là UCS-4. Vấn đề chính của cách này là nó hao chỗ hơn 4 lần so với trước kia, do đó nó ít được dùng trong các vật nhớ ngoài (như đĩa, băng). Tuy nhiên, nó rất đơn giản, nên một số chương trình sẽ sử dụng mã hóa 32 bit bên trong khi xử lý Unicode.\n\nUTF-16 là một cách mã hóa dùng Unicode 20 bit. Các ký tự trong BMP được diễn tả bằng cách dùng giá trị 16-bit của code point trong Unicode CCS. Có hai cách để viết giá trị 16 bit trong một dòng (stream) 8-bit. Có lẽ bạn đã nghe qua chữ endian. Big Endian có nghĩa là cho Most Significant Byte đi trước, tức là nằm bên trái – do đó ta có UTF-16BE. Còn Little Endian thì ngược lại, tức là Least Significant Byte đi trước – do đó ta có UTF-16LE. Thí dụ, giá trị 16-bit của con số Hex1234 được viết là Hex12 Hex34 trong Big Endian và Hex34 Hex12 trong Little Endian.\n\nNhững ký hiệu không nằm trong BMP được biểu diễn bằng cách dùng surrogate pair (cặp thay thế). Code points có giá trị từ U+D800 đến U+DFFF được dành riêng ra để dùng cho mục đích này. Trước hết, một code point có 20 bit được phân ra làm hai nhóm 10 bit. Nhóm Most Significant 10 bit được map vào một giá trị 10 bit nằm trong khoảng từ u+D800 đến u+DBFF. Nhóm Least Significant 10 bit được map vào một giá trị 10 bit nằm trong khoảng từ U+DC00 đến U+DFFF. Theo cách đó UTF-16 có thể biểu diễn được những ký hiệu Unicode có 20 bit.\n\nUTF-8 là một cách mã hóa để có tác dụng giống như UCS-4 (cũng là UTF-16), chứ không phải có code point nào khác. UTF-8 được thiết kế để tương thích với chuẩn ASCII. UTF-8 có thể sử dụng từ một (cho những ký tự trong ASCII) cho đến 6 byte để biểu diễn một ký tự.\n\nChính vì tương thích với ASCII, UTF-8 cực kỳ có lợi thế khi được sử dụng để bổ sung hỗ trợ Unicode cho các phần mềm có sẵn. Thêm vào đó, các nhà phát triển phần mềm vẫn có thể sử dụng các hàm thư viện có sẵn của ngôn ngữ lập trình C để so sánh (comparisons) và xếp thứ tự. (Ngược lại, để hỗ trợ các cách mã hóa 16 bit hay 32 bit như ở trên, một số lớn phần mềm buộc phải viết lại do đó tốn rất nhiều công sức. Một điểm mạnh nữa của UTF-8 là với các văn bản chỉ có một số ít các ký tự ngoài ASCII, hay thậm chí cho các ngôn ngữ dùng bảng chữ cái Latinh như tiếng Việt, tiếng Pháp, tiếng Tây Ban Nha, v.v.; cách mã hóa kiểu này cực kỳ tiết kiệm không gian lưu trữ.\n\nUTF-8 được thiết kế đảm bảo không có chuỗi byte của ký tự nào lại nằm trong một chuỗi của ký tự khác dài hơn. Điều này khiến cho việc tìm kiếm ký tự theo byte trong một văn bản là rất dễ dàng. Một số dạng mã hóa khác (như Shift-JIS) không có tính chất này khiến cho việc xử lý chuỗi ký tự trở nên phức tạp hơn nhiều. Mặc dù để thực hiện điều này đòi hỏi phải có độ dư (văn bản sẽ dài thêm) nhưng những ưu điểm mà nó mang lại vẫn nhiều hơn. Việc nén dữ liệu không phải là mục đích hướng tới của Unicode và việc này cần được tiến hành một cách độc lập.\n\nCác quy định chính xác của UTF-8 như sau (các số bắt đầu bằng 0x là các số biểu diễn trong hệ thập lục phân)\n\nHiện nay, các giá trị khác ngoài các giá trị trên đều chưa được sử dụng. Tuy nhiên, các chuỗi ký tự dài tới 6 byte có thể được dùng trong tương lai.\n\nChuẩn hóa được ít dùng nhất có lẽ là UTF-7. Chuẩn MIME yêu cầu mọi thư điện tử phải được gửi dưới dạng ASCII cho nên các thư điện tử nào sử dụng mã hóa Unicode được coi là không hợp lệ. Tuy nhiên hạn chế này thường bị hầu hết mọi người bỏ qua. UTF-8 cho phép thư điện tử sử dụng Unicode và đồng thời cũng phù hợp với tiêu chuẩn. Các ký hiệu ASCII sẽ được giữ nguyên, tuy nhiên các ký tự khác ngoài 128 ký hiệu ASCII chuẩn sẽ được mã hóa bằng một escape sequence hay một dấu '+' theo sau một ký tự Unicode được mã hóa bằng Base64, và kết thúc bằng một dấu '-'. Ký tự '+' nổi tiếng sẽ được mã hóa thành '+-'.\n\nTiêu chuẩn Unicode còn bao gồm một số vấn đề có liên quan, chẳng hạn character properties, text normalisation forms và bidirectional display order (để hiển thị chính xác các văn bản chứa cả hai loại ngôn ngữ có cách viết từ phải qua trái như tiếng Ả Rập hay tiếng Hebrew) và trái qua phải.\n\nHầu hết các trang web tiếng Việt sử dụng cách mã hóa UTF-8 để đảm bảo tính tương thích, tuy nhiên một số trang web vẫn còn giữ cách mã hóa theo chuẩn ISO-8859-1 cũ. Các trình duyệt hiện đại ngày nay như Mozilla Firefox có chức năng tự động chọn cách mã hoá (encoding) thích hợp nếu như máy tính đã được cài đặt một font thích hợp (xem thêm Unicode và HTML).\n\nMặc dù các quy tắc cú pháp có thể ảnh hưởng tới thứ tự xuất hiện của các ký tự nhưng các văn bản HTML 4.0 và XML 1.0 đều có thể bao trùm hầu hết các ký tự trong Unicode, chỉ trừ một số lượng nhỏ ký tự điều khiển và dãy chưa được gán D800-DFFF và FFFE-FFFF. Các ký tự này biểu thị hoặc là các byte nếu bộ mã có định nghĩa hoặc là chuỗi số của Unicode nếu bộ mã không định nghĩa. Chẳng hạn: Δ Й ק م ๗ ぁ 叶 葉 냻 sẽ được hiển thị là Δ, Й, ק, م, ๗, ぁ, 叶, 葉 và 냻 nếu máy tính đã có cài đặt font thích hợp. Các ký tự này lần lượt là chữ \"Delta\" trong bảng chữ cái Hy Lạp, \"I ngắn\" trong bảng chữ cái Cyril, \"Meem\" trong bảng chữ cái Ả Rập, \"Qof\" trong bảng chữ cái Hebrew, số 7 trong bảng chữ cái Thái, Hiragana \"A\" của tiếng Nhật, chữ Hán \"diệp\" giản thể, chữ Hán \"diệp\" phồn thể và âm \"Nyrh\" bằng Hangul trong tiếng Hàn/Triều Tiên.\n\nPhông chữ Unicode có thể được tải về từ nhiều trang web, hầu hết chúng là miễn phí. Dù đã có hàng ngàn phông chữ trên thị trường, nhưng hầu hết chỉ hỗ trợ ở một mức độ nhất định một số ký hiệu ngoài ASCII của Unicode. Thay vì đó, các phông chữ Unicode thường tập trung hỗ trợ các ký tự ASCII và những chữ viết cụ thể hoặc tập các ký tự hay ký hiệu. Có vài nguyên do của điều này: các ứng dụng và tài liệu rất ít khi cần hiển thị ký tự từ nhiều hơn hai hệ thống chữ viết; phông chữ thường là những tập không đầy đủ; hệ điều hành và các ứng dụng ngày càng xử lý tốt hơn các ký tự từ nhiều bộ phông khác nhau... Thêm vào nữa, việc thiết kế một hệ thống chi tiết hàng nghìn ký tự là công việc đòi hỏi nhiều thời gian và công sức trong khi hầu như không thu lợi gì từ việc này....\n\nPhông chữ Unicode cho phép gõ tiếng Việt ở các phông Times New Roman hay Tahoma hay Arial\n\nISO/IEC 10646-2:2001\n\nISO/IEC 10646-2:2001",
		"url": "https://vi.wikipedia.org/wiki/Unicode"
	},
	"zh": {
		"content": "Unicode，全稱為Unicode標準（The Unicode Standard），其官方機構Unicode聯盟所用的中文名称为統一碼[1]，又译作萬國碼、統一字元碼、统一字符编码[2]，是信息技术領域的業界標準，其整理、編碼了世界上大部分的文字系統，使得電腦能以通用劃一的字元集來處理和顯示文字，不但減輕在不同編碼系統間切換和轉換的困擾，更提供了一種跨平臺的亂碼問題解決方案。Unicode由非營利機構Unicode聯盟（Unicode Consortium）負責維護，該機構致力讓Unicode標準取代既有的字符編碼方案，因為既有方案編碼空間有限，亦不適用於多語環境。\n\nUnicode伴隨著通用字符集ISO/IEC 10646的標準而發展，同時也以書本的形式[3]對外發表。Unicode至今仍在不斷增修，每個新版本都加入更多新的字符。目前最新的版本為2022年9月公布的15.0.0[4]，已經收錄超過14萬個字符（第十萬個字符在2005年獲採納）。Unicode標準不僅僅只是為文字指定代碼。除了涵蓋視覺上的字形、編碼方法、標準的字符編碼资料外，聯盟官方出版品還包含了關於各書寫系統的細節及呈現方式，如規格化的準則、拆分、定序、繪製、雙向文本顯示、书写方向、字符特性（如大小寫字母）等等。此外還提供參考資料和視覺圖像，以幫助開發者和設計師正確應用標準。\n\nUnicode備受認可，為ISO納入国际标准，成為通用字符集，即 ISO/IEC 10646。Unicode兼容ISO/IEC 10646，能完整对应各个版本标准[5][6]。Unicode廣泛應用於電腦软件的國際化與本地化過程。很多新科技，如可扩展置标语言（Extensible Markup Language，簡稱：XML）、Java程式語言以及現代作業系統，都採用Unicode來編碼。Unicode最普遍的編碼格式是和ASCII兼容的UTF-8，以及和UCS-2兼容的UTF-16。\n\nUnicode為解决傳統字元編碼方案的侷限而產生，例如ISO 8859-1所定義的字元雖然在不同的國家中廣泛地使用，可是在不同國家間卻經常出現不相容的情況。很多傳統的編碼方式都有共同的問題，即容許電腦處理雙語環境（通常使用拉丁字母以及其本地語言），但卻無法同時支援多語言環境（指可同時處理多種語言混合的情況）。\n\nUnicode编碼包含了不同寫法的字，如“ɑ／a”、“強／强”、“戶／户／戸”。然而在汉字方面引起了一字多形的認定爭議，詳見中日韓統一表意文字。\n\n在文字處理方面，統一碼為每一個字符而非字形定義唯一的代碼（即一個整數）。換句話說，統一碼以一種抽象的方式（即數字）來處理字符，並將視覺上的演繹工作（例如字體大小、外觀形狀、字體形態、文體等）留給其他軟件來處理，例如網頁瀏覽器或是文字處理器。\n\n目前，幾乎所有電腦系統都支持基本拉丁字母，并各自支持不同的其他编碼方式。Unicode为了和它們相互兼容，其首256个字元保留給ISO 8859-1所定義的字元，使既有的西歐語系文字的轉換不需特別考量；并且把大量相同的字元重複編到不同的字元碼中去，使得舊有紛雜的編碼方式得以和Unicode編碼間互相直接轉換，而不會遺失任何資訊。舉例來說，全形格式區段包含了主要的拉丁字母的全形格式，在中文、日文、以及韓文字形當中，這些字元以全形的方式來呈現，而不以常見的半形形式顯示，這對豎排文字和等寬排列文字有重要作用。\n\n在表示一個Unicode的字元時，通常會用「U+」然後緊接着一組十六進位的数字來表示這一個字元。在基本多文種平面裏的所有字元，要用四个数字（即2位元組，共16位元，例如U+4AE0，共支持六萬多個字符）；在零號平面以外的字元則需要使用五或六個數字。舊版的Unicode標準使用相近的標記方法，但卻有些微小差異：在Unicode 3.0裏使用「U-」然後緊接着八個數字，而「U+」則必須隨後緊接着四個數字。\n\n位於美國加州的Unicode組織允許任何願意支付會費的公司和個人加入，其成員包含了主要的電腦軟硬體廠商，例如Adobe系統、蘋果公司、惠普、IBM、微軟、施乐等。\n\n20世纪80年代末，組成Unicode組織的商業機構，和國際合作的國際標準化組織在電腦普及和資訊國際化的前提下，分別各自成立了Unicode組織[7]和ISO-10646工作小組。他們不久便發現對方的存在，而大家工作目的一致。1991年，Unicode Consortium与ISO/IEC JTC1/SC2同意保持Unicode碼表與ISO 10646標準保持兼容並密切協調各自標準進一步的擴展。雖然實際上兩者的字集編碼相同，但本質上兩者確為不同的標準。Unicode 1.1對應於ISO 10646-1:1993，Unicode 3.0對應於ISO 10646-1:2000，Unicode 3.2對應於ISO 10646-2:2001，Unicode 4.0對應於ISO 10646:2003，Unicode 5.0對應於ISO 10646:2003及附錄1–3。\n\nUnicode自2.0版本開始保持了向後兼容，即新版本僅增加字符，原有字符不會删除或更名。但從Unicode 14.0起，既有的區段可擴展或縮減（必須在沒有字符使用空間的前提下，若已有字符佔用空間不可縮減區段），第一個自Unicode 1.1以來擴展的既有區段為阿洪姆文（Ahom）。[8]\n\n統一碼聯盟在1991年首次發佈了The Unicode Standard。Unicode的開發結合了國際標準化組織所制定的ISO/IEC 10646，即通用字符集。Unicode與ISO/IEC 10646在編碼的運作原理相同，但The Unicode Standard包含了更詳盡的實現資訊、涵蓋了更細節的主題，諸如位元編碼（bitwise encoding）、校對以及呈現等。The Unicode Standard也列舉了諸多的字元特性，例如必須支援兩種閱讀方向的字符（由左至右或由右至左的文字閱讀方向，例如阿拉伯文是由右至左）。Unicode與ISO/IEC 10646兩個標準在術語上的使用有些微的不同。[5]\n\nUnicode的第十萬個字元（用於馬拉雅拉姆語）于2005年引入標準。\n\n截至目前的Unicode各版本及其發佈時間如下：\n\n其中，因應2019冠狀病毒病疫情，Unicode 14.0由2021年3月延後至2021年9月发布[9]。\n\nISO/IEC 10646-2:2001\n\nISO/IEC 10646-2:2001\n\n大概来说，Unicode编码系统可分为编码方式和实现方式两个层次。\n\n《The Unicode Standard Version 6.2 – Core Specification》[44] 文档给出了Unicode的十大设计原则：\n\n統一碼的编碼方式與ISO 10646的通用字符集概念相對應。目前实际应用的統一碼版本对应于UCS-2，使用16位的编码空间。也就是每个字符占用2个字节。这样理论上一共最多可以表示216（即65536）个字符。基本满足各种语言的使用。实际上目前版本的統一碼並未完全使用这16位编码，而是保留了大量空间以作为特殊使用或将来扩展。\n\n上述16位統一碼字符构成基本多文种平面。最新（但未实际廣泛使用）的統一碼版本定义了16个辅助平面，两者合起来至少需要占据21位的编码空间，比3字节略少。但事实上辅助平面字符仍然占用4字节编码空间，与UCS-4保持一致。未来版本会扩充到ISO 10646-1实现级别3，即涵盖UCS-4的所有字符。UCS-4是更大而尚未填充完全的31位字符集，加上恒为0的首位，共需占据32位，即4字节。理论上最多能表示231个字符，完全可以涵盖一切语言所用的符号。\n\n基本多文种平面的字符的编码为U+hhhh，其中每个h代表一个十六进制数字，与UCS-2编码完全相同。而其对应的4字节UCS-4编码后两个字节一致，前两个字节則所有位均为0。\n\nUnicode的实现方式不同于编码方式。一个字符的Unicode编码确定。但是在实际传输过程中，由于不同系统平台的设计不一定一致，以及出于节省空间的目的，对Unicode编码的实现方式有所不同。Unicode的实现方式称为Unicode转换格式（Unicode Transformation Format，简称为UTF）。\n\n例如，如果一个仅包含基本7位ASCII字符的Unicode文件，如果每个字符都使用2字节的原Unicode编码传输，其第一字节的8位始终为0。这就造成了比较大的浪费。对于这种情况，可以使用UTF-8编码，这是变长编码，它将基本7位ASCII字符仍用7位编码表示，占用一个字节（首位补0）。而遇到与其他Unicode字符混合的情况，将按一定算法转换，每个字符使用1-3个字节编码，并利用首位为0或1识别。这样对以7位ASCII字符为主的西文文档就大幅节省了编码长度（具体方案参见UTF-8）。类似的，对未来会出现的需要4个字节的辅助平面字符和其他UCS-4扩充字符，2字节编码的UTF-16也需要通过一定的算法转换。\n\n再如，如果直接使用与Unicode编码一致（仅限于BMP字符）的UTF-16编码，由于每个字符占用了两个字节，在麥金塔電腦（Mac）机和個人電腦上，对字节顺序的理解不一致。这时同一字节流可能会解释为不同内容，如某字符为十六进制编码4E59，按两个字节拆分为4E和59，在Mac上读取时是从低字节开始，那么在Mac OS会认为此4E59编码为594E，找到的字符为“奎”，而在Windows上从高字节开始读取，则编码为U+4E59的字符为“乙”。就是说在Windows下以UTF-16编码保存一个字符“乙”，在Mac OS環境下開啟会显示成“奎”。此类情况说明UTF-16的编码顺序若不加以人为定义就可能发生混淆，于是在UTF-16编码实现方式中使用了大端序（Big-Endian，简写为UTF-16 BE）、小端序（Little-Endian，简写为UTF-16 LE）的概念，以及可附加的位元組順序記號解决方案，目前在個人電腦上的Windows系统和Linux系统对于UTF-16编码默认使用UTF-16 LE。（具体方案参见UTF-16）\n\n此外Unicode的实现方式还包括UTF-7、Punycode、CESU-8、SCSU、UTF-32、GB18030等，这些实现方式有些仅在一定的国家和地区使用，有些则属于未来的规划方式。目前通用的实现方式是UTF-16小端序（LE）、UTF-16大端序（BE）和UTF-8。在微软公司Windows XP附带的记事本（Notepad）中，“另存为”对话框可以选择的四种编码方式除去非Unicode编码的ANSI（对于英文系统即ASCII编码，中文系统则为GB2312或Big5编码）外，其余三种为“Unicode”（对应UTF-16 LE）、“Unicode big endian”（对应UTF-16 BE）和“UTF-8”。\n\n目前辅助平面的工作主要集中在第二和第三平面的中日韩统一表意文字，因此包括GBK、GB18030、Big5等简体中文、繁体中文、日文、韩文以及越南喃字的各种编码与Unicode的协调性受重点关注。考虑到Unicode最终要涵盖所有的字符。从某种意义而言，这些编码方式也可视作Unicode的出现于其之前的既成事实的实现方式，如同ASCII及其扩展Latin-1一样，后两者的字符在16位Unicode编码空间中的编码第一字节各位全为0，第二字节编码与原编码完全一致。但上述东亚语言编码与Unicode编码的对应关系要复杂得多。\n\nUnicode 將編碼空間分成 17 個平面，以 0 到 16 編號。\n\n第 0 平面（或者說基本多文種平面）中的碼點，都可以用一個 UTF-16 單位來編碼，或者以 UTF-8 來編碼的話，會使用一、二或三個位元組。而第 1 到 16 平面（或稱輔助平面）中的碼點，UTF-16 會以代理對的方式來使用，而 UTF-8 則會編碼成 4 個位元組。\n\n在每個平面中，會先將相關的字符集結為區段的形式。雖然區段可以是任意大小，但會以 16 個碼點的倍數，且通常是 128 個碼點的倍數。而一份文稿中使用到的區段，可能會散布在多個區段中。\n\n在非Unicode环境下，由于不同国家和地区采用的字符集不一致，很可能出现无法正常显示所有字符的情况。微软公司使用了代码页（Codepage）转换表的技术来过渡性地部分解决这一问题，即通过指定的转换表将非Unicode的字符编码转换为同一字符对应的系统内部使用的Unicode编码。可以在“语言与区域设置”中选择一个代码页作为非Unicode编码所采用的默认编码方式，如936为简体中文GB码，950为繁体中文Big5（皆指PC上使用的）。在这种情况下，一些非英语的欧洲语言编写的软件和文档很可能出现乱码。而将代码页设置为相应语言中文处理又会出现问题，这一情况无法避免。只有完全采用统一编码才能徹底解決這些問題，但目前尚无法做到这一点。\n\n代码页技术现在广泛为各种平台所采用。UTF-7的代码页是65000，UTF-8的代码页是65001。\n\nXML及其子集XHTML采用UTF-8作为标准字符集，理论上我们可以在各种支持XML标准的浏览器上显示任何地区文字的网页，只要电脑本身安装有合适的字体即可。可以利用&#nnn;的格式显示特定的字符。nnn代表该字符的十进制Unicode代码。如果采用十六进制代码，在编码之前加上x字符即可。但部分旧版本的浏览器可能无法识别十六进制代码。\n\n过去电脑编码的8位标准，使每个国家都只按国家使用的字符而编定各自的编码系统；而对於部份字符系统比较复杂的语言，如越南语，又或者东亚国家的大型字符集，都不能在8位的环境下正常显示。 只是最近才有在文本中对十六进制的支持，那么旧版本的浏览器显示那些字符或许可能有问题——大概首先会遇到的问题只是在对于大于8位Unicode字符的显示。解决这个问题的普遍做法仍然是将其中的十六进制码转换成一个十进制码（例如：♠用&#9824;代替&#x2660;）。\n\n也有一些字符集标准将一些常用的标志存放在字符内码外面，那么你可能使用像—这样的文本标志来表示一个长划（—）的情况，即使它的字符内码已经使用，这些标准也不包含那个字符。\n\n然而部分由于Unicode版本发展原因，很多浏览器只能显示UCS-2完整字符集，也即现在使用的Unicode版本中的一个小子集。下表可以检验您的浏览器如何显示各种Unicode代码：\n\n一些多语言支持的网页浏览器，比如Microsoft Windows系统的Internet Explorer 5.5及以上版本，以及跨平台的浏览器Mozilla/Netscape 6，可以在安裝時根据需要动态地使用相应的字符集，预先安装了合适的语言包，就可以同时显示页面上的各种Unicode字符。Internet Explorer 5.5还提出用户可以在需要新字体时，即装即用。另外的浏览器如Netscape Navigator 4.77，则只能显示跟页面编码相应字符集中的文字。当你使用后一种浏览器时，你不大可能预先安装所有的字体，即使有了字体，浏览器也不一定能将这些字体完全应用起来。可能遇到的情况是，这种浏览器只能够显示部分文字，因为它们是按照标准编码，尽管理论上在兼容的系统中，只要有了相应的Code2000字体，就可以正确显示。一种变通的办法，是将某些少见的字符，通过“名称实体引用”的方式来使用。\n\n不同的操作系统，各有直接输入Unicode字符的方法：\n\n在SGML、HTML、XML的文本中，使用字符值引用或字符实体引用表示一个Unicode字符。\n\n截至2011年10月，可以使用微軟拼音2003或2007版本、倉頡輸入法第三代第五代第六代版本、鄭碼Unicode版本、海峰五筆9.3版本、新注音輸入法和VimIM輸入。\n\n使用Microsoft IME 2007，可以在IME Pad找到Unicode的点击表。点击字符即可输入。选择字体可以预览字符效果。\n\n除了輸入法外，操作系统也會提供另外幾種方法輸入Unicode。像是Windows 2000之後的Windows系統就提供可點擊的字符映射表。又或者在Microsoft Word下，按下Alt鍵不放，使用數字鍵盤輸入0和某個字符的Unicode编码（十进制），再鬆開Alt键即可得到该字符，如Alt033865會得到Unicode字元葉。另外，按AltX组合键，Microsoft Word也会将光标前面的字符同其十六进制的四位Unicode编码互相转换。",
		"url": "https://zh.wikipedia.org/wiki/Unicode"
	}
}
